{
  "0": [
    {
      "author": "rdalcroft",
      "body": "When there is load on the GPU, you never get spikes.\n\nPlaying a game I never have DPC latency issues.\n\nThe only time i get spikes is either at idle, doing Audio work in a DAW, watching Movies/streaming.\n\nUsing a balanced profile in windows is the worst as the CPU will change clocks all the time, this is the worst DPC\n\nUsing a High power plan, CPU stays at 5.1ghz almost 0 DPC latency spikes bar the odd one every 10 minutes or so.  so Acceptable.\n\n&#x200B;\n\nBut to have the GPU constantly on Highest performance makes no sense as there is minimal difference in games from a Normal to Max Performance.\n\nI don't ever see spikes in game, and for people blaming game stutters on DPC latency, im sorry they have a different problem.\n\n&#x200B;\n\nAnd thats why I suggested to use the profiles per game, if they are having stutters in game, but only do it for the games they have issues with."
    },
    {
      "author": "RickyTrailerLivin",
      "body": "> I don't ever see spikes in game, and for people blaming game stutters on DPC latency, im sorry they have a different problem.\n\nExactly. That's why I said to enable it globally, it mostly resolves the problems at idle.\n\nI think you misunderstood what I was trying to say."
    },
    {
      "author": "rdalcroft",
      "body": "DPC \"Deferred procedure call\"\n\nIts the time it takes for the CPU to answer a call from the GPU.\n\n(or something like that)\n\nSo having the CPU on a balanced power plan (Windows) takes longer for the CPU to Jump to life and do the requested action from the GPU.\n\nHaving a High Power Plan (Windows) makes this much snappier, giving much Better Latency.\n\n&#x200B;\n\nThe GPU power plan never comes into play.\n\nBecause when in game the CPU is in full use, max clocks, therefore no Spikes.\n\n&#x200B;\n\nTotally nothing to do with the GPU Power whether its Normal/Max Performance.\n\nBut for gaming stutters not related to DPC, then yes a Max Performance in the GPU can help.  But I would never have it set to Max Globally, because your fans would ne spinning all the time when idle.  Not a good Idea. well not for me.\n\nHope I am making sense. lol"
    },
    {
      "author": "RickyTrailerLivin",
      "body": "Hmm, max performance on nvcp shouldn't make your fans run fast.\n\nMine just goes from 17watts to 40 watts, hardly a massive difference.\n\nThe clocks are just a bit higher."
    },
    {
      "author": "rdalcroft",
      "body": "But enough to turn them on, as mine stop spinning once below 45 degrees."
    },
    {
      "author": "RickyTrailerLivin",
      "body": "Fair enough.\nMine start spinning at 49c."
    },
    {
      "author": "rdalcroft",
      "body": "could be the same for mine, I was making a guess lol"
    },
    {
      "author": "RickyTrailerLivin",
      "body": "AIB's usually have different limits on what they deem acceptable.\n\nBut its always between 45 and 50 afaik."
    },
    {
      "author": "rdalcroft",
      "body": "I mean it all a personal choice, Power plans and such.\n\nI prefer a balanced profile when im watching movies and web browsing\n\nHigh power plan when gaming and doing audio work.\n\nand I use Process lasso to switch my power plans according to what programs i use, and Games also.\n\nSo once I finished playing a game Process lasso will auto switch to the Balanced Power profile.  \n\nQuite handy."
    }
  ],
  "1": [
    {
      "author": "jesterc0re",
      "body": "Side note: I'm not sure how it will be with the vertical mounted GPU. It's overall worse for gpu temps."
    },
    {
      "author": "Danimaro777",
      "body": "Alright I will change them Ty"
    },
    {
      "author": "jesterc0re",
      "body": "With a vertical mounted GPU, imho your current fan layout should be better, because it actively pulls hot air from the GPU heatsink."
    },
    {
      "author": "jesterc0re",
      "body": "Oh you better try both."
    },
    {
      "author": "Danimaro777",
      "body": "Also when I replaced the thermal pads and used my own thermal paste the temps seem way better than when I had the stock thermal pads and paste maybe that help\ud83e\udd37\u200d\u2642\ufe0f not sure"
    },
    {
      "author": "Danimaro777",
      "body": "My gpu temps are good I play a 2k Max everything apex legends it runs around 40-48 that fans don\u2019t even spin only when it reaches 50 also warzone 2k Max everything  runs around 45-50 not sure if that\u2019s good"
    }
  ],
  "2": [
    {
      "author": "Worldly-Wealth7707",
      "body": "Not really. Every generation people call the newest models overkill and in a few years they are needed while the old parts of today won't keep up.\n\nAlso anyone that games competively will say that getting the best of what you can get is worth it.\n\nLastly, it's dumb trying to judge others who make more than you on what they should or should not spend their money on just because you can't afford it."
    },
    {
      "author": "Rugged_as_fuck",
      "body": "Perfect, thank you, I'll check it out."
    },
    {
      "author": "Worldly-Wealth7707",
      "body": "Sorry, I wasn't making assumptions. You said yourself that you can't afford to spend 2600 at once or else you might end up getting evicted. Thus not only do you not have a house, you also can't spend as much as I can while still living comfortably. \n\nAlso again, a  \"bad purchase\" to you is not the same as a \"bad purchase\" to someone else.\n\nI don't need a better pc to be better than other in the games I play. I already am better than 99.9% of the playerbase. But why would I limit myself compared to my competition when I can have more. \n\nEven for open world games, just because older parts can run them now with 4k 120 hz. Why wouldn't I want parts that do the same, but better. \n\nBetter looking games, with better fps, with ray tracing capabilities, and will last longer.\n\nAnd again, why would I limit myself to what I can do with my pc based on the things I do now without it? I again, constantly learn new things and may or may not want to use other aspects of it in the future and have that at my disposal. Either way it's not for youu to judge what I, op, or anyone else spends their money on pleb."
    },
    {
      "author": "Worldly-Wealth7707",
      "body": "You did say that though :P not too hard to find. What makes it better? The fps, the ability to do more, the ability to last longer as games get better and have increased requirements. Playing at 4k with higher refresh rates. The ability to move up to 8k when that becomes more normalized. Etc"
    },
    {
      "author": "Worldly-Wealth7707",
      "body": "I'm not getting 8k now. But it will be the norm in coming years just like 4k is now.\n\nAlso, your exact words were:\n\n\"I would be in big trouble this month, probably get evicted, and my lady would turn my face into a fold (could not afford an $1800 phone, a $600 tablet, and a $200 pair of buds at the same time without a HUGE discount, my last name is not Gates or Bezos lol)\""
    },
    {
      "author": "Worldly-Wealth7707",
      "body": "Are you really comparing smartphones to monitors and tvs? Smartphones have different priorities, aka battery life, which would be negatively affected by going to 4k. Not to mention you don't hold your phone the same distance from your face as a monitor or tv."
    }
  ],
  "3": [
    {
      "author": "Exotic_Scratch9450",
      "body": "my first build in 2008 had gtx 285"
    },
    {
      "author": "HattersUltion",
      "body": "I've enjoyed it so far. Tbh my gf wanted to get the case as a Christmas present so I gave her a list of 3 and she chose this. It shows off the components nicely. The thermals are good but the 4080FEs cooler is so stupidly massive that it just refuses to go above the 60s when gaming. I've heard of people with thicker coolers having issue in the y60. But it's my opinion that any cooler that is at or less than the thickness of the 4080FE leaves enough room between the front glass for the cooling solution to work efficiently. Any bigger might reduce thermal performance though. As for fan speed/noise the CPUs cooler puts out more noise than the GPUs. I don't think the GPU fans have gone above 50%(it can get thru multiple matches of WZ2 in the zero fan state \ud83d\ude02). And the CPU cooler never gets loud enough to hear over my headset when gaming so I'll take it."
    },
    {
      "author": "HattersUltion",
      "body": "Yeah well I did remain a console gamer into my 30s. This and cars are my hobbies. Started out with hot wheels and a NES now we here. Both are money pits. But fun."
    },
    {
      "author": "Exotic_Scratch9450",
      "body": "nice"
    }
  ],
  "4": [
    {
      "author": "pixelcowboy",
      "body": "Still rocking my x470 itx motherboard which I've kept through multiple processor upgrade and cases. Started with a Nano S Mini, moved up to a Thermaltake S100, and now a Lancool 216 just to fit the new card. Processor wise went from a 2600x to a 3800x (returned), then a 5600x and now a 5800x3d, which is perfect for the 4090.\n\nThe Lancool 216 is amazing and the card runs nice and quiet."
    },
    {
      "author": "Technical-Titlez",
      "body": "Hahaha. Love it."
    },
    {
      "author": "Technical-Titlez",
      "body": "I think it looks awesome dude.\n\nYou won't have a single enthusiast refrain from commenting on this, it's a talk piece for sure."
    },
    {
      "author": "pixelcowboy",
      "body": "It's ridiculous but it works. And the motherboard hasn't given me any reason to want to upgrade."
    },
    {
      "author": "Technical-Titlez",
      "body": "Naw, no need really, especially with that GPU."
    }
  ],
  "5": [
    {
      "author": "Ok-Sherbert-6569",
      "body": "The naming is certainly unfortunate and the pricing is fucking mental but I find it funny when the reviewers use the naming issue as a negative of this card. No one is buying a 4090 laptop without having a pretty solid knowledge of the industry and if they don\u2019t they clearly have enough money that the naming discrepancy is not of any importance to them at all"
    },
    {
      "author": "Ifuqaround",
      "body": "Plenty of people will buy a 4090 laptop without having solid knowledge of the industry.\n\nIt's why they MAKE THESE PRODUCTS.\n\nKids will see 4090 and go after it (asking their parents for $) then be here on Reddit asking about thermal throttling.\n\nGaming laptops are a joke. Always have been imo."
    },
    {
      "author": "Ok-Sherbert-6569",
      "body": "Then those people have enough money to not have to consider Lowe option or research the best bang for buck so you don\u2019t need to worry about them"
    },
    {
      "author": "Ifuqaround",
      "body": "not worried about them at all\n\nlet them gobble up these shitty laptops lol"
    },
    {
      "author": "Ok-Sherbert-6569",
      "body": "Exactly which strengthen my point that the naming is not an issue. People like us will not buy this product and those who do would have bought it even if it was called 4090000xh"
    }
  ],
  "6": [
    {
      "author": "Similar-Doubt-6260",
      "body": "Lol if anything get the 4080 over a 4070ti. 12gb of vram looking iffy with all these new games coming out unoptimized or not."
    },
    {
      "author": "ElonMuskWeekly",
      "body": "I hear you. I will for sure give it some serious thought \ud83d\udc4d"
    },
    {
      "author": "ElonMuskWeekly",
      "body": "Just a small follow up. Are you happy with the Gigabyte Gaming OC in general?\nAnd did you get a GPU anti sag bracket with the card?\nI would want a bracket of good quality to hold a card like that steady."
    },
    {
      "author": "Similar-Doubt-6260",
      "body": "Yes it's been perfect so far. Been using it for about a month with no issue. Undervolted im seeing no more than 340w and 57c. It comes with a sag bracket that attaches to your case so you should be good. I had a zotac one first that was faulty and constantly rebooted my pc (never going zotac again) and the msi one whines when anything pushes 150w. 0 issues with my current gaming oc card."
    }
  ],
  "7": [
    {
      "author": "kasakka1",
      "body": "RDR2 is a special case in this. Nothing else I have seen cramps the in engine cutscenes from both sides on a superultrawide.\n\nTo me how they handle it shows a fundamental lack of understanding why movies use those wider aspect ratios when they have literally just added black bars to 16:9. It's not \"more cinematic\"."
    },
    {
      "author": "Soulshot96",
      "body": ">RDR2 is a special case in this. Nothing else I have seen cramps the in engine cutscenes from both sides on a superultrawide.\n\nFH5 does this on 21:9, probably the same on wider screens. There are a few other games but I can't remember the names off the top of my head. \n\n> To me how they handle it shows a fundamental lack of understanding why movies use those wider aspect ratios when they have literally just added black bars to 16:9. It's not \"more cinematic\". \n\nI don't disagree, I feel the same about bullshit like 24fps continuing to be defended, despite lower and lower persistence displays (like OLED), becoming more mainstream and 24fps becoming more and more headache inducing, but there is an army of 'cinematic' preachers ready to scream at you in defense of stuff like this, and as long as there are, creatives will continue to do this shit."
    },
    {
      "author": "kasakka1",
      "body": "I can accept it for games that use pre-rendered cutscenes so they are basically video files, but for in-game cinematics they should respect the user resolution and at minimum support 21:9 in modern games, though I wish more developers put in the effort to solve 32:9 FOV distortion problems.\n\nThere's a surprising amount of game developers who want to be movie makers. I mean Max Payne 3 for example is constantly interrupting gameplay with trivial cutscenes because its developers seemingly wanted to make a movie instead."
    },
    {
      "author": "Soulshot96",
      "body": "I'd appreciate it, but I fully understand why things are the way they are, and I don't really expect change from all but the absolute top tier PC port studios tbh.\n\nGood chance that once there are *good* 32 inch QD OLED monitors around that I will ditch ultrawide entirely tbh. Only here because the only QD OLED monitor you can get is 21:9 as is lol."
    }
  ],
  "8": [
    {
      "author": "Affectionate_Suit895",
      "body": " You're saying I shouldn't buy pny, right?"
    },
    {
      "author": "NutellaGuyAU",
      "body": "Yes AVOID the PNY card, the galax card is the better purchase which is the card I\u2019m also purchasing"
    },
    {
      "author": "Affectionate_Suit895",
      "body": "So are you glad you bought it? from its performance, cooling, stability? Did you have any problem with galax rtx4090?"
    },
    {
      "author": "NutellaGuyAU",
      "body": "Galax is a huge company they make a lot of GPUS, I don\u2019t personally have the card but reviews are pretty much on par with every other aftermarket rtx4090 there really isn\u2019t a much difference between cards other than design of the cooler, and the price"
    }
  ],
  "9": [
    {
      "author": "hicks12",
      "body": "Another work around is have your secondary monitor on the left, this avoids the resizing issue as it's a windows quirk working left to right (still up to Nvidia to work around it).\n\nYou can test this just by moving your secondary monitor to the left in display settings and it will be fixed, it is jarring to have to use the mouse in the wrong way though to get to the screen so I'd suggest moving it physically if you go with this workaround."
    },
    {
      "author": "qwertyalp1020",
      "body": "Alright!\n\nChanging the native res made the text a bit weird. Don't know if I have to recalibrate ClearType though.\n\nI'll try the monitor moving method now!"
    },
    {
      "author": "hicks12",
      "body": "I tried using the desktop in the high res but it then limited me to 60hz, I assume it's a bug for this or my windows is borked.\n\nThe best option was to just move my monitor around, has taken awhile to get used to it but it's easier this way at least haha"
    },
    {
      "author": "qwertyalp1020",
      "body": ">I tried using the desktop in the high res but it then limited me to 60hz, I assume it's a bug for this or my windows is borked.\n\nYou can fix that by changing the refresh rate directly in Nvidia Control Panel."
    }
  ],
  "10": [
    {
      "author": "phero1190",
      "body": "Take out the cable cover?"
    },
    {
      "author": "No-Platform-5873",
      "body": "Not sure if my bracket wasn\u2019t bent properly but I ended up bending it to 90 degrees and got it to work"
    },
    {
      "author": "No-Platform-5873",
      "body": "Is that the only option? Was wondering if others had run into the situation"
    },
    {
      "author": "phero1190",
      "body": "If that's causing the issue, get rid of the issue. Either that or get a new case."
    },
    {
      "author": "No-Platform-5873",
      "body": "Wouldn\u2019t just not using the support be better?"
    }
  ],
  "11": [
    {
      "author": "Tobi97l",
      "body": "It is a really nice feature so save power. Let's say i have to go afk for 5 minutes. I can just alt tab out of the game and it will reduce to 20fps. I don't need the full performance when i'm not even tabbed in anyway."
    },
    {
      "author": "heartbroken_nerd",
      "body": "Did you try disabling max background fps specifically for the 2 apps? Nvidia Control Panel -> Manage 3D Settings -> Program Settings ->either find on the list or ADD"
    },
    {
      "author": "heartbroken_nerd",
      "body": "That's cool but this person said they DON'T want the background applications to have 20fps. They specifically have the option to turn it off. Why not use it?"
    },
    {
      "author": "Tobi97l",
      "body": "I understood you wrong. I thought you meant that he should disable it globally. Yes for this application they need to disable it."
    }
  ],
  "12": [
    {
      "author": "OverloadedConstructo",
      "body": "according to outervision, a 400w psu is minimum if using rtx 2060 and ryzen 5 3600, but yeah I even use RTX 3060 with 400w psu (although 80 plus gold)."
    },
    {
      "author": "Large-Face5241",
      "body": "yeah i saw a lot of people recommending 550w, but i didnt know if it was really needed."
    },
    {
      "author": "Large-Face5241",
      "body": "yeah but i think i may get 550w, i feel more secure following the recommendations."
    },
    {
      "author": "Large-Face5241",
      "body": "thanks"
    }
  ],
  "13": [
    {
      "author": "Celerun",
      "body": "Sorry man, I should be more specific. Your hose should be in the bottom. \nNot to be \u201cbUt sTEve sAyS sO\u201d but, Steve says so :D\n\nhttps://youtu.be/BbGomv195sk\n\nWatch it, he makes it make sense."
    },
    {
      "author": "cwwjr1681",
      "body": ">Watch it, he makes it make sense.\n\nNo it shouldn't I would have to reverse the fans to do that.  Like I said Arctic by default is the reverse of most AIOs.  When he did the review of the Arctic he top mounted it just like I did. You have to understand how liquid moves man.  Top or front is best. Bottom is BAD. You want the houses running down. The same way water flows.\n\nWhy the F would you make an aio work in the reverse of gravity? Thats FORCING flow.  Thats bad man. You are 100% wrong"
    },
    {
      "author": "Celerun",
      "body": "Oh shit really \ud83d\ude02\nMy bad. \nI have pull from front and pull on the inside when I had a 3080 (no room for that now).\nGuess I have to watch that review then, even though I figured it was the same with any AIO due to orientation and flow. Hmm."
    },
    {
      "author": "cwwjr1681",
      "body": "You want the houses running down!  Like a waterfall.  You dont want to FORCE flow.  Don't work against gravity ever. Doing what you said to do with it at the bottom is WRONG and he explained why NOT to do that in the video.  It can make air bubbles in your line dude.  Thats why he top mounted the NZXT in this same video and specifically said to make sure the houses run down.\n\nYou were doing the reverse of that\n\nhttps://preview.redd.it/vs7d81xb4lha1.jpeg?width=1548&format=pjpg&auto=webp&v=enabled&s=18d9e82249c207c60d55dab106f66405a7a012ff"
    },
    {
      "author": "cwwjr1681",
      "body": ">Guess I have to watch that review then\n\nThis is an image of an \"upside down\" radiator. Its Bad placement because the tubes run up and you are FORCING flow. Think of it like trying to reverse flow a waterfall. Just dont do it. There is a CHANCE you can get tiny bubbles in your line. He explains why this is bad placement.\n\nhttps://preview.redd.it/xy212em97lha1.jpeg?width=1531&format=pjpg&auto=webp&v=enabled&s=bab7f43f005c348d343b78d76bf14c505ae0f8c2"
    }
  ],
  "14": [
    {
      "author": "blank_dota2",
      "body": "I ironically used JAN24EMOB1 today. Interesting the February version is working already. The Jan code should work for a few more days."
    },
    {
      "author": "zfancy5",
      "body": "So I bought my 4090 online through Best Buy. I was unable to use my Best Buy credit card or a coupon. Could I return it, then just re purchase in store and have them apply both?"
    },
    {
      "author": "blank_dota2",
      "body": "It\u2019s worth trying but in my experience bestbuy will probably make you return it and then you\u2019d have to reorder from the website to use the coupon."
    },
    {
      "author": "zfancy5",
      "body": "dang. Maybe I\u2019ll call and see. Thanks!"
    }
  ],
  "15": [
    {
      "author": "Professional_Note257",
      "body": "That was a great chip in those days if I recall ( try to go from memory instead of go ogling everything) we are getting older :)"
    },
    {
      "author": "Hetstaine",
      "body": "Same, paired it with two 7900gs in sli. Never went sli again."
    },
    {
      "author": "Professional_Note257",
      "body": "Ehh i had 7900gtx at one time went thru 3 off them, all brand new but had artifacts had to underclock each card ( still rma them) kept last and gave up."
    },
    {
      "author": "Hetstaine",
      "body": "I couldn't afford the gtx at that time, gs had to suffice. I ended up giving my one to one of our it guys at work and he made sure the sites i wanted to surf, mostly car and flight sim forums were unlocked...so it worked out well in the end :)"
    }
  ],
  "16": [
    {
      "author": "SophisticatedGeezer",
      "body": "FE. I got super lucky. I don\u2019t think you can go too wrong with any card. Some, such as the Zotac Trinity cards and palit cards have lower quality PCBs. I would personally avoid those, but they should be fine. Solid options include the Gigabyte Gaming OC, MSI Suprim X and ASUS TUF."
    },
    {
      "author": "misiek685250",
      "body": "Yea I know your pain about gigabytes of software (I'm using Aorus Z790 ELITE AX, and bios lags like hell, it's a common problem), and I'm familiar with that rgb software also. Thx for the info, I like Gigabytes of hardware so maybe I'll go for it also"
    },
    {
      "author": "misiek685250",
      "body": "MSI Gaming X Trio is quite good. But for me, PCB is not the most important thing that I'm looking at, just for cooling performance, and the performance of the GPU itself"
    },
    {
      "author": "SophisticatedGeezer",
      "body": "Are cards like the Suprim X much more than the Gaming X Trio where you live? If so, don\u2019t bother, and get a mid-range card like the Trio or Gaming OC."
    },
    {
      "author": "misiek685250",
      "body": "In terms of cooling, yes (I'm always using fan curve in MSI Afterburner). But in terms of PCB then there are some differences in build quality"
    }
  ],
  "17": [
    {
      "author": "sittingmongoose",
      "body": "You might know that you should check wattage.  But I don\u2019t think I know another soul who does.  Why would I expect that a 4090 and a 4090 are different?  \n\nWork in a bestbuy or microcenter for 1 week and you will realize why this is all a huge issue."
    },
    {
      "author": "NBAYoungBoyGOD",
      "body": "It takes a couple minutes of \u201cresearch\u201d, as in you type a couple words and watch a video or just read a article. If this was some obscure information hidden from the public I\u2019d be with you but it isn\u2019t sooo \ud83e\udd37\ud83c\udffd\u200d\u2642\ufe0f"
    },
    {
      "author": "sittingmongoose",
      "body": "Most people walk into a bestbuy or some random online page.  See the bulleted spec, and grab it.  The people that do any kind of research, ESPECIALLY about tdp is in a severe minority.  \n\nI spent 3 different days trying to explain to my friends dad why his computer died because one of his drives in raid 0 died.  (Which dell setup by default this way).  Using analogies, drawings, etc and it didn\u2019t sink in.  Hell, I told him 4 different times in a week not to plug into the motherboard hdmi port.  EVEN BLOCKING IT WITH TAPE AND A NOTE!  And he still plugged into it.  \n\nThat is the normal experience for MOST people."
    },
    {
      "author": "NBAYoungBoyGOD",
      "body": "That\u2019s their fault for being dumb tho. Also I highly doubt most treat 3-5 THOUSAND dollars like this. It\u2019s no different to someone grabbing a 6900xt, expecting it to perform like a 3090 with max settings; only to realise it can\u2019t ray trace for shit. 5 minutes of basic research is not some crazy ask for such expensive products, hell even day to day cheap ones."
    },
    {
      "author": "sittingmongoose",
      "body": "You are in incredibly out of touch from reality.  \n\nGo walk around a bestbuy for an hour and listen in on people\u2019s conversations."
    },
    {
      "author": "NBAYoungBoyGOD",
      "body": "Idc if most Americans are dumb and neither should nvidia. Also it\u2019s interesting you think they\u2019d be disappointed after \u201crealising\u201d their 4090 laptop is half the performance of a real one. However assuming they did no research they\u2019d never know regardless and would be happy with their purchase regardless."
    }
  ],
  "18": [
    {
      "author": "Im_simulated",
      "body": "Well yes they are interchangeable (somewhat, they won't work on a 4090) they are not exactly the same. I'm not saying this is what made the difference because honestly we don't know, But I know I held off on the 3090ti cuz I knew the 4090 was dropping soon. I'd say it's a pretty safe bet a lot of people had this train of thought. I'd guess for every 3090ti sold, 10 of the 4090s were. The 3090tis did not have the sense pins in the adapters, and the PCB design for it has to be slightly different as we now have a red indicator light on the GPU. Maybe this contributed nothing to it, or maybe this is what is stopping it from sinking in fully for most ppl. I have no idea."
    },
    {
      "author": "jzltk",
      "body": "Yes perhaps it was the sense pins and perhaps it was just the low volume.\n\nBtw the 3090Ti adapters do indeed work on a 4090, and of course there's no reason they wouldn't since they both have the same power limit, the sense pins are not required which is why the the 3090Ti has them just like the 4090 despite the adapter not having them."
    },
    {
      "author": "Im_simulated",
      "body": "I stand corrected, maybe they do technically work on the 4090s, however you can't get the full 600 can you? I know the ones with only 2 ends going into the PSU didn't work with my 4090 and found out after I ordered it that it was for the 3090tis. I'll take your word for it though that others can be used."
    },
    {
      "author": "jzltk",
      "body": "Yep they do but you can't push the power limit past the 100% limit of 450W."
    }
  ],
  "19": [
    {
      "author": "Crintor",
      "body": "What do you mean.\n\nSuppress noise from your mic for something on Firefox?\n\nOr suppress noise going to your speakers coming from Firefox?"
    },
    {
      "author": "__Reddit_username",
      "body": "Suppress noise and using the Nvidia broadcast webcam. Mainly for google meet"
    },
    {
      "author": "Crintor",
      "body": "You should only have to point Firefox to use broadcast as it's input device, and point broadcast to use your input device."
    },
    {
      "author": "__Reddit_username",
      "body": "How do you do that with Video? I tried to search around and I only found a way through the audio side of things"
    },
    {
      "author": "Crintor",
      "body": "I'm not sure exactly what you mean.\n\nEvery program that uses your camera has had it where you choose what camera input it should use.\n\nI am not familiar with trying to run things for your exact use case though."
    }
  ],
  "20": [
    {
      "author": "CableMod_Alex",
      "body": "[eustore.cablemod.com](https://eustore.cablemod.com) :)"
    },
    {
      "author": "MAXIMEOWNIT",
      "body": "Thanks, i checked in, but couldn't find the 90 or 180 degree adapters, i saw only the 12vhpwr cables, i guess you don't have the 90 or 180 degree adapters on your website"
    },
    {
      "author": "CableMod_Alex",
      "body": "They\u2019re not available to order yet, they will be within a couple of weeks. :)"
    },
    {
      "author": "MAXIMEOWNIT",
      "body": "Ok \ud83d\udc4d  no wonder i couldn't find them on the website, alright then i'll wait for them to be available, thanks \ud83d\udc4d\ud83d\udc4d"
    }
  ],
  "21": [
    {
      "author": "ksperdc",
      "body": "https://preview.redd.it/2p0j30kpmnga1.jpeg?width=2160&format=pjpg&auto=webp&v=enabled&s=25a759cad0b73bbd5cd279ce87d4b0402d5e2962\n\nRtx 4090 gigabyte OC the pcb Is half of the card cooling system."
    },
    {
      "author": "WinThenChill",
      "body": "That is awesome! I hadn\u2019t thought about it. Thank you very much!"
    },
    {
      "author": "WinThenChill",
      "body": "Thanks to your idea I\u2019ve already found most custom 4000 series cards\u2019 PCB sizes. Thanks a ton!"
    },
    {
      "author": "WinThenChill",
      "body": "Thanks a lot :D Appreciate it! Do you have any numbers? Or way to measure it at the moment?"
    },
    {
      "author": "ksperdc",
      "body": "https://preview.redd.it/hxnt70523oga1.png?width=1054&format=png&auto=webp&v=enabled&s=0a5ba37ea2f5c6ed750fcc1246a8609ca515f289"
    }
  ],
  "22": [
    {
      "author": "dhskiskdferh",
      "body": "Sounds like you have a bad psu"
    },
    {
      "author": "LightyLittleDust",
      "body": "Yeah, right. :)\n\n850W Seasonic Prime Platinum for the last few years."
    },
    {
      "author": "dhskiskdferh",
      "body": "Sorry, I don\u2019t mean it\u2019s a bad choice. I mean there\u2019s a problem with it. I\u2019ve swapped thru 3 PSUs and 3 GPUs this past month and coil whine was only bad (on all cards) with one of the PSUs, surprisingly the only platinum one (others were gold)"
    },
    {
      "author": "LightyLittleDust",
      "body": "One died at some point, and they've changed it for a new one, same coil whine still. But as I've said, does not bother me in the slightest. GTX 1060, 1080 Ti, RTX 3080, now 4070 Ti, all had & have coil whine. The only card that wasn't doing it was GTX 750 Ti, if I recall correctly, which I had in 2015-2016."
    }
  ],
  "23": [
    {
      "author": "ZookeepergameBrief76",
      "body": "Ahh, I have it set to \u201cfast\u201d on global. Seems to be working but for sure I\u2019ll try your suggestion."
    },
    {
      "author": "sebseb88",
      "body": "https://preview.redd.it/cxv9yhnvqrga1.png?width=1380&format=pjpg&auto=webp&v=enabled&s=924d24d8ce3b64fe081eb1c1f285d226c177ef5a\n\nGsync+Vsync+frame cap, fast vsync is a different beast all together. If you have a gsync monitor don't use anything else but normal vsync. Normally on its own it would introduce latency BUT with gsync it will work as intended until you go over your refresh rate hence the frame cap \ud83d\udc4d"
    },
    {
      "author": "ZookeepergameBrief76",
      "body": "Oh thank you, I\u2019m going to do this from now on. I\u2019m always up for the best optimized settings so much appreciated!"
    },
    {
      "author": "sebseb88",
      "body": "No problem \ud83d\udc4d just remember that if you're playing a game that has FG to turn off the frame limiter as reflex will kick in (unless said game is DL2 until they fix the broken reflex) Having a frame cap on top will hurt performance massively by introducing some very weird stuttering/lag"
    }
  ],
  "24": [
    {
      "author": "jjoncm1",
      "body": "Thankfully the ones sold recently have newest firmware, bought one a couple months ago and already had a couple firmwares newer than the problematic one, always great to double check though!"
    },
    {
      "author": "quarkninja",
      "body": "Thanks! It's a [coolermaster](https://www.amazon.com/gp/product/B08BWZCY56/) gpu mount with [linkup](https://www.amazon.com/dp/B0B4T7KLP7) riser cable."
    },
    {
      "author": "quarkninja",
      "body": "Wow, TIL."
    },
    {
      "author": "quarkninja",
      "body": "Thanks! I know I could have gone with a higher end CPU or even 7000MHz+ ddr5 ram kit. But decided to use the budget for other parts of the system instead.  Pretty satisfied with the performance."
    }
  ],
  "25": [
    {
      "author": "LumpyBeach8275",
      "body": "okay, because ive haerd and have seen some videos on yt that it has fan issue or smth."
    },
    {
      "author": "kristaintoth",
      "body": "And they are probably true especially if they demonstrated it in a video, I can only say my own experiences."
    },
    {
      "author": "LumpyBeach8275",
      "body": "but did you have fan issues?"
    },
    {
      "author": "kristaintoth",
      "body": "As I said I have not had any issues so far."
    }
  ],
  "26": [
    {
      "author": "Competitive-Ad-2387",
      "body": "Hello OP. I have a 3060 in my sub PC and broadcast effects work fine. I tried out the camera background blur / replacement and RTX voice. Both work perfectly. I did not try the eye contact one, but I would assume it works too.\n\nHope this helps!"
    },
    {
      "author": "ytross",
      "body": "Thanks! Can you use 2 at the same time? (Doesn't matter which)"
    },
    {
      "author": "Competitive-Ad-2387",
      "body": "Sure! I tried the camera effects plus RTX voice enabled. Works fine! Just keep in mind it pulls plenty of GPU resources, so one needs to be realistic if using Broadcast + running a game.\n\nIt\u2019s super good for recording though. 3060 is a beast as a small workstation card thanks to the 12 gigs of vram, yet people sleep on it.\n\nEnjoy man!"
    },
    {
      "author": "ytross",
      "body": "Thank you!!"
    }
  ],
  "27": [
    {
      "author": "Healthy-Aioli3693",
      "body": "So u mean like i should crank up the power limit to max and undervolt?"
    },
    {
      "author": "frostygrin",
      "body": "No - you *lower* the power limit, so that it keeps power consumption in check all the time. Then you can undervolt without worrying about power consumption - and set a higher clock cap. Some games will have low enough power consumption even at 1800MHz. Some games will be more demanding - and then the power limit will downclock them, but only as much as necessary.\n\nAlso use Nvidia's framerate limit together with Adaptive power mode - this will make the card downclock more aggressively at partial load, leading to lower power consumption."
    },
    {
      "author": "Healthy-Aioli3693",
      "body": "I see im going to try it out thanks"
    },
    {
      "author": "Healthy-Aioli3693",
      "body": "So i did try it but keep on getting blue screens when limiting the power limit and undervolting......\n\n\nCan u please tell me how ur settings were while doing it?"
    },
    {
      "author": "frostygrin",
      "body": "You can't get blue screens from lowering the power limit - so they must be from the undervolt (or maybe something else being overclocked, like the CPU). \n\nIf you want to get it over with, you can just reset the settings to defaults, skip the undervolt and just lower the power limit. This will give you lower power consumption with a small hit to performance. \n\nIf you want to get as much performance as possible, you can reset the settings to defaults, then use OC Scanner in Afterburner (or GPU tuning in Geforce Exprerience - it's the same thing). Then lower the power limit. \n\nOr you can turn down your existing undervolt little by little (by 15MHz because that's how clocks get adjusted). Clocks are going to vary from card to card."
    },
    {
      "author": "Healthy-Aioli3693",
      "body": "I see im going to try that\n\n\nIdk i think i reduce to much with the ubdervolt with the poqer limit also reduced\nSo maybe thats why i got BOSD\n\n\nAlso cant you reduce the power limit and do the OC in afterburner to get lower temps with better performance?"
    },
    {
      "author": "frostygrin",
      "body": "> Idk i think i reduce to much with the ubdervolt with the poqer limit also reduced So maybe thats why i got BOSD\n\nNo, the card will be clocking lower with a lower power limit, so it's less likely to crash. \n\n> Also cant you reduce the power limit and do the OC in afterburner to get lower temps with better performance?\n\nYou can, yes, that's what I suggested, with the OC Scanner. You can do it manually too - but even then you can run the OC Scanner first to check what's likely to be stable on your card. (Don't forget to reset these settings if you decide to try manual overclocking after automatic overclocking - the result of automatic overclocking will be applied automatically even after reboot and even without Afterburner running, until you reset it)."
    },
    {
      "author": "Healthy-Aioli3693",
      "body": "I see i did try it out but msi afterburner OC said \n\n\nResults are considered unstable:/\n\nGoing to play with it more and see if i need to bump up more of the temp limit thing"
    },
    {
      "author": "frostygrin",
      "body": "> Results are considered unstable:/\n\nIt's a typical result, not a problem. You won't necessarily see any instability in games. So give it a try.\n\n> Going to play with it more and see if i need to bump up more of the temp limit thing\n\nIt isn't worth it. You get diminishing returns pushing the card further than the stock settings. Plus the card will downclock slightly with higher temperature, to keep things stable."
    },
    {
      "author": "Healthy-Aioli3693",
      "body": "I see thanks for the help bro \ud83d\ude4f\n\nAlso what temp/power limit do u recommend to reduce it at ? \nI have it at 75/75\nIs it to low ?"
    },
    {
      "author": "frostygrin",
      "body": "75% power limit is OK - you're getting most of the performance from the card. But you can check the power consumption in Watts in Afterburner - then adjust the power limit how you like. 70-80% is a good range.\n\n75C temperature limit is maybe a bit too safe - you can decouple it from the power limit in Afterburner, with the little lock button, and return to the original 83C or set to 80C. Or even leave it at 75C - it's OK too. Depending on the fan settings you may be losing some performance to keep the card at 75C even as it isn't very necessary."
    },
    {
      "author": "Healthy-Aioli3693",
      "body": "I see so just to confirm\nI cant fet bosd from reducing the temp/power limit to 75 ?\n\nBecause i did that and got it......"
    },
    {
      "author": "frostygrin",
      "body": "You certainly can't get a BSOD from that. You can get a BSOD from an unstable undervolt/overclock if they're still being applied. Or a CPU/RAM overclock. Or sometimes from the drivers. I've had that happen, recently too."
    },
    {
      "author": "Healthy-Aioli3693",
      "body": "I mean i just did the reducing with only temp/power limit to 75% and i got bosd while gaming.....\n\nI think im going to stick with undervolting without touching the temp limit because idk way my card gets unstable\n\nEither my card is just extremely shit or im tripping\n\nRn im trying 825V and 1500mhz\n\n\nI tried 800v and 1500mhz but card was crashing\n\nLets hope this work otherwise I'm going to cry my self to sleep later \ud83e\udd72"
    },
    {
      "author": "frostygrin",
      "body": "1500 MHz is way too low. Just make sure you reset all the curves, offsets, automatic OC to default settings - then test for stability. Make sure it's stable at stock settings first. Then lower the power limit.\n\nOther components, like the PSU, can cause instability too. Maybe update the drivers, or uninstall them and install an older version."
    },
    {
      "author": "Healthy-Aioli3693",
      "body": "Ik 1500mhz is low but thats the only setting that didn't fuck up my gpu \ud83d\ude2d and workee at 825Volts\n\nI wanted to be at 800v mark was my goal\n\n\nIdk why dont work \nSeen people with a 2080 ti with 825v with 1800mhz......\n\nI have made sure its stable at stock and everything works ! But only when i try to undervolting/reduce temp it goes bananas \ud83c\udf4cit works for some hours but then bosd after ... I hate it man \n\nAlso already updated drivers and all to nothing wrong with those\n\n\nI think either my gpu is just bad at undervolting below 900v mark or im doing something wrong....."
    }
  ],
  "28": [
    {
      "author": "dave3573",
      "body": "https://preview.redd.it/h023r09rzgga1.jpeg?width=4000&format=pjpg&auto=webp&v=enabled&s=1553e50847df05d5de7069af492d5ad9a5f1a618"
    },
    {
      "author": "Beautiful-Musk-Ox",
      "body": "no i'd say that's not daisy chain. daisy chain is like this: https://preview.redd.it/cs1amhwxctt91.jpg?width=640&crop=smart&auto=webp&v=enabled&s=5016b55cfa02e48912e389949f7f0af2b74814a6, notice how it's 8 pins on the PSU to two sets of 8 pins on the video card side, so 8 on the PS to 16 on the video card, but yours is 12 on the PS to 16 on the video card\n\n i assume you have two of those cables in your picture, so like the other person said just plug both cables into the PSU then plug three of the other end in to the video card leaving the 4th one dangling. If your ps only came with one of those cables then it's probably not high enough wattage to drive your 3080ti"
    },
    {
      "author": "dave3573",
      "body": "Awesome thanks! Yes bequiet strait power 11"
    },
    {
      "author": "Beautiful-Musk-Ox",
      "body": "you didn't reply to the other person's comment btw, you just made a new top level comment. i'm curious which straight power 11 is it? there's 12 different ones.."
    }
  ],
  "29": [
    {
      "author": "-Hovercorn-",
      "body": "Do you have the non-X version?  If so, how's the idle power consumption?  I noticed that it's *really* high for the Gaming X Suprim X (as per TPU and Guru3d reviews).\n\nI'm wondering if the non-X variant is any different.  If so, I'll probably snag it -- still can't get my hands on an FE or TUF non-OC."
    },
    {
      "author": "DHonor125",
      "body": "I have the Gaming X. I haven't noticed too high of a power draw, but I can't really confirm how much exactly right now as I'm not home for some days. How much is the power draw you're referring to though?\n\nAlthough I can honestly say that this is probably the most well built card that I ever had, also the first MSI one."
    },
    {
      "author": "-Hovercorn-",
      "body": "This [TechPowerUp](https://www.techpowerup.com/review/msi-geforce-rtx-4090-suprim-x/39.html) review shows the Suprim X using *far* more power than the FE in multiple scenarios, including idle (38W vs 21W).\n\nThe other [review](https://www.techpowerup.com/review/msi-geforce-rtx-4080-gaming-x-trio/39.html) I was thinking of was for the *4080* variant of the Gaming X, which uses 2x the idle power of the FE (28W vs 14W).\n\nI mean, I can afford a 4090, so I can afford the extra electricity cost.  Still, I'd rather have it be as efficient as possible.\n\nEnjoy your card!"
    },
    {
      "author": "DHonor125",
      "body": "I don't remember using more than 30W idle but I could be wrong. But I know for a fact that while gaming it doesn't draw much, usually like 300ish, most of the time less because I usually cap FPS to the refresh rate (1440p 144hz). \nAs far as I'm aware the 4080 is very efficient compared to the 4090. But the 4090 is a whole different beast. I would've gotten a 4090 but the price difference here was about 700\u20ac."
    },
    {
      "author": "-Hovercorn-",
      "body": "Thanks for the reply.  Just by luck, I stumbled upon a 4090 TUF (non-OC) as MSRP and managed to snag it! Now to find a bigger case...\n\nEnjoy your card!"
    }
  ],
  "30": [
    {
      "author": "hyalimoe",
      "body": "Its not bad but it is definitely not good for a 4070ti. It is 3 generations behind. He will notice a big bottleneck with demanding games."
    },
    {
      "author": "Blackhawk-388",
      "body": "Nah. Not really. If a given game is particularly CPU bound, then less performance. But he will still get great frame rates. 3 Gens back doesn't make it wheezy 90 year old trying to run a Marathon."
    },
    {
      "author": "hyalimoe",
      "body": "I wouldnt personally pair a 4070 ti with a 10700k. Bottleneck and it doesnt make any sense. He can definitely do it and he will be happy but why not give the rig its full potential with a good matching cpu?"
    },
    {
      "author": "Blackhawk-388",
      "body": "Dude, it's not gonna fuckin bottleneck shit. That's still an 8 core, 3.8GHz base, 5.1GHz boost on 2 cores CPU. \n\nGames will likely come out that are CPU bound but he's not gonna have an issue at this time. Do some research before you say shit like that."
    },
    {
      "author": "hyalimoe",
      "body": "1st. He would get much higher frames with a much better cpu like a 12600k for example. Also reasonably priced right now. \n\n2. On demanding cpu title games his system def gonna bottleneck. \n\n3. If he has the money to buy a 4070 ti why not upgrade cpu to make the rig perfect? \n\nI know what im talking about im not trynna sound cocky or anything but just saying the truth and what will give him the best results. Also fix your attitude thats not how u talk to people brother."
    }
  ],
  "31": [
    {
      "author": "Phobos15",
      "body": "Eulas mean absolutely nothing.  They cannot bury something like this in a eula, that is not what a eula is even for."
    },
    {
      "author": "Sevicfy",
      "body": "Except they do and have been found they can be enforceable, either in part or as a whole, in court multiple times. The fact is however the EULA states they may update the software and the user is asked to read and accept the EULA prior to installation, complaining people don't know that they perform automatic updates due to their own fault in not reading such documentation and their naivete in how software typically works with regards to updates doesn't change that simple fact. Furthermore searching for \"NVIDIA Driver Update\" through Google brings up [this page](https://www.nvidia.com/en-in/drivers/nvidia-update/) right near the top of the list which, while outdated, does explicitly state that their update process includes \"automatic updates for game and program profiles\" and their control panel had once included easily found configuration to disable/enable it.\n\nThusly your argument that they are downloading updates, and specifically that of application profiles, without user knowledge is demonstrably false. It is not their fault people like you lack the ability to read agreements prior to using software or do simple research yourself and are now crying foul due to your ignorance.\n\nAnd while you're here crying foul over NVIDIA would it surprise you to learn that Discord has an automatic update system that runs at system startup too? And that it has no built-in options for the user to configure or disable it? And that unlike NVIDIA they have no mention of doing automatic software updates in their EULA (or seemingly elsewhere in any other document on their service)? And that this whole issue stems an automatic update they pushed out adding AV1 support that triggers a bug in the NVIDIA driver when probing for codec support? And that they also pushed out another small partial update of just a few files a few days ago to seemingly resolve this issue on their end for which I cannot find any announcement of? Instead of crying foul over NVIDIA why don't you go cry at Discord instead for their more egregious lack of transparency about their automatic updates?"
    },
    {
      "author": "Phobos15",
      "body": "And it does not have to be accepted by anyone, grow up.  No one is reading all of this just to learn about how much of a eula fanboy you are."
    },
    {
      "author": "Sevicfy",
      "body": "Where did I ever say I was a fan of EULAs? Pointing out the factual demonstrable truth that they inform users within their EULA prior to installation that their software may automatically update is not the same thing as defending the(ir) use of EULAs you know. Nor is pointing out that automatic software updates at system startup, be it informed or not, is common place throughout the software industry is not the same thing as defending the practice.\n\nYou can argue all you want about the enforceability of EULAs or that people don't read or accept them it does not change the undeniable fact that NVIDIA have informed users their driver software may automatically update and have explicitly informed users that this includes updates of application profiles. The whole point I am making is to show that your argument that they're downloading without the user knowing is just flat out false, and you are ignoring this because it doesn't fit whatever agenda you have here in trying to call them out over nothing and in typical \"You're right but I can't admit it\" fashion you are now pathetically saying \"no one is reading all that you fanboy\". Of course I doubt you will properly read or respond in any meaningful way to this comment either and it will just further show how foolish, ignorant and wrong you are here."
    }
  ],
  "32": [
    {
      "author": "Capt-Clueless",
      "body": "...what?"
    },
    {
      "author": "Koboldx",
      "body": "If you can understand german, i could give you a link to AMD driver problem News.\n\nIt happend 6 month ago, where AMD changed with the help of UEFI access, the Bios settings after a driver update issue (by mistake) and alot  AMD Users had unstable PC's after that."
    },
    {
      "author": "Capt-Clueless",
      "body": "My UEFI having AMD PC is fine. Nothing updated here."
    },
    {
      "author": "Koboldx",
      "body": "The issue was related to AMD GPU Drivers."
    },
    {
      "author": "Capt-Clueless",
      "body": "What does UEFI have to do with GPU drivers? GPUs don't have UEFI, and drivers are OS based."
    },
    {
      "author": "Koboldx",
      "body": "[https://www.computerbase.de/2022-04/amd-radeon-adrenalin-ryzen-auto-oc-bios/](https://www.computerbase.de/2022-04/amd-radeon-adrenalin-ryzen-auto-oc-bios/)\n\nI also just found the english version for you...:\n\n[https://www.tomshardware.com/news/amd-confirms-its-gpu-drivers-are-overclocking-cpus-without-asking](https://www.tomshardware.com/news/amd-confirms-its-gpu-drivers-are-overclocking-cpus-without-asking)\n\n&#x200B;\n\nIts also nice to be downvoted for telling the truth... thanks bro, very mature."
    },
    {
      "author": "Capt-Clueless",
      "body": "Wow, AMD has shitty drivers, what a shocker! We haven't known that for 20+ years or anything..."
    },
    {
      "author": "Koboldx",
      "body": "omg, its just show that even official drivers can change your Bios settings... thats the point, did you even read the article?"
    }
  ],
  "33": [
    {
      "author": "meathole",
      "body": "I run a 350 watt 3080 12gb and a 120 watt overclocked 3700x on a Seasonic GX-550. The most draw measured at the wall I\u2019ve seen for the entire system is 570 watts running a cpu benchmark and gpu benchmark at the same time. You will be fine on a decent 550 watt psu."
    },
    {
      "author": "blackcyborg009",
      "body": "Interesting.  \nQuick question:  \nWhat would be the Intel 13th gen equivalent of your AMD 3700x processor?  \n\n\nOn some websites, I heard it's being compared with Intel Core i7-9700k.  \nBut what would be its Intel Raptor Lake counterpart?"
    },
    {
      "author": "meathole",
      "body": "What do you mean by \u201cequivalent\u201d? Performance, power draw, what metric are you comparing?"
    },
    {
      "author": "blackcyborg009",
      "body": "Performance in this case"
    },
    {
      "author": "meathole",
      "body": "Basically every Intel thirteenth gen is faster in gaming"
    }
  ],
  "34": [
    {
      "author": "MichiganRedWing",
      "body": "Serenity now!"
    },
    {
      "author": "mariusmoga_2005",
      "body": "No comment, thanks for nothing man"
    },
    {
      "author": "MichiganRedWing",
      "body": "What's so difficult about asking from the place that you purchased from? If it were me, I'd only want an official answer, especially since we're not talking about some 50 Euro purchase here."
    },
    {
      "author": "mariusmoga_2005",
      "body": "Simple ... its Sunday, place is closed till tomorrow ... Like I said, created a support ticket on their system already, waiting for feedback. Whats up with this attitude?"
    },
    {
      "author": "MichiganRedWing",
      "body": "What attitude?  Good lord lol"
    },
    {
      "author": "mariusmoga_2005",
      "body": "Are you from Germany? Had a card bought from NBB? Any experience with their warranty? Did you offer any useful info?\n\nLooks to me you posted just to make fun, which is kind-of not helping at all"
    },
    {
      "author": "MichiganRedWing",
      "body": "> Are you from Germany? \n\nYes I am\n\n>Had a card bought from NBB? \n\nYes I have\n\n> Any experience with their warranty? \n\nNo I haven't\n\n> Did you offer any useful info? \n\nYes I did\n\n&#x200B;\n\nSch\u00f6nen Abend noch"
    },
    {
      "author": "mariusmoga_2005",
      "body": "Gleichfalls \ud83d\ude00"
    }
  ],
  "35": [
    {
      "author": "oishi1205G",
      "body": "Do you mean the 40 series? Man my 3080ti draws exactly 400w if i didnt undervolt...\n\nGuess I'll be getting 4080 sooner than i think for those up coming games. Especially those UE5 game\n\nBtw do you think 12700k can handle 4080?\n\nMB: aorus z690 pro\nRAM: Trident Z5 DDR5 6000mhz cl30\nPSU: Seasonic Prime Ultra 1000w (I'll order cable for 40series)\n\nI have check pc-builds just for bottleneck reference. Since like my cpu would be too weak for ultrawide 4K gaming but works great for 5K 5120x2160"
    },
    {
      "author": "notlikethis_wokege",
      "body": "Yeah my 4080 draws 130-200W depending on how graphically intensive the game is (there are some exceptions, such as 40-70W in indie games and 200-250 with max RT settings, but it's still less than my 3080). And yeah a 12700K will be just fine"
    },
    {
      "author": "oishi1205G",
      "body": "Alright good to know or else I'll be swap to 13700k as well as z790 and my ram kit, but thats whole pc setup basically.\n\nHope i could sell my 3080ti at a decent price that way i dont lose much LOL since i took very good care of my gpu"
    },
    {
      "author": "notlikethis_wokege",
      "body": "You can put a 13700k in your current mobo"
    },
    {
      "author": "oishi1205G",
      "body": "Ya for sure. I jist thought that if i swap my cpu i might as well do the same for mb cuz i have one in mind as the price was okay. Its the aorus z790 elite, ram up to 7600mhz i think"
    },
    {
      "author": "notlikethis_wokege",
      "body": "No point, it'd be a waste of money. DDR4 is perfectly fine still"
    },
    {
      "author": "oishi1205G",
      "body": "Oh im on ddr5 already so...but ya, z690 more than enough i know xD"
    },
    {
      "author": "notlikethis_wokege",
      "body": "Oh in that case you really dont need a new board lmao"
    },
    {
      "author": "oishi1205G",
      "body": "Ya i know. I was just \"thinking\" about it you know LOL"
    },
    {
      "author": "notlikethis_wokege",
      "body": "Lol I know the feeling"
    }
  ],
  "36": [
    {
      "author": "Background-Touch-744",
      "body": "Ah yes I have the Apple TV that came out in 2021. What\u2019s the setup process like, do I just plug in a hdmi cable from my gpu into my Apple TV then connect my HomePods to my Apple TV?"
    },
    {
      "author": "monsieur_beau19",
      "body": "Your TV needs to have an HDMI eARC/ARC input. You\u2019ll need to plug your ATV 4K into the ARC port, then plug the 3080ti into any other available HDMI port.\n\nIn the ATV settings, select Video and Audio, and under the audio section, change the audio output to make the HomePods the default audio output, and under the Audio Return Channel (ARC) Beta, select play television audio and make sure it says \u201cOn (ARC).\n\nLastly, make sure your TV audio sound output is set to HDMI ARC and you should be good to go."
    },
    {
      "author": "Background-Touch-744",
      "body": "I\u2019m using a monitor, so I plug the Apple TV into my monitor and the GPU into my monitor right?\n\nI can\u2019t seem to find if my monitor supports earc, I have the Samsung odyssey neo g9?"
    },
    {
      "author": "Background-Touch-744",
      "body": "Not able to find the ARC beta button under Video and Audio. I\u2019d imagine I need to install Apple TV beta for this?"
    },
    {
      "author": "monsieur_beau19",
      "body": "I don\u2019t believe the neo g9 has ARC so you\u2019re most likely out of luck there."
    },
    {
      "author": "Background-Touch-744",
      "body": "Hmm not able to find the answer to whether it does or not online \u2639\ufe0f"
    }
  ],
  "37": [
    {
      "author": "Pyke64",
      "body": "Not sure I get the downvotes when simply asking a question. But whatever, there are different types of lcd, not all are equal."
    },
    {
      "author": "vedomedo",
      "body": "Reddit logic I guess"
    },
    {
      "author": "Pyke64",
      "body": "I wish I owned the aw3423dw but my room is overly bright so a glossy oled is not a practical solution :("
    },
    {
      "author": "vedomedo",
      "body": "It's an insane monitor, stuff looks craaazy on it.  \nHonestly though, having a bright room per se is not the issue, it's more the fact of does the sun hit the screen directly or not. If you get sunlight directly on the screen it does look washed out, but ambient light in the room is completely fine."
    }
  ],
  "38": [
    {
      "author": "OkPiccolo0",
      "body": "If you can run 1440p native you can run 4K DLSS quality mode. It's the same resolution."
    },
    {
      "author": "Greennit0",
      "body": "That\u2018s not how that works."
    },
    {
      "author": "OkPiccolo0",
      "body": "Yeah that is pretty much how it works barring you don't run out of VRAM because of the 4K framebuffer. I have a 3080 10GB with a 1440p 240hz monitor right next to a 4K OLED TV. I am extremely familiar with how the technology looks and scales.\n\nEdit: Just tested it in Cyberpunk. 50fps native 1440p = 46fps 4K DLSS quality. \n\nThere's no way you'd have to pick 4K ultra performance if 1440p native is on the table. That doesn't make any sense."
    },
    {
      "author": "Greennit0",
      "body": "\"That is not how that works\" referred to your claim that 1440p native and 4k DLSS Quality will result in same fps, which you said yourself isn't true now."
    },
    {
      "author": "Greennit0",
      "body": "I didn't say you have to choose DLSS ultra performance to get the same fps as native 1440p.\n\nI said I'd prefer the image of 4k DLSS Ultra Performance to native 1440p on a 4k monitor, let alone DLSS Quality..."
    },
    {
      "author": "OkPiccolo0",
      "body": "Don't put words in my mouth. I didn't say same FPS. My statement stands.\n\n>If you can run 1440p native you can run 4K DLSS quality mode."
    },
    {
      "author": "OkPiccolo0",
      "body": ">native 1440p on a 4k monitor\n\nSo inherently that wouldn't be a \"native\" 1440p. That would be upscaled 1440p to 4K. \n\nI'm just telling you flat out that [1440p TAA/DLAA is better picture quality than DLSS 4K Ultra Performance.](https://imgsli.com/MTUyNTk3) Which makes sense, it's twice the resolution. \n\nYes always use a native resolution and upscale with DLSS, you are correct."
    }
  ],
  "39": [
    {
      "author": "IEnjoyElectric",
      "body": "Awesome thank you for the good news cant wait to use this beats. What are your pc specs?"
    },
    {
      "author": "megajawn5000",
      "body": "I play @ 4k/120 on a C9 \u2014 4090 will replace my 3080ti paired with a 5900x and 32gb of slowish ddr4 RAM.\n\nUndecided if I want to sell the 3080ti to offset the cost or keep and put it back in this PC, then build a new SFF PC with the 4090 and newer hardware.\n\nHow about you?"
    },
    {
      "author": "IEnjoyElectric",
      "body": "Damn that's a nice set up. Mine is almost similar to yours. My setup is 1440p/165hz, 3070 oc Strix replaced by 4090FE, 5800x3d, 32gb 3600mhz ddr4 ram. Going to replace monitor to 4k oled soon. Going to give my old gpu to my girlfriend for her set up. Also I sold my old rtx 3080 vision OC to help pay the 4090 FE. So, it just comes down to what you what to do honestly."
    },
    {
      "author": "megajawn5000",
      "body": "Dude 4k/120 OLED is the truth, gonna blow your mind compared to your 1440p/165 IPS panel.\n\nSince getting my OLED 3 years ago, my PG279QZ collects dust as an extra work monitor and I moved to a wireless \u201ccouch\u201d setup."
    },
    {
      "author": "IEnjoyElectric",
      "body": "Thanks man, I can't wait. Well I hope everything works out for all of us. Update me if you were able to get compensated for the coupon code and I'll update you to when my order comes and I got to bestbuy."
    },
    {
      "author": "megajawn5000",
      "body": "Will do brotha"
    },
    {
      "author": "IEnjoyElectric",
      "body": "\ud83d\ude4c"
    }
  ],
  "40": [
    {
      "author": "ThisPlaceisHell",
      "body": "You don't know what you're talking about. For real though.\n\nI'll enjoy my display that doesn't lose 75% of it's brightness if i display more than 25% window size, or have to worry about burn in on my desktop, or have inferior subpixel arrangement. Oh yeah, do you even know about that? OLEDs use weird subpixel layouts because of burn in. WOLED literally has a white subpixel to make up for burn in and how dim these screens are. QD-OLED use triad arrangements again because of different size subpixels for burn in resistance. This breaks ClearType and causes fringing on text. Some superior technology lmfao"
    },
    {
      "author": "Dolo12345",
      "body": "ClearType works fine here on C2.  You bring up more random issues no one cares about.  At end of day my C2 and 4090 LOOKS better than any non OLED display you have. The C2 is as bright as fuck that it\u2019s blinding, I don\u2019t want it brighter.  Don\u2019t care about QDOLED honestly atm, maybe down the road.\n\nMy miniLED MBP and iPad Pro goes past my 800 nits C2, but it\u2019s in no way better."
    },
    {
      "author": "ThisPlaceisHell",
      "body": "Lmao the C2 is blinding huh?\n\nhttps://www.rtings.com/tv/reviews/lg/c2-oled\n\nThe data says otherwise. Like I said you are \"blinded\" by 150-200 nits. That's a pathetic HDR experience. And VA MiniLED displays have practically no blooming whatsoever but can sustain well over 1000 nits, now THAT'S what HDR should be. Hell it won't be done until it can mimic the kinds of lighting conditions and color we see in real life. You ever step outside on a sunny day? Are you blinded by the cement sidewalks or glint off your car? Those are orders of magnitude brighter than the dim OLED you love so much friend: https://i.imgur.com/xefTpOv.jpg"
    },
    {
      "author": "Dolo12345",
      "body": "Again max nits of 800 not 150-200 keep trying troll.  800 and 1000 are very close for most people.  I own screens that do all levels.  The C2 is just fine.  Why the fuck would I want sustained 800 nits ever is beyond me."
    },
    {
      "author": "ThisPlaceisHell",
      "body": "Click the link and look at the data. It cannot hit the required scene brightness levels in ANY of the tests. The worst one, the landscape pool, comes in at 127 nits. Embarrassing showing. Keep denying reality. No one cares about 2% window size. Not even 10% matters. 25% minimum and above and this thing falls apart. I literally linked you to one of the most highly trusted review sources showing you the truth but you refuse to acknowledge it because you want to validate your purchase to an Internet stranger. Sad."
    },
    {
      "author": "Dolo12345",
      "body": "\u201cClick the link and look at the data\u201d is the shittiest way to prove something.\n\nYou put a link \u201cshowing me the truth\u201d and never referenced anything in the link ya dumb ass.  Try quoting what you referenced from the link :/"
    },
    {
      "author": "ThisPlaceisHell",
      "body": "The one thing we've been talking about this entire time, screen brightness, has it's own huge section right there.  Don't come to me calling it made it up when you refuse to look. All you've given me is anecdotes and subjective opinion. Those are worthless."
    },
    {
      "author": "Dolo12345",
      "body": "Because it\u2019s 100% subjective, watching a screen is an experience.  You tell me my screen isn\u2019t bright enough and I tell you it\u2019s already too bright when it does show 800 nits.  I tell you I have screens that get to 1500 and the C2 does good in comparison."
    },
    {
      "author": "ThisPlaceisHell",
      "body": "Do you not see the part where it only shows that brightness on a window the size of a dime? Do you not understand what window sizes mean? That's not how HDR is supposed to work. HDR is being able to display the full brightness of reality across a whole scene, not just some tiny speck in the distance. Any scene on a beach or in a desert, or just outdoors in general is going to fall apart and pale in comparison to a high quality MiniLED display that can push 10x the sustained brightness of your OLED at 50% or larger window sizes. Compare them fairly and objectively and you'll see.\n\nOr don't and continue this useless charade. I'm over sounding like a broken record with you. Have fun, I know I will."
    },
    {
      "author": "Dolo12345",
      "body": "Cool so it\u2019s brighter.  I already told you I have several miniLED devices.  My OLED still looks better than all my apple miniLEDs despite being brighter.  Display quality and HDR arent only bout brightness.  I\u2019m also watching in a dark room.\n\nFind me a miniLED TV or monitor that looks better (not just brighter\u2026) than my C2 and link it.  I already know there\u2019s better OLEDs."
    }
  ],
  "41": [
    {
      "author": "GruffChris",
      "body": "Itll be good for vr. Id say that card is the starting point to have a food experience. I had a 2080s and I believe the 3060ti is better than that."
    },
    {
      "author": "couchpotatochip21",
      "body": "Would grab a 6700xt but I have been told it's horrible for editing"
    },
    {
      "author": "couchpotatochip21",
      "body": "Ok, thank you\n\n3070s and 3080s aren't going for much used either :D"
    },
    {
      "author": "GruffChris",
      "body": "Well if you can get a 3080 for under 300 the  go for it!"
    }
  ],
  "42": [
    {
      "author": "TokeEmUpJohnny",
      "body": "Funny how the Ada cards are still missing from those lists."
    },
    {
      "author": "pidge2k",
      "body": "Wow good catch.  Let me get that fixed.  Thanks."
    },
    {
      "author": "TokeEmUpJohnny",
      "body": "It's been a day and still not fixed, but I hope you guys get to it, cheers :)"
    },
    {
      "author": "pidge2k",
      "body": "Web pages need to be reviewed whenever there are any changes but all Ada GPUs have Compute Capability 8.9."
    },
    {
      "author": "TokeEmUpJohnny",
      "body": "Awesome, cheers!"
    }
  ],
  "43": [
    {
      "author": "Mido50974",
      "body": "thanks"
    },
    {
      "author": "Revolutionary-Bar980",
      "body": "OP didn't cause that, that looks like sloppy factory soldering, Op noticed it while repasting the card and had the heatsink removed. You must be thinking of reflow soldering to fix defective PCBs,  Deb8auer broke his 4090 strix trying to reflow it."
    },
    {
      "author": "Mido50974",
      "body": "I read initial pasting is kinda crappy to be honest, the default pads seemed at least poor quality when I opened it, I originally did it when I switched to a 4k monitor, the temps were getting to high in memory and computer randomly shut down during gaming (it depended of the game but basically the more hungry the game the faster it shut down), it almost stopped after the paste with a 30\u00b0+ drop on memory temp (it still occurs on particulary demanding games, such as RDR2 when I go overkill on the graphic settings)"
    },
    {
      "author": "Mido50974",
      "body": "it seems so indeed"
    },
    {
      "author": "Mido50974",
      "body": "Whishing would be more appropriate \\^\\^"
    },
    {
      "author": "Mido50974",
      "body": "whew that felt toxic af, I had 118\u00b0+ on memory and nearly 100 on gpu, I may have a warranty (which i dont anymore) but I also may working from this computer and I also may not have a graphic chip on my processor which prevents me to just jump on the warranty and send the card away. for me to do that, I would have to buy a spare graphic card or another processor (u\\_U). OR I could just just attend a recognised temperature problem with the stock pads and fix it. we see things differently. you can replace the thermal pads without voiding the warranty  anyway you can find some topics on reddit about it. (I apologize for the english not my native language)"
    }
  ],
  "44": [
    {
      "author": "Djxgam1ng",
      "body": "And you choose resolution multipliers based on the resolution your gaming on\u2026.like, I don\u2019t have to figure out what it best??"
    },
    {
      "author": "Donkerz85",
      "body": "You go to Manage 3D settings then scroll down.\nLook for DSR factors and tick 1.78x and 2.25x and 4.0 times. The first two have DL next to then to highlight they are using Deep Learning.\n\nLeave your monitor resolution as native (unless game doesn't support full screen). You can now select these resolutions in game.\n\nEnjoy"
    },
    {
      "author": "Djxgam1ng",
      "body": "I already select 1440 in the game\u2026.and I want to say I can already enable DLSR in the game\u2026so would that mean that I have this enabled already. \n\nSide Note: In Control Panel, you can select the OS in general or you can actually select a separate program\u2026how do you know which is appropriate for any situation. Like, why wouldn\u2019t I select Cyberpunk2077 from the drop down menu? Or would that just enable this for that specific game?"
    },
    {
      "author": "Donkerz85",
      "body": "OS not program specific."
    },
    {
      "author": "Djxgam1ng",
      "body": "u/heatlesssun"
    },
    {
      "author": "Djxgam1ng",
      "body": "I can\u2019t remember, but is DLSR enabled in game or is just DLSS\u2026can\u2019t remember but I think I have one active but I also have other options\n\nI\u2019m gonna be on in a couple hours, would you be cool if I did a screen share in real time and maybe you can explain it to me (do it through text don\u2019t have to do it through voice chat)"
    }
  ],
  "45": [
    {
      "author": "ArshiaTN",
      "body": "Thank you!"
    },
    {
      "author": "ArshiaTN",
      "body": "Thank you! I guess I have to use NIS for old games if GTX 1080 cannot get 60fps in them. Let's see :D. The pc should arrive on friday. I am actually so happy to check everything, install games and etc xd"
    },
    {
      "author": "ArshiaTN",
      "body": "All good, I know it is not a real 4k card. I just want to get best picture quality out of it on my brother's 4k screen by upscaling 720p-1080p to 4k instead of running games in 1080p and let the tv upscale it."
    },
    {
      "author": "ArshiaTN",
      "body": "Thank you, I will install my favorite games for him and use 4k as option and whenever there is a FSR 2.1/  TAAU / TSR  (sadly dlss 2.5 isn't available for gtx 1080), I will put them on. Like this he just needs to start a game without configuring anything (he is disabled but can play with m&k).\n\nWorst case scenario I configure NIS in specific games that cannot run in 4k 60fps (his tv supports 4k 60hz) or doesn't have FSR 2.1. (I want him to play MGSV in 4k :D. I remember how I played it in 4k maxed out with 40-50fps on a gtx 1070. ah good old times)"
    }
  ],
  "46": [
    {
      "author": "NotYourSonnyJim",
      "body": "What Render Engine are you using ? They'll generally consume less power GPU rendering in say, Octane or Redshift, than they will in the highest-power games. I don't think you'll need to power limit the card at all for 3D rendring, I would just forget about that & go back to your cost/ power calculations."
    },
    {
      "author": "Dheorl",
      "body": "Trying to merge the results from the two videos linked, it looks like at the 245W at which the 4080 is producing 127fps in Remnant, with a quick back of the envelope calculation the 4090 wouldn't be much above 130fps, so certainly closing the gap compared to when they're both at 290-300W. I guess I'm curious to know whether that calculation works out in practise, and whether that gap just keeps getting smaller or eventually flips.\n\nI know I'm probably being silly over tiny differences, but hey, this is a PC sub, I would have thought that kind of curiosity goes with the territory."
    },
    {
      "author": "Dheorl",
      "body": "It's mainly cycles, although occasionally others.\n\nI came across this article:\n\n[https://www.cgdirector.com/rtx-4080-review-content-creation/](https://www.cgdirector.com/rtx-4080-review-content-creation/)\n\nIt mirrors what you say with regards to power draw being much lower than gaming, but I am wanting to run this as cool as and quiet as possible in a small case, so was planning on experimenting with going lower and seeing what effect it has. Seemingly the sweet spot for a 4090 purely efficiency wise in gaming is around 50%, and it would be interesting to know if this applies to a 4080 as well, putting it at 160W, and which is more efficient at both their respective peaks and at 160W.\n\nThe linked article seems to agree with my assessment of value with regards to performance per dollar, so the 4080 seems like the better choice from that point of view, but I guess I just like poking things and seeing what can be done, so I'm still curious about the comparison of the two."
    },
    {
      "author": "NotYourSonnyJim",
      "body": "Sadly, i think you'd have to buy both cards & run those benchmarks yourself to get such detailed data. I haven't seen any formalised testing.  \n\nGiven the (relatively) small price gap between 4080 & 4090, I'd go for the 4090 just based on the extra 8GB of VRam. I suppose it depends what kind of scenes you render. For product visualisation or something it might not be so important. Both the 4080 & 4090 have very over-built coolers in geneal, so I'd expect that for most models, temperatures & noise should be pretty reasonable. Overall, they consume less power than the 3090 & 3090ti did, even in most gaming workloads."
    },
    {
      "author": "Dheorl",
      "body": "I suspect doing that may outweigh the savings :)\n\nWhere I am the price is large enough to make the 4080 better value purely on performance. The extra 8GB VRam would be nice, but I'm used to working with only 8GB as it is, so 16GB will already feel like a luxury. I'm used to optimising scenes enough that I doubt I'll find that a limit, although much like with bags, I'm sure I'll fill whatever space I'm given with pointless rubbish.\n\nCooling wise its going to be in a mITX case, probably initially just as stock but hopefully in the future with a custom loop. Might be able to squeeze 2x280mm rads into my planned build, which I guess should do the trick, but as this is sometimes just going to be sitting in the corner of our open plan living area rendering away, it would obviously be nice to keep noise to a minimum."
    }
  ],
  "47": [
    {
      "author": "c47v3770",
      "body": "I meant hassle of selling the 1080 haha"
    },
    {
      "author": "No_Introduction_9537",
      "body": "Heheh. Yeah but that is usually no problems."
    },
    {
      "author": "c47v3770",
      "body": "My main concern is the cooling on an OEM card but I guess it\u2019s good enough for regular use/gaming? I would like to game at 1440p since it would be overkill for 1080p.."
    },
    {
      "author": "No_Introduction_9537",
      "body": "It should be ok. :)"
    }
  ],
  "48": [
    {
      "author": "Vehin77",
      "body": "Rolling back the GPU drivers to 528.02 seems to resolve this for me, I'd recommend rolling yours back as well until the next update"
    },
    {
      "author": "ilovetreesandbush",
      "body": "I\u2019ve actually already tried that and it didn\u2019t work for me thanks for the advice tho and I\u2019m hyped it worked for you ! I learned from nvidia tech support that when a driver update comes out you do not want to select express install you want to select custom and then tick the clean install box. This ensures that the old previous driver is completely removed and that you\u2019re only installing the newest driver. Hope that makes sense!"
    },
    {
      "author": "ilovetreesandbush",
      "body": "And btw did you roll them back by downloading the old previous beta version 528.02 from nvidia\u2019s website ?"
    },
    {
      "author": "Vehin77",
      "body": "I don't believe it was a beta version, it was just the previous public release, but yea I downloaded it through Nvidia's tool on their website"
    }
  ],
  "49": [
    {
      "author": "casual_brackets",
      "body": "Bruh reflex has been out for years it\u2019s nothing new, so if no cards but rtx 4xxx support Frame Generation and dlss 3.0 is just reflex + FG then only rtx 4xxx cards support dlss 3.0  \n\nWhat are you on about. Dlss 3.0 is only supported by rtx 4xxx. Backwards compatibility for games means that it can be added later\u2026\n\nIf you see dlss 3.0 running on an older card it\u2019ll be dlss 2.0 running in backwards compatibility.\n\nhttps://www.reddit.com/r/nvidia/comments/xjo42x/for_those_complaining_about_dlss3_exclusivity/?utm_source=share&utm_medium=ios_app&utm_name=iossmf&utm_term=link"
    },
    {
      "author": "CptTombstone",
      "body": "DLSS and Frame Generation are two independent technologies, you can turn off DLSS all together, if you wish to. If you enable both, then technically DLSS is done first, as Frame Generation works with two already generated, full frames (including UI) where's DLSS first lowers the render resolution, then the GPU renders the frame, then DLSS upscales to the target resolution, then the UI is drawn. (Simplified) Frame Generation then linearly interpolates between the last frame and the next frame (input latency impact comes into picture here) \n\nAs of the adoption, as far as I'm aware, an SDK for DLSS 3 (the new version of the whole tech stack including DLSS, Reflex and Frame Generation) is not publicly available yet, so only developers selected by Nvidia have access to it."
    },
    {
      "author": "CptTombstone",
      "body": "I don't know what is so hard to understand about this. Frame Generation and DLSS 3 are not equivalent terms. DLSS 3 has 3 features, Super Resolution, Frame Generation and Reflex. All RTX cards support super resolution and reflex, a subset of DLSS 3. Only Frame Generation is exclusive to Lovelace, so only Lovelace cards support the full extent of DLSS 3, just like Pascal only supported parts of DX12 (no async compute), yet DX12 games run just fine on Pascal cards, just not as fast as they could have been. This is the same with DLSS 3. You said DLSS 3 and frame generation are one and the same, which is simply not true. Just as a set containing the number 3 is not the same as a set containing all whole numbers. This is elementary level math."
    },
    {
      "author": "casual_brackets",
      "body": "https://www.reddit.com/r/nvidia/comments/xjo42x/for_those_complaining_about_dlss3_exclusivity/?utm_source=share&utm_medium=ios_app&utm_name=iossmf&utm_term=link\n\nAs you will learn: FG is a **necessary** component of dlss 3.0 without it, all you\u2019ll have is reflex+dlss 2.0 therefore dlss 3.0 is limited to cards which support FG.\n\nWhich brings us back to my original statement: if OP is using dlss 3.0 he is on a 4070 TI at minimum."
    },
    {
      "author": "CptTombstone",
      "body": "You are still treating DLSS 3 and Frame Generation as the same thing, but they are not. Bryan is talking about the Frame Generation feature there. Read the original article you linked if you are not clear on the terms still."
    },
    {
      "author": "casual_brackets",
      "body": "It\u2019s a superset. The three things you mentioned: FG, reflex, super sampling. If you can\u2019t do FG then you aren\u2019t using the full superset meaning you aren\u2019t using dlss 3.0. Just bc someone can activate reflex and dlss (deep learning super sampling\u2026)like we\u2019ve been able to do **for years** doesn\u2019t mean they\u2019re using dlss 3.0 to do that you must be able able to activate the full 3. If you can\u2019t do FG it isn\u2019t the full set it\u2019s not dlss 3.0 you\u2019d argue with Jensen himself I\u2019m out"
    },
    {
      "author": "CptTombstone",
      "body": "Your hardware can support only parts of DirectX 12, yet you can still run games using DX12, same in this case. DLSS 2 is outdated, newer games are implementing DLSS 3, like Portal RTX, yet people with RTX GPUs can still make use of DLSS, even if they can't take advantage of frame generation. The same is true with Windows 11, you might not have P and E cores or you might not have virtualisation on, but you can still use the OS."
    }
  ],
  "50": [
    {
      "author": "Reality_Check_101",
      "body": "If you weren't experiencing issues before with the same setup, rollback the drivers otherwise just wait."
    },
    {
      "author": "Emperiex",
      "body": "I was getting like 120-140 but this was before my 1440 monitor,,,. I bought 3dmark when it was on sale and it says it\u2019s performing above average and I should be getting ~115 fps which is a rare occurrence.\nOh well :("
    },
    {
      "author": "Reality_Check_101",
      "body": "Did you turn on the sync stuff for nvidia, like gsync and freesync and put the monitor on performance mode?"
    },
    {
      "author": "Emperiex",
      "body": "I\u2019ll have to check that out, only setting I touched is under the game display or whatever. I turned on the FPS preset because it increased brightness and stuff"
    },
    {
      "author": "Emperiex",
      "body": "Where do I turn that stuff on though?"
    },
    {
      "author": "Reality_Check_101",
      "body": "There should be some sort of performance preset for the monitor that you can toggle in the settings. As for the gsync/freesync, just right click on the desktop and go into the nividia control panel, gsync or freesync should be under one of those settings."
    },
    {
      "author": "Emperiex",
      "body": "Thanks man I\u2019ll check it out when I\u2019m awake"
    }
  ],
  "51": [
    {
      "author": "Platinumjsi",
      "body": "Interesting, I just unplugged my second screen which was 1440p and VR headset and its now showing the right values."
    },
    {
      "author": "gimpydingo",
      "body": "Is bugged. If you have multiple displays of different res it shows the highest. But in game the values are correct, so no need to unplug anything."
    },
    {
      "author": "Platinumjsi",
      "body": "Cheers I have multiple displays, this was working off the res of the lowest (1440p) rather than my 2160p screen"
    },
    {
      "author": "gimpydingo",
      "body": "You would think it am easy fix on Nvidia side. Enjoy DLDSR/DSR. I love using it with DLSS. God of War in 8k is glorious. Still glorious in 5k as well.  :)"
    }
  ],
  "52": [
    {
      "author": "wadimek11",
      "body": "Im not gonna clear cache but I will record it with msi afterburner. I have ultra smooth experience in both elden ring and dark souls 3. Im more curious if you gonna show console frametime at 4k 60 through because thats how I played the whole game which was snooth experience."
    },
    {
      "author": "L0to",
      "body": "All you have to do to clear the cache is move the folder and then move it back when you are done; it's not really a super valid test otherwise. Darksouls 3 always microstutters on my system with maxed settings every 5 minutes or so of gameplay so if you record that game even if you don't clear your cache it will likely happen. \n\nThe only question is if there is a difference in hardware that causes it such as drivers, cpu, etc. \n\nAnyway 10-15 minutes of dark souls 3 gameplay would likely be sufficient with a frametime graph. The stutters randomly happen during gameplay and will show up on the frametime graph if they are occurring. \n\nI don't have a great way to measure the frametimes on consoles objectively so I'm just going off my perception on that one and what little technical analysis I've seen from sites like digital foundry. Obviously that's not sufficient and I would like to have the resources to do more thorough analysis myself. \n\nIn Darksouls 3 on console there was obvious stuttering walking back and forth on the stairs to the tree boss which you will experience in the PC version as well, but that's different from what I'm talking about. On the PC version it also has a reproducible stutter while walking through the hole in the wall connecting the middle section of the jail; that stutter is non-existent on the console version. \n\nOtherwise running DS3 I didn't see any real slowdowns or framedrops that were particularly obvious through the entire game on a ps5. Elden ring ran very smooth in the ps4 version as well for the most part, primarily only experiencing issues during heavy rain and heavy physics effects. In both cases it would result in a temporary frame rate dip rather than stutter or dropped frames. I could see it during the tree sentinel boss when he would perform sweep attacks; the particle effects and particularly the physics of uprooting foliage in the surrounding area overwhelms the ps5's cpu resulting in an fps decrease. The vast majority of the game was a locked 60fps however and I'm very picky about these things. The decreases during rain were honestly more annoying but very intermittent.\n\nElden ring is upsampled to 1800p on the ps4 pro version, and dark souls 3 is only running in 1080 on a ps5 so the aliasing is pretty bad. \n\nI've never tried downsampling 8k because I never had access to a 4090 before. Currently I'm still on my 1070 computer as I'm building the new one and swapping parts around. Dunno if 8k downsampling fixes the aliasing, I will have to try it. 4k native didn't seem much better than 1080p surprisingly. \n\nIn Darksouls 2 1080p has horrible aliasing which 4k DSR or native basically fixes."
    },
    {
      "author": "wadimek11",
      "body": "Here you go. You can watch in probably an hour when youtube will process the 4k quality.\n\n[https://youtu.be/JFbdrvDVS\\_A](https://youtu.be/JFbdrvDVS_A)\n\nI can show you ds3 results later to but its smoother overall experience than elden ring."
    },
    {
      "author": "L0to",
      "body": "I appreciate you making the video but at around 3:47 there is a visible freeze in the gameplay. Unless this is an encoding error this is exactly the kind of stutter I am talking about.\n\nThat video with a freeze in the middle doesn't exactly disprove my point."
    },
    {
      "author": "wadimek11",
      "body": "Show me a recording of the console then that doesn't have it. I also runned it for the test on linux through proton and it didn't had those framedrops but it had 20% lower fps so with this 3070ti wasn't enough to make 60fps max settings. Either way they also happen on ps5 version and they don't exist in ds3."
    },
    {
      "author": "L0to",
      "body": "Doesn't happen on the ps4 version running on a ps5. Ds3 doesn't have the major stutter like that but still has periodic micro stutter. Cope, seethe, dilate. \n\nI can record footage later as I only have boss fights recorded currently, but none of my footage from the ps5 has stutter like that. \n\nKinda hilarious you set out to prove me wrong that the PC version has no stutter and you record a video with an incredibly obvious stutter in it."
    },
    {
      "author": "wadimek11",
      "body": "Your time to record and prove it now"
    },
    {
      "author": "L0to",
      "body": "I will do that within the next 48 hours. I can currently link an 11 minute video of the final boss that has no stutters but obqviously that doesn't have much area traversal.\n\nCurrently I don't have the game installed and am not at home so it will take me a little bit of time to make another video. I can link a few boss fights now if you want."
    },
    {
      "author": "L0to",
      "body": "https://youtu.be/mN-1J4hkfPo\n\nIgnore the fact I am playing like shit, the point is that the gameplay footage is smooth with no stutters.\n\nIf that isn't sufficient I can get more footage because the ps4 ver on a ps5 NEVER stutters. It only suffers slowdowns during heavy rain in a couple locations and during heavy physics when the cpu becomes overwhelmed which I only saw during the tree sentinel fight when he was uprooting trees.\n\nIf it doesn't stutter on proton, that's great but as you noted has a performance penalty. Proton includes both valve and the community dxvk fixes so it's entirely plausible this stops those major hitches. What this won't stop however is shader compilation stutter.\n\nTo see that all you would need to do is clear your shader cache (you can back up the existing cache first and restore it after so as not to lose anything,) create a new character and fight the tree sentinel. It will stutter the first time you fight the tree sentinel consistently under these conditions. \n\nA ps5 doesn't have this problem as it has fully precompiled shaders which is a huge advantage consoles have over PCs. It is worth noting that the steam deck also has fully precompiled shaders because of its nature as a fixed hardware platform, so it too doesn't have shader comp stutter either despite being a PC.\n\nI was actually more interested in your Darksouls 3 results because that game has microstutter that isn't shader related. Elden ring doesn't seem to suffer from microstutter other than from shader compilations; the other stutter isn't micro at all and is just stutter.\n\nThe Darksouls 3 stutters are harder to see with the naked eye on a video because they are a fraction of a second and happen once every 3-5 minutes or so. Obviously that isn't completely game breaking but it is an annoyance. I can't definitively say if every system suffers from this issue, but I know from talking to others who play the game that my configuration is not alone in dealing with it.\n\nHence my hypothesis is that everyone has the darksouls 3 microstutter and that those who claim to not experience it simply don't perceive it. I can't know this 100%, hence I would be legitimately curious to look at your footage of that game if you are absolutely convinced you have no microstutter."
    },
    {
      "author": "wadimek11",
      "body": "Where is frame time and fps then? I can prove there are no such stutters in ds3 as they happen in elden ring when shader loads. I can send you footage but Im currently in London I probably will post it on Wednesday"
    }
  ],
  "53": [
    {
      "author": "MarkusFATA",
      "body": "They are few and far between but they do exist and it\u2019s nice to see. I saw a guy who was buying 4090s left and right to test all the brands and see which one he was happy with. Any one he didn\u2019t want to keep he was selling for MSRP, paying for shipping, and would cover any warranty issues."
    },
    {
      "author": "FrodoTBaggins88",
      "body": "Yeah bummer the same warranty situation applies to that scenario and scalpers but I guess there\u2019s no way of knowing which is which. Fuck scalpers."
    },
    {
      "author": "MarkusFATA",
      "body": "I couldn\u2019t agree more. And it doesn\u2019t help when BB let\u2019s one person order 20+ 4090 FE\u2019s."
    },
    {
      "author": "FrodoTBaggins88",
      "body": "Yeah that\u2019s just madness to me. I tried for months to get a drop from BB. Never even managed to get one into my cart. I live in a small area with only 1 BB within 150 miles and they never saw a single 4090. I asked the employees in person and no one had ever seen one come into the store."
    }
  ],
  "54": [
    {
      "author": "CableMod_Matt",
      "body": "Thank you for the share! The 3x8 and 4x8 will both do 600w, but if you want that extra redundancy, the 4x8 does have the extra cable for that and is a great option for sure. :)"
    },
    {
      "author": "Panchovilla64",
      "body": "T2 which is a better version of the cable they linked me for amazon. I've seen mod flex and mesh do u have the link to which flex for my asus tuf 4090"
    },
    {
      "author": "CableMod_Matt",
      "body": "Gotcha, the ModFlex version we don't have on Amazon at the moment, just the ModMesh. That can be found here: https://www.amazon.com/dp/B0BJ29NLBL\n\nFull Kit: https://www.amazon.com/dp/B0BJ33J945"
    },
    {
      "author": "Panchovilla64",
      "body": "Thanks \u2764\ufe0f"
    },
    {
      "author": "Panchovilla64",
      "body": "Forgot to ask my tuf has 4 is it OK with 3?"
    },
    {
      "author": "CableMod_Matt",
      "body": "You're very welcome!"
    },
    {
      "author": "CableMod_Matt",
      "body": "Yep, fully safe with the 3, you still get redundancy with the 3x8 option. You only need our 2x8 option for full power in fact, even for 600w on the 4090 cards. :)"
    }
  ],
  "55": [
    {
      "author": "Johnson20041",
      "body": "well, here they are 650-1000\u20ac used"
    },
    {
      "author": "Kuli24",
      "body": "650 eh?  That's a decent amount cheaper than 1000+tax for the 4070ti."
    },
    {
      "author": "Johnson20041",
      "body": "The 1000 is with the tax, and it is 650 for a 2 year old used card.Got to keep in mind that in Belgium warranty rules are pretty good so getting a new card in that perspective better imo"
    },
    {
      "author": "Kuli24",
      "body": "Interesting!  I bought a used evga 3080 a few weeks ago and it still has a year of warranty left.  Gotta love evga."
    }
  ],
  "56": [
    {
      "author": "Broder7937",
      "body": "There's really no way of knowing and it's likely Nvidia themselves hasn't set in on a price yet. Reasonable (if you can even use such a word) pricing would be $1999 for the 90 Ti and $2499 for the TITAN. Remember, the 4090 itself happens to be very reasonably priced given what it offers. But it's unreasonable to expect anything \"reasonable\" at this point."
    },
    {
      "author": "Low_Air6104",
      "body": "yeah that\u2019s kinda what im getting at. pretty sure the board is starting to learn just how big the hobby has gotten recently, and as a result how much they will pay"
    },
    {
      "author": "Broder7937",
      "body": "GPU sales are at a record low right now (you can Google it), and the reasons for that couldn't be any more obvious."
    },
    {
      "author": "Low_Air6104",
      "body": "yeah crypto slump recently. but nvidia has already shipped more than 200k 4090s (which is the top of the line product you are saying wouldnt sell) so lately it has picked up A LOT. will be interested to see what their next quarterly report shows. they are selling like hotcakes. the hobby has grown and there are more big spenders in it now."
    }
  ],
  "57": [
    {
      "author": "Rngade85",
      "body": "it definitely does. And thanks for the recommendation. I'll look into it!"
    },
    {
      "author": "DuneKlide9",
      "body": "That ain\u2019t bad"
    },
    {
      "author": "DuneKlide9",
      "body": "Got myself them steel series arctic one wireless ones"
    },
    {
      "author": "DuneKlide9",
      "body": "Headphones are the only thing I\u2019ll use wireless, I struggle enough with the input latency"
    }
  ],
  "58": [
    {
      "author": "eoL-methoD",
      "body": "That\u2019s a cap\u2026 youtube or it didn\u2019t happen."
    },
    {
      "author": "cycle_you_lazy_shit",
      "body": "https://overclock3d.net/reviews/power_supply/corsair_hx850w_850w_atx_psu/4\n\nGet rekt I guess, lol. Read it and weep."
    },
    {
      "author": "eoL-methoD",
      "body": "And that\u2019s Your results? Joke\u2019s on you"
    },
    {
      "author": "cycle_you_lazy_shit",
      "body": "Hahahahaha. Nice one."
    }
  ],
  "59": [
    {
      "author": "exclaimprofitable",
      "body": "Yes, and they are limited on driver not hardware limit, so there is no possible work around to this limit. No displayport splitters etc work, because the card still detects how many displays it needs to be running, and turns off any display that is over the 4 limit.\n\nOnly way to get around it is if the OP has a cpu with an IGPU, you can run the 5th monitor off the motherboard then, not the gpu."
    },
    {
      "author": "NOLA_2023",
      "body": "Got it, thanks.\n\nMy current GC setup is:\n\n1. GC HDMI port is connected to a ceiling projector (Monitor 1)\n2. (Monitor 2) - connected to GC 1st DD port and Mirrors Monitor 1\n3. (Monitor 3) - connected to GC 2nd DD port\n\nSince I am Mirroring Monitor 1 with Monitor 2, can I purchase a HDMI splitter to use\n\nfor Monitor 1 & 2, freeing up a GC DD port?  Then I will have 3 GC DD ports available\n\ngiving me a total of 5 monitors, your thoughts?\n\n&#x200B;\n\nScott"
    },
    {
      "author": "exclaimprofitable",
      "body": "It can't be an hdmi splitter, because it will still present 2 monitors to the computer, meaning that the gpu will see that it has 5 monitors connected.\n\nIt has to be some kind of signal duplicator, so the computer never knows that there are actually 2 monitors in that hdmi port. idk, maybe some splitter has that functionality, maybe not. If your motherboard has an hdmi port try that first."
    },
    {
      "author": "NOLA_2023",
      "body": "NVIDA support just sent me the following comments:\n\n\" If  you connect the splitter to any port then you can set up two displays  in that splitter only in duplicate mode. It will replicate the exact  image on the second display connected to the splitter. \"\n\n&#x200B;\n\nYour thoughts?"
    },
    {
      "author": "exclaimprofitable",
      "body": "I mean if the splitter has duplicate mode and the computer only sees one monitor, it will work as i already said.\n\nI looked on amazon and many splitters say that they work in that \"duplicate\" mode.\n\n> Splitting one HDMI input signal into two HDMI output signals identical to the input signal. \n\nsplitters that do that will work fine"
    },
    {
      "author": "NOLA_2023",
      "body": "Great, thank you for concurring."
    },
    {
      "author": "NOLA_2023",
      "body": "How can you make the HDMI port on the back of the GC  Monitor 1, in windows 11?  I have 4 displays, 1 HDMI (projector), 3 PC displays (DP). Win11 will assign the HDMI port. Display 2, and the 1st port (DP) away from the HDMI port on the back of the GC will be assigned. display 1, the 2nd DP will be display 3 and the 4th DP will be Display 4. Your thoughts?"
    }
  ],
  "60": [
    {
      "author": "bushytas",
      "body": "I've been wondering if its worth replacing the fan with  Accelero Xtreme IV.\n\nonly issue is i can't find anyone that stocks them"
    },
    {
      "author": "n19htmare",
      "body": "That's a custom card for Acer prebuilts. [Based on this thread](https://linustechtips.com/topic/1361227-weird-2070-blower-card-from-a-acer-prebuilt-can-you-guys-help-me-find-a-aftermarket-cooler/), the PCB is pretty much an exact copy of the Zotac RTX 2070 Blower and the Zotac RTX 2070 OC Mini.[Based on this video](https://www.youtube.com/watch?v=8DyvxCqn-bg) of the Zotac RTX 2070 Blower, It's pretty much the same blower (because the PCB is an exact copy).\n\nYou can disassemble and look at any markings on the fan, usually there will be a sticker with more identifying information.\n\nAlternatively, you can measure the fan  but recommend looking at the sticker on the fan. To me it looks like a 65mm as they're common on midrange blower style cards but don't hold me to it.\n\nIf it's 65mm Search  for PLB06625B12HH\n\nIf it's 75mm, search for PLB07525B12HH\n\nThey're all over ebay/aliexpress."
    },
    {
      "author": "n19htmare",
      "body": "It's a Zotac based on pictures of PCB and coolers. [https://linustechtips.com/topic/1361227-weird-2070-blower-card-from-a-acer-prebuilt-can-you-guys-help-me-find-a-aftermarket-cooler/](https://linustechtips.com/topic/1361227-weird-2070-blower-card-from-a-acer-prebuilt-can-you-guys-help-me-find-a-aftermarket-cooler/)"
    },
    {
      "author": "n19htmare",
      "body": "No. Just change the fan. I provided info in my other post. Fan shouldn't cost more than $15 AU shipped (Aliexpress). No point in spending more money on that card."
    },
    {
      "author": "bushytas",
      "body": "Just brought one thanks"
    }
  ],
  "61": [
    {
      "author": "Sniper_One77",
      "body": ">Gameplay (before):  \n>  \n>https://youtu.be/\\_CoYMpIA5x0  \n>  \n>Gameplay (after):  \n>  \n>https://youtu.be/M98W\\_GSPmvg\n\nHard to see how much FPS you gained before vs after. Where is the average FPS value?"
    },
    {
      "author": "OwlyEagle-",
      "body": "I haven\u2019t shown any 3dmark prior of testing.\n\nThe two videos are where i benchmarked in realtime gaming performance (for my self).\n\nDifference can be seen there \u2014 3dmark result was as reference for the results and not the numbers prior as mentioned in the image description of both."
    },
    {
      "author": "OwlyEagle-",
      "body": "After recording, I realized shadowplay doesn't show the built-in performance overlay I saw while playing, unfortunately. FPS is still displayed top left, but hard to tell I guess. Anyways, I have adjusted some timings and now averaging well above 300, sometimes hitting frames close to 400. The 0.1 & 1% lows improved strongly as well. Will probably have to record a new one; do you have any idea how I can get nvidia perf overlay to show up in shadowplay or do I need the msi one?"
    },
    {
      "author": "Sniper_One77",
      "body": "I use only MSI afterburner + RTSS overlay and OBS to record. I have used shadow play in the past, but never with the overlay so I couldn't comment. Plus shadow play records in variable framerate, it gives audio de-sync issues in editing software. Hence I didn't use it that much.\n\nI now see you have played in 1080p res, which means CPU bound scenario and RAM OC can give you this boost. 3Dmark scores show the results are in 1440p, so the difference in score seems negligible as the load is more on GPU."
    }
  ],
  "62": [
    {
      "author": "RodroG",
      "body": "What's your PSU model? How did you connect the board? I only use prefer max performance in NVCP on a per-game profile basis, and I have never experienced any black screens or BSOD issues on idle, doing desktop tasks, or during gaming or full load stress tests."
    },
    {
      "author": "RashGambit",
      "body": "SF750. \n\nIt\u2019s connected directly to the pcie slot, set as gen 4 in the bios. Was originally connect with separate pcie power cables from the psu into the nvidia 12vhpwr adapter, now using the genuine Corsair 12vhpwr cable. \n\nWhen I put the 3080 back in the issues went away too, so it rules out potentially damaging the mobo when changing cards. \n\nIf I turn off prefer max performance it will black screen when launching just about anything, it isn\u2019t gpu load dependant. \n\nI can only speculate it\u2019s some kind of conflict somewhere with the motherboard bios, gpu bios and the driver."
    },
    {
      "author": "RodroG",
      "body": ">I can only speculate it\u2019s some kind of conflict somewhere with the motherboard bios, gpu bios and the driver.\n\nMmm, it can be. Have you any updates available for your UEFI-SBIOS and VBIOS?\n\nI'm sorry about what's happening to you, mate, honestly. :/ The 4070 Ti is a good board, and you should be able to enjoy it. I hope it can be fixed soon."
    },
    {
      "author": "RashGambit",
      "body": "Motherboard bios is up to date, hasn\u2019t had a new one available since last year. Hopefully Asus will continue to support the board as it\u2019s a Strix B550i, not a cheap one. I can\u2019t find that any 4070 ti bios updates have been released. \n\nIt\u2019s still a great card and other than MW2 DMZ I don\u2019t get crashes using prefer max performance. I\u2019m just not a fan of the card clock and memory speeds being pegged at max all the time."
    },
    {
      "author": "RodroG",
      "body": ">I\u2019m just not a fan of the card clock and memory speeds being pegged at max all the time.\n\nI agree. The card should work without issues using the Normal power management mode, even during gaming workloads. I hope it gets fixed."
    }
  ],
  "63": [
    {
      "author": "RegularFries",
      "body": "Your current PSU will work just fine, I have a 4070ti paired with a 5800x3d powered by a 650w PSU and have experienced no issues."
    },
    {
      "author": "Otherwise_Pitch_4645",
      "body": "Thank you... Whit my 3070 ti I play in monitor at 120hz 1440p and my tv 60HZ 4K... And I never hame any problem.... I wish with the 4070 ti I have the same result... I never monitoring the consumption of watts \ud83e\udd14\ud83e\udd14"
    },
    {
      "author": "Otherwise_Pitch_4645",
      "body": "Thank you ! I have R7 5800x ... I think consumes less than the R7 5800X3D \ud83d\ude05"
    },
    {
      "author": "RegularFries",
      "body": "Enjoy your new GPU!"
    }
  ],
  "64": [
    {
      "author": "MonitorShotput",
      "body": "... Can you even comprehend what I wrote? A CPU bottleneck is when your GPU can't hit 100%. NuMbErS HiGhEr doesn't matter for real world use. If someone is asking about if x will bottleneck y they don't mean if it is theoretically possible to do so using tests designed to bottleneck. They mean if they are playing a game at their chosen resolution, will it bottleneck them before they hit their chosen refresh rate.\n\n1% lows aren't showing a bottleneck, they show microstutter. Microstutter can be virtually eliminated by using a variable refresh rate display. Also, 60% increase in 1% of frames is what exactly? Barely a difference at all, considering the average framerate includes that 1% and is still only 10% off.\n\nI'm not arguing that a 1680v2 is old and outdated, I'm arguing that the performance difference you are implying is exaggerated at best and, in the case of OP's original question, misleading at worst. All those benchmarks show is that my CPU will bottleneck before a 9900k bottlenecks, which is obvious. It doesn't show that it is bottlenecking at 1440p 60Hz gaming with modern games."
    },
    {
      "author": "Mr_Octo",
      "body": "You have no idea what you\u2019re talking about. You got a 3080 for 1440p60? What the fck man.\nTests designed to bottleneck??? Do you mean REGULAR GAMING?\u2026\nI\u2019m done. You\u2019re a clown."
    },
    {
      "author": "MonitorShotput",
      "body": "Lol, okay. No point explaining real life to a child anyway."
    },
    {
      "author": "Mr_Octo",
      "body": "No point in explaining how computers work to a guy who thinks microstutters can be negated by using variable refresh rate on a monitor. Wtf dude for real. \ud83e\udd26\ud83c\udffb\ud83e\udd26\ud83c\udffb\ud83e\udd26\ud83c\udffb"
    }
  ],
  "65": [
    {
      "author": "EmilMR",
      "body": "Dont overclock more. It can do 2ghz easily but the cooling and vrm on this card are limited. The fans are just very cheap and rattly even brand new."
    },
    {
      "author": "tony359",
      "body": "Really? Marketing usually works the other way round! \"1755Mhz when running outdoor, while snowing and while dancing on your tiptoe\"!\n\nIt feels a bit weird that they set it to 1950 but then marketed at 1755 in case someone decides to run it under a blanket. \n\nAnyways, if this is expected, then I rest my case! :)"
    },
    {
      "author": "tony359",
      "body": "I'm not even thinking of overclocking LOL! The opposite, I was trying to see if I could cut the temps.\n\nWhen I got the card I was getting 86 degrees C after 4 minutes of Furmark, after lapping and re-pasting I get 76 degrees after 4 minutes and it then settles at 79 degrees (fan set to 75% on both tests).\n\nHere some pics: [https://forums.overclockers.co.uk/threads/lapping-gpu-heatsink.18966036/](https://forums.overclockers.co.uk/threads/lapping-gpu-heatsink.18966036/)\n\nI managed to open the fans and \"service\" them, they're are a bit less \"rattly\" then when I got it!\n\nWell that must explain the price I paid it. Still, it's a 2060 and does what it says on the tin.\n\nI'll look into a slight undervolt then. Thanks again for confirming."
    },
    {
      "author": "EmilMR",
      "body": "Yeah. You put more effort into this than gigabyte. With that said the cards has lasted 4 years and counting and I have never opened it to service it. I guess they did a good enough job."
    },
    {
      "author": "tony359",
      "body": "Indeed! :)"
    }
  ],
  "66": [
    {
      "author": "Miloapes",
      "body": "Yeah I can understand that, mines like 50cm away. Does it look weird when it\u2019s pulled closer then?"
    },
    {
      "author": "iamRBT",
      "body": "Tbh i did that and its still uncomfortable! I am sure screen size is my problem"
    },
    {
      "author": "iamRBT",
      "body": "Ha, not an option :))"
    },
    {
      "author": "iamRBT",
      "body": "I gave it 1 year ! I feel thats enough time ! I simply cannot , but thx for suggestion"
    },
    {
      "author": "iamRBT",
      "body": "Closer than that, and i miss a lot of info on map also minimap."
    },
    {
      "author": "Miloapes",
      "body": "I felt it was a bit difficult at first too too see the minimap but after a while I got the hang of it. Saying that I am a bit shit with only a 1kd, if you play competitively I can see it being more of an issue."
    },
    {
      "author": "iamRBT",
      "body": "Basically, i need to know if its an overkill with my system\n Play 1080p when now the standard has migrated to 1440p . And i read that 3080ti performs better on 2k better than fullhd"
    },
    {
      "author": "Miloapes",
      "body": "Yeah it\u2019s defo a waste of a 3080ti should I say. Have a look around see if you can find anything that fit your needs. I checked and there is hardly any 1440p 24 inch monitors"
    },
    {
      "author": "iamRBT",
      "body": "Exactly my point :) , i searched quite a lot before posting here"
    }
  ],
  "67": [
    {
      "author": "FrezlyGod",
      "body": "Thanks,u2! Lemme know how ur asus tuf treats you?"
    },
    {
      "author": "Moznomick",
      "body": "I will and I can't wait to pick it up soon."
    },
    {
      "author": "Moznomick",
      "body": "Hey thought I'd give you a f/u as I got my card and it's been amazing. Played cyberpunk 2077 with ultra settings and no coil whine, thank God for that. The card has been a pleasure and the upgrade was definitely well worth it. Hope you have the same blessing with yours."
    },
    {
      "author": "FrezlyGod",
      "body": "Thats amazing bro! Im happy to hear everything worked out for you. I got my MSI card, I ended up getting a MSI Suprim X 4080 since I got a huge deal on it. Normally its 1699\u20ac here and i got it for 1300\u20ac which is almost the same as a 4070ti suprim. The card itself is so amazing, fans make no sound and it keeps like 40 degrees at load which is mad, but it has same coil whine as the others.:/ maybe even a bit louder im ngl. Which brings me to the next chapter: I have decided to upgrade to AM5 from my AM4 platform. I figured ''It cant be so that I try out four different 40-series GPUs and they all give me the same coilwhine?''. It isnt my PSU either or nothing like that, so I figured maybe theres something going on with 40-series and AM4 and upgrading to am5 with dd5 and new mobo + cpu will give me different results, maybe they are more compatible and that will reduce the coilwhine since its a combination of things that results in coilwhine? Anyways, even if it doesnt fix the coilwhine issues i've been having,I got the am5 upgrade with a better cpu and better ram and mobo from what I have for a total of about 250euro(with selling my old parts), which isnt that bad considering its an upgrade on my rig and gives me the ability to upgrade further in the future and not being stuck on end-of-line am4. xD another adventure. Hopefully this might fix it??? What parts are you on? Do you run intel or am4 or 5?"
    },
    {
      "author": "FrezlyGod",
      "body": "By the way, do you have a multirail or single rail psu?"
    },
    {
      "author": "Moznomick",
      "body": "Sorry to hear about the coil whine and yeah that is odd for 4 different ones to all coil whine. I'm on intel and running a 10700k 32gb 850watt single rail psu. It's my first build which I did during covid back in January 2021 and no major problems. The only issue I've had is that the power button doesn't work so after testing my components and being able to short the power, I rma'd the psu and I still have that issue. Having the power switch cable replaced, which should be today.\n\nBoth my 3070 & 4070ti have had no issues running though and no coil whine either."
    },
    {
      "author": "FrezlyGod",
      "body": "Okay man! I was already thinking about swapping to a multi rail psu from single rail, but i dont really know if that makes any difference, maybe i have just been very unlucky xD I have the rm1000x corsair one, which is brand new. Im glad for you though! Very good to hear it worked out, im sure it will for me too with patience! :)"
    }
  ],
  "68": [
    {
      "author": "CirillaVaCintra",
      "body": "Alright, thank u mate. That will do."
    },
    {
      "author": "xnrnx",
      "body": "No problem it made it hard for me to choose an upgrade from a 2080s. First I was going for the 4070 ti did tons of research. Then decided I needed more memory and switched to a 4090 did lots of research, almost bought one then stopped. 2k is alot for a $1500 card. So I side stepped to 4080 seeing as how it's best for 4k60."
    },
    {
      "author": "CirillaVaCintra",
      "body": "Those 4080 are ridiculous expensive. I had to build myself a whole pc and bought me a ultrawide screen aswell, no way I am gonna pay this much for a card now. But if u have the money. Enjoy it must be blast to play on it."
    },
    {
      "author": "xnrnx",
      "body": "I just upgraded gpu. My rig is still good thank god."
    }
  ],
  "69": [
    {
      "author": "ArkanElvis",
      "body": "I did get supported clocks to output a list, but it maxed out at 1291 core and 1500(3000 effective) for mem. I have to wonder if a 1060 mobile bios could be modded to reduce the amount of VRAM to 4gb from 6GB.  Then flash it to the card manually."
    },
    {
      "author": "NERBORUTO",
      "body": "probably yes. In the past I've mod quadro k600 bios to unlock extra 1gb that was actually on the card. I have never increased or reduced the number of memory channels. In my opinion, however, this is not the correct way to proceed, it is better to raise boost and memory limits and see what the tdp limit is in your bios."
    },
    {
      "author": "NERBORUTO",
      "body": "0002ad11 8502 = 0285 (645x2) = 1290mhz --> 2a03 = 1620mhz\n\n0002ad54 154A 8502 =1290mhz --> a80c 2a03 = 1620mhz\n\nthis unlocks Max Customer Boost Clocks up to 1620mhz  at about 1.0v\n\nshould be a safe setting for mobile cards as well...\n\nabove 1670mhz voltages start to rise significantly and therefore also power draw.\n\nfor memory speed it would be the case to at least know what mounts this gpu. if they are real 7ghz you can set them like this.obviously this also increases overall power draw and overheat ."
    },
    {
      "author": "ArkanElvis",
      "body": "So the stock MSI 1060 6gb is a 100w mxm 3.0b card and was an upgrade option for my laptop.\n\nhttps://eurocom.com/ec/upgradepricelist(2,357)MSI_GT70_0NC\n\nSo, the stock specs for the GPU speeds ought to do fine.  Memory speeds are questionable but stock 1060 mobile is 4ghz effective.  \n\nhttps://www.techpowerup.com/vgabios/214132/214132"
    },
    {
      "author": "NERBORUTO",
      "body": "assuming you can flash bios of 1060  with ram changes it doesn't unlock anything. the card will continue to be called p106 and if there are any limitations in the drivers you will still have them. I suggest you to mod  your bios. the 11.6w limit is strange.. perhaps only unlocking the mox boost clock could be enough."
    },
    {
      "author": "NERBORUTO",
      "body": "the maximum limit of the 1060 is 137w while the target limit is 78w...\n\nto make bios mod you need cheap spi programmer ch341a and the pliers to hook on bios chip or you have to solder wires."
    },
    {
      "author": "ArkanElvis",
      "body": "Well if you think modding the current bios is best, then that is what I can work towards.  I have ran the TDP tweaker and set the target and max TDPs.  I am working on getting the eeprom programmer.  Not sure how I could unlock max boost clock in the bios.  I have tried a hex editor but I am not good enough to make heads or tails of the hex code."
    },
    {
      "author": "NERBORUTO",
      "body": " first you have to mark the checksum of original bios then modify those 2 offsets that I gave you with hexeditor. then fix the checksum. the tdp only mod it if the card does not reach 1620mhz. I don't think 11.6w is the real tdp limit. probably the limit field is empty."
    },
    {
      "author": "ArkanElvis",
      "body": "Still need to write the bios with programmer?"
    },
    {
      "author": "NERBORUTO",
      "body": " to make flash you need the ch341a spi programmer with pliers or wire solder direct to the  spi chip"
    },
    {
      "author": "ArkanElvis",
      "body": "Ok. I'll have to toy around with it when I have time"
    }
  ],
  "70": [
    {
      "author": "Kuli24",
      "body": "Well maybe.  But we're still 9 series and 3080 bros."
    },
    {
      "author": "Graywulff",
      "body": "Yeah, it still is fast enough for games but I wonder if the i5 is a bottleneck for the 3080.\n\nI had a 5700xt that broke twice, the second time they gave me a new one, the warranty was up in a month and they were going for 1300 on eBay so I sold it\u2026 waited for gpu prices to crash and then bought the 3080 and did limited gaming on my laptop.\n\nI kind of wish I sold the motherboard cpu ram combo then bc with microcenter deals I could get a 13th gen i7 with a z6xx board for what the i5 combo was going for in march. So the computer just sat there unused.\n\nMight have gone to 32gb of ram at the same time.\n\nI mean even if I sold the z390 now, with what I made on the 5700xt I could still get a new board and cpu and ram and still be on top.\n\nThe joke of it is the price of ether crashed pretty soon after I sold the 5700xt and then it was only worth $220! The miner bought it to make ( a day thinking 5x365 not taking into account how much price fluctuates. So I doubt they made their investment back.\n\nI\u2019m putting money in my retirement account when it\u2019s something I don\u2019t need and selling stuff rn bc the market is down so I need to \u201cbuy the dip\u201d.\n\nI bought nvidia stock and it\u2019s up 38%! I wish I\u2019d spent the whole 3080 budget on nvidia stock now."
    },
    {
      "author": "Kuli24",
      "body": "hoho nicely done on the nvidia stock!\n\nThe i5 will definitely bottleneck the 3080 especially at less than 4k.  I mean my 9900kf bottlenecks my 3080 sometimes even at 4k.  But the fps is still high enough.  Also just recently cranked it to 5ghz all cores, so that helps."
    },
    {
      "author": "Graywulff",
      "body": "Yeah, so I kind of want another motherboard, but I know I should buy more nvidia stock.\n\nThe reason it\u2019s doing well is bc they built one of the first full stack deep learning system.\n\nI think I should put more in that."
    }
  ],
  "71": [
    {
      "author": "eugene20",
      "body": "No it's a holdover from every day someone posts problems with a card swap and then posts back again that the advice they were given to use Wagnardsoft's DDU to wipe worked.  \nAnd the first part of your first paragraph is only correct for the very short time after a cards release when there has not previously been a driver that supports both the old and the new generation.   \nAfter that release lots of people on older hardware update to that newer driver too, then if they get a new card they swap them over and the driver works as it does technically support both, it's just they very often have strange problems that only go away after DDU and then a clean install of the driver."
    },
    {
      "author": "jzltk",
      "body": "You've quite rightly pointed out that the driver package (amongst families of GPU, geforce for example) is the same so when you upgrade the GPU the act of installing the new driver is the just the same as upgrading the driver which people do all the time without using DDU.\n\nSo yes, you're exactly right :)"
    },
    {
      "author": "jzltk",
      "body": "Your suggestion doesn't make sense though, since you need to have both drivers installed if you have multiple GPUs. How are you suggesting you manage multi-GPU systems?\n\nThe first part of the first paragraph makes perfect sense, even if I install the latest drivers for the 3090 on a clean system that won't install the drivers for the 4090, it only installs the drivers for the GPU(s) in the system."
    },
    {
      "author": "eugene20",
      "body": "(edited above for clarity)Same deal with multi gpu systems, if you want them to work optimally you DDU wipe and install the latest driver for your newest model card after both cards are in the machine.\n\nPerformance might not be as good on the older card as the older drivers that were only written with the older card in mind, but that should be less of a concern than getting the most out of the newer card .\n\nIf  one of the cards is a workstation card (eg Quadro) then I believe you need the separate specific driver for that too, they won't work for both."
    },
    {
      "author": "jzltk",
      "body": ">Performance might not be as good on the older card as the older drivers that were only written with the older card in mind\n\nNo that's not correct. What you are installing is a driver \"package\" which comes with drivers for a whole heap of different devices. If you install say the 527 driver package on a system with a 4090 and a 3090 installed you will get 2 drivers installed, a 527 version of the 4090 driver and a 527 version of the 3090 driver. There's no reason to suggest the newer driver for the 3090 will perform worse than that older driver for it.\n\n&#x200B;\n\n>If of of the cards is a workstation card (eg Quadro) then I believe you need the separate specific driver for that too, they won't work for both.\n\nRight, which is why you don't remove the driver. I have to install the Geforce driver package which will install the driver for the 4090 and then the workstation driver package which will install the driver for the A6000 because the system needs *both* drivers (one for each different device). There's no reason to remove the 4090 driver first because then I just have to reinstall it anyway."
    },
    {
      "author": "eugene20",
      "body": "I made a semi educated guess as to why clean installs fix problems because you asked.  \nAs for performance degradation of newer drivers on older cards, it does happen, it's not usually a lot, and sometimes later drivers get it back, you can find people that do driver benchmarks in this sub.  \nHave a good day, bye bye."
    },
    {
      "author": "jzltk",
      "body": "I can kind of see the point of removing an old driver before installing a new one.\nFor example if you have a 3090 installed with driver 514 and then add 4090 and install the 527 driver package it will install the 527 driver for the 3090 and the 4090.\nNow you could run DDU and remove the 514 driver before installing 527 but if that was important then wouldn\u2019t you do that on every driver upgrade?"
    }
  ],
  "72": [
    {
      "author": "rubenalamina",
      "body": "You already did different configs but if it was me considering a new build, I'd be interested in something like 4080 + 13600k and DDR5. this gives you a fast card, fast CPU and fast memory regardless of what you do with your monitor. If price is an issue, I wouldn't even consider the 4090, best to get an all around great PC build for the next few years than to cheap out on CPU and RAM just for getting a 4090. I'm not sure how the 7600X stacks vs the 13600k but see if it's competitive, that's a good option then.\n\nI'd definitely consider upgrading the monitor in the near future once you put the build together. The AW3423DW or F is fantastic and the visual upgrade with OLED and HDR for gaming and content is great.\n\nYour current build seems to be capable so getting a monitor instead is tempting but I think you would benefit more from making a new build AND upgrade monitor by years end, for example. If you don't or can't afford to upgrade it, keep your build and get the monitor. You will get a pretty significant visual upgrade and can always make the build you want later as well."
    },
    {
      "author": "Ok_Calligrapher4811",
      "body": "Thanks very much. Looks like I underestimated the difference Raptor Lake / Zen 5 make! That's ruled out a couple of the options!"
    },
    {
      "author": "Ok_Calligrapher4811",
      "body": "Thanks very much for taking the time to give me your thoughts!  I decided to go with something similar to your first suggestion (I suppose you'd call this more balanced than plonking a 4090 into a system that's not really ready for it!) but I wasn't able to get a 4080 FE. As a result the 4080s I could purchase were not that far from 4090 territory so I reconsidered.\n\nAfter lots of thought (and after seeing DLSS3 do its thing in CP2077 and DL2) I ended up pulling the trigger on a new AM5 build with the 4090 as the jewel in the crown. It was a fair bit more than I'd budgeted for but should last me a good while and means I'll not be constrained in any way should I decide I'd like to move to a 4K monitor later. Unfortunately it also means I'm probably on washing up duty for the next 6 months. \n\nComponents arrived this morning, haven't been this excited in ages! Hoping the build goes well and with any luck I should be chippin' in with Johnny and the gang with RT enabled tomorrow night!"
    },
    {
      "author": "rubenalamina",
      "body": "Nice. Enjoy the new build."
    }
  ],
  "73": [
    {
      "author": "Rhythm_and_Brews",
      "body": "If it's used w/o warranty, EVGA 3080.\nIf it's new w/factory warranty, MSI 3080. \n\nEnjoy :)"
    },
    {
      "author": "EzeqSaldu",
      "body": "Just wanted to let you know guys, i bought the MSI Ventus 3x OC 3080,thank you for your help!"
    },
    {
      "author": "EzeqSaldu",
      "body": "It's new with factory warranty,how's MSI customer support? Is it good even for third world countries?"
    },
    {
      "author": "Rhythm_and_Brews",
      "body": "TBH I have no idea how MSI support is outside of the U.S.A.. I only suggest them because EVGA is no longer making GPUs and would be concerned that they would not be able to support you in 2+ years if you have an RMA issue."
    },
    {
      "author": "EzeqSaldu",
      "body": "That's a really good point,guess i'll buy the MSI then. It's like 275.000 AR$ (~720 USD),there's barely any stock here and less around that price, unless i want to pay like a 100 USD more which i don't have currently and i don't want to wait because of how unstable is the currency here.\n\nThank you."
    }
  ],
  "74": [
    {
      "author": "Hellgate93",
      "body": "A friend of mine had one of those, but with 2 instead of 4gb. That was getting a bottleneck for him, but the card died to him anyway and upgraded to a 1070."
    },
    {
      "author": "_jul_x_deadlift",
      "body": "Rest in frames brother sad to hear it"
    },
    {
      "author": "_jul_x_deadlift",
      "body": "Love to hear it"
    },
    {
      "author": "_jul_x_deadlift",
      "body": "Yeah I have the 2gb version. I'm looking to upgrade to the Palit 3060 12gb since I only have a 550w psu"
    }
  ],
  "75": [
    {
      "author": "alanoid164",
      "body": "Might want to check how much clearance you have between the side fan mount and the back panel. MSI website lists the radiator and fan to be 55mm thick. With that in mind, I managed to fit that behind my side fan mount on an 011 xl, despite the user manual reporting only 50mm of space between the back panel and the side fan mount, and have lian li unifans to cover the radiator in the front.\nIf you do prefer to change the radiator fans, you may need to adjust the fan curve due to the default fan curve accommodating zero rpm fans."
    },
    {
      "author": "Benji2108",
      "body": "Thanks. Ok so if I did horizontal or vertical mount I still want the GPU rad behind the intake fans on the side. Am I understanding you correctly? What exactly do I need to do? \nI have the vertical mount but I\u2019ve also seen the push pull config with the GPU rad in the back behind the 3 intakes. I think one guy said he got slim 15mm arctic fans to swap out the stock ones on the GPU rad to make it sandwitch together. \nMind explaining the best way to mount this rad if I\u2019m going for a white build?"
    },
    {
      "author": "Benji2108",
      "body": "Got it! Swapped the GPU rad fans with 15mm arctic skins and it worked and fit perfectly. https://www.reddit.com/r/lianli/comments/10oybh9/build_complete/?utm_source=share&utm_medium=ios_app&utm_name=iossmf"
    },
    {
      "author": "alanoid164",
      "body": "Nice! Just curious, were the stock fans to thick as well to fit behind the side fan mount?"
    }
  ],
  "76": [
    {
      "author": "spajdrex",
      "body": "There is also https://reshade.me but it's not recommended for MP games (can't work due to DRM protection mostly). But it's faster and more configurable than freestyle."
    },
    {
      "author": "boarlizard",
      "body": "where are you seeing the options for sharpen plus? When I alt f3, all I see is the sharpening slider tied to image scaling"
    },
    {
      "author": "spajdrex",
      "body": "Game need to support it (sharpen+) and not all of them do."
    },
    {
      "author": "boarlizard",
      "body": "Ahh, that makes sense, thanks for the info"
    }
  ],
  "77": [
    {
      "author": "CableMod",
      "body": "Which psu are you using ? We offer a wide range of replacement cables to reduce the cable mess in your system."
    },
    {
      "author": "yogurtshooter",
      "body": "EVGA G2 1000. I just purchased one from your Amazon store yesterday\n\nThe \n\nCableMod Basics E-Series 12VHPWR PCI-e Cable for EVGA G7 / G6 / G5 / G3 / G2 / P2 / T2 (Black, 16-pin to Triple 8-pin, 60cm)"
    },
    {
      "author": "CableMod",
      "body": "That\u2019s the correct one - thank you for your support - hope you will enjoy it."
    },
    {
      "author": "yogurtshooter",
      "body": "Yup should be here tomorrow. Just from the looks of it I hope is much better than the OEM one."
    },
    {
      "author": "CableMod",
      "body": "It will be - if you encounter any issues then feel free to DM me."
    },
    {
      "author": "yogurtshooter",
      "body": "Great thanks!"
    }
  ],
  "78": [
    {
      "author": "piter_penn",
      "body": "DLDSR may look better not everywhere and not anytime. 4k will do it everywhere and every time."
    },
    {
      "author": "w00t692",
      "body": "Dldsr down samples a larger res onto the monitor.  Really it's similar to super sampling. An option I wish more games gave you.\n\nIt also does some anti aliasing on top so they say dldsr 2.25 looks similar to dsr 4x.\n\nIn my opinion it doesn't..but it does look way better than dsr 2.25x."
    },
    {
      "author": "piter_penn",
      "body": "It does nothing to aliasing, it is just a side effect of downsampling."
    },
    {
      "author": "w00t692",
      "body": "Dldsr absolutely does... That'snearly the entire difference between it..it has a more intelligent way it deals with the picture so it does anti aliasing better than dsr when it down scales."
    }
  ],
  "79": [
    {
      "author": "spajdrex",
      "body": "That's possible, but since I flashed Suprim X bios on my Ventus 3X OC model I can confirm it's been raised to 365W.\n\nIt's also mentioned here  \n[https://www.overclock.net/threads/official-nvidia-rtx-4070-ti-owners-club.1803155/](https://www.overclock.net/threads/official-nvidia-rtx-4070-ti-owners-club.1803155/)  \n\n\nAnd also here if you click on all three BIOSes Details  \n[https://www.techpowerup.com/vgabios/?architecture=NVIDIA&manufacturer=MSI&model=RTX+4070+Ti&version=&interface=&memType=&memSize=&since=](https://www.techpowerup.com/vgabios/?architecture=NVIDIA&manufacturer=MSI&model=RTX+4070+Ti&version=&interface=&memType=&memSize=&since=)"
    },
    {
      "author": "Xiievilz",
      "body": "Thank you saved me from spending on a new PSU"
    },
    {
      "author": "Xiievilz",
      "body": "Does that mean increase in performance if i get it to 365? Or should i use it normally with pigtail ?"
    },
    {
      "author": "spajdrex",
      "body": "Unless you plan to do heavy overclock it will be hard to reach 350+W power usage. Otherwise 2x8 PCIE would be enough as PCIE port itself bring 75W to the card."
    },
    {
      "author": "Xiievilz",
      "body": "So each pcie cable provide 150w which equal 300w and with pcie port providing 75w means total of 375w which is more than enough for 365w required right? Or it comes with risks? Also i dont overclock i hate it because of the artifacts i get and the spikes also better be stable rather than non consistent"
    },
    {
      "author": "spajdrex",
      "body": "Yes, it should be enough. Enjoy your card :-)"
    }
  ],
  "80": [
    {
      "author": "nssoundlab",
      "body": "Dlss and frame generation is 2 different technology... We were have dlss1 then dlss 2.x and we we have dlss3 ygat is dlss2. X and FG? Nvidia marketing.... Still dlss do not need glu scheduling... Frame generation needs... I'm not sure what you're not understanding... And real confusion will be if someone will post ygat have issue with dlss 3 but instead of freak dlss with FG he will mean new library for dlss 3.1.1... Frame generation is separate technology... It is not even upscaler... It is interpolation."
    },
    {
      "author": "Spork3245",
      "body": "To be \"DLSS3\", by NVDA's naming schemes, means it has frame generation. It's NVDA's name for if a game has FG or not, whether you agree with their naming system or not isn't exactly relevant,"
    },
    {
      "author": "nssoundlab",
      "body": "\ud83d\ude02"
    },
    {
      "author": "Spork3245",
      "body": "I'm sorry your attempt to needlessly troll the OP didn't go well."
    }
  ],
  "81": [
    {
      "author": "spacev3gan",
      "body": "Are you sure they restock *\"all the time\"*? I live in Finland and I have not seen those models for at least two weeks. Granted in Germany your chances should be better. But I am skeptical you can simply find those MSRP Asus TUF almost at will.  \n\n\nBesides, windows idle should be 0% fan speed, as the GPU has almost no work to do. Why are you manually setting it to 30%?  \n\n\nAnyway, if you are certain it is not coil whine related, then yeah, try to get your money back and search for another model."
    },
    {
      "author": "MaxxPlay99",
      "body": "Already tried it with Afterburner \n30% is bare minimum. That\u2019s around 1100 RPM."
    },
    {
      "author": "MaxxPlay99",
      "body": "I set the fan speed to 30% while idling to make sure that the noise isn\u2018t coil whine. And it goes away after reaching around 50%. But because the cooler is so overbuild the fans actually never get to 50%. (only with a custom fan curve) And they are loud. Unexpected loud for 50%. \n\nGerman stock is quite good tbh. \n\nhttps://i.imgur.com/l1cIzHO.jpg \n\nThe TUF is already available again but not for 899\u20ac so I\u2018ll wait a bit more. \n\nhttps://i.imgur.com/K69exDJ.jpg"
    },
    {
      "author": "spacev3gan",
      "body": "You might have gotten a faulty card. What you are describing doesn't sound like normal behavior. I don't think you necessarily need to get the TUF. I think any card - even another KFA2 one - should perform better. But if you want the extra peace of mind, yeah, the TUF is one of the highest rated designs there are."
    }
  ],
  "82": [
    {
      "author": "Secths",
      "body": "4K is selectable without DSR on my PC. And all the options for DSR are above 4K, I think that's because I have a 2K monitor."
    },
    {
      "author": "gimpydingo",
      "body": "What's the make and model?"
    },
    {
      "author": "Secths",
      "body": "MSI GS273QF"
    },
    {
      "author": "gimpydingo",
      "body": " Hmm being 1440p not sure why you would see 4k as on option without dsr turned on.  Very odd."
    },
    {
      "author": "Secths",
      "body": "Right? I don't understand it at all I have a beefy GPU, but I don't think that's why."
    }
  ],
  "83": [
    {
      "author": "BuckieJr",
      "body": "Gotchia I\u2019ll leave it be lol.\n\n I\u2019ve never noticed any latency in the game on pc but I certainly do when playing on Xbox. I get frustrated trying to aim on Xbox that I end up just moving to my computer to play lol. \n\nThis one instance was just way out of character for the game for me and the only difference was I turned on frame generation to see how it worked. It definitely works, I mean 4k RT hitman 3 is a hell of a demanding game and it doubled the fps I was getting. But frame generation only being useful in games that can already get 60 fps makes this kind of a useless technology for me.  When I was getting 120fps and turned it on, I could hardly tell it was on but when I was getting 30fps in the freelance basement with rt on and all those reflections.. it was unplayable, literally. It was nice to see it double the fps and the game looked smoother, but planning the camera and it taking half a second to match my mouse was hell lol. \n\nIf I\u2019m only getting 30fps and turning it on creates input lag that makes using a mouse nearly impossible, what good is the feature? In like the premise of it, but I think it has some work to go still."
    },
    {
      "author": "heartbroken_nerd",
      "body": "By the way was Reflex turned on? Because it really shouldn't be nowhere close to half a second of latency. Maybe it's just some rare case where Frames Generation craps out that's fixable by driver update in the future."
    },
    {
      "author": "BuckieJr",
      "body": "It was on yes, but just on, no boost. \n\nWhen I\u2019m home from work come morning I\u2019ll load the game up and make a quick YouTube video showing just how long it takes for the game to react to mouse movement for me. \n\nI\u2019m sure it\u2019s just a my system issue and I have some work to figure it out but yeah, it\u2019s a fairly long time for the game to react to mouse movement with frame gen on in the freelance base for me."
    },
    {
      "author": "heartbroken_nerd",
      "body": "Could be a game issue just as well."
    },
    {
      "author": "BuckieJr",
      "body": "See we spoke here as well lol"
    }
  ],
  "84": [
    {
      "author": "Snydenthur",
      "body": "From what I've seen, 3090ti is better in both RT and raster above 1440p (although, at 1440p, it's negligible with that \\~1% lead).\n\nAnd, I don't really count frame gen as an advantage, since I personally don't really see a situation where one should use it.\n\nLike I said, 4070ti is still the \"better\" one here though, since it provides that full warranty and smaller power bill at pretty similar performance numbers."
    },
    {
      "author": "heartbroken_nerd",
      "body": ">And, I don't really count frame gen as an advantage, since I personally don't really see a situation where one should use it.\n\nHow about: LITERALLY ANY GAME that suffers from CPU bottleneck while your GPU has available resources but cannot do anything with them? Enter DLSS3 Frame Generation."
    },
    {
      "author": "Snydenthur",
      "body": "And add input latency. As long as it has an actual downside, it's not for everyone.\n\nAt best, it's like a sidegrade."
    },
    {
      "author": "heartbroken_nerd",
      "body": "\"Latency\" is such an interesting issue to have. You know?\n\nGiven the fact that DLSS3 (which includes Reflex, mind you) is in most cases the same or even lower latency than Native resolution without Reflex, do you consider Native resolution to be unplayable then? I mean, not all games have Reflex and some game engines genuinely have higher latency than DLSS3 gives you anyway, so how do you even make sense of that?\n\nI guess we've all been playing so many games with unplayable latency at Native resolution, eh?"
    },
    {
      "author": "Snydenthur",
      "body": "Normally, going for higher fps lowers latency. In the case of frame generation, not only do you stay at the same latency as your base fps (whether it's native or dlss), it also adds some latency on top of it.\n\nYou love frame gen, good for you. But not everyone wants to increase their latency."
    }
  ],
  "85": [
    {
      "author": "ETHBTCVET",
      "body": "I wouldn't think 8700k being not enough until 2030, it seems like an immortal CPU."
    },
    {
      "author": "docace911",
      "body": "Yeah got this killer Asus 42\u201d and now can do 4k but cpu keeping me from 138hz"
    },
    {
      "author": "docace911",
      "body": "Right? Your 9900 slightly faster than mine. Well I can do DSR 8k titanfall2 but still 100FPS on my 8700k"
    },
    {
      "author": "docace911",
      "body": "Yeah at 60fps it\u2019s ok but was kinda shocked no matter how low I turned forza graphics was still the same frame rate \n\nHoping I can hit 138hz on my oled with the 13700k/6400"
    }
  ],
  "86": [
    {
      "author": "Capt-Clueless",
      "body": "OP games at 4k and wants high frame rates since they have a 120hz display. No card is lasting them 5-10 years."
    },
    {
      "author": "BigManRN",
      "body": "True. maybe I skip this gen."
    },
    {
      "author": "BigManRN",
      "body": "I might stick with the 6900xt the more I think about it. I can always lower render res or screen res to 1440p."
    },
    {
      "author": "BigManRN",
      "body": "I live in Texas so it\u2019s not the worst."
    }
  ],
  "87": [
    {
      "author": "cycle_you_lazy_shit",
      "body": "Interesting blanket statment there, mate. I am a competitive minded player. Usually top 1% in whatever skill distribution of whatever game I play. \n\nI'm a competitive amateur though, not a professional esports player, and I did make that clarification that if OP is infact a professional player, then yeah fair enough, 1080p 360hz or maybe even higher makes sense. If you're like me though, 1440p 240hz is a way better option. It's still very fast, but it also doesn't look as bad as 1080p does. The difference between 240 and 360hz for a non-pro? Probably not a big deal. Again, I say this as a very competitive player myself."
    },
    {
      "author": "Historical_Article_2",
      "body": "I do agree now I've been investigating on people's suggestions and the i7-13900K is a better choice for gaming thx9"
    },
    {
      "author": "cycle_you_lazy_shit",
      "body": "Yeah obviously if OP already has the monitor well they're already bought into that. \n\nJust out of interest, are you maxing your GPU out in the above titles you've mentioned? Are you seeing 99-100% GPU load when in game?"
    },
    {
      "author": "Historical_Article_2",
      "body": "What do you get on low settings as I get around that now with my 2080ti on low"
    },
    {
      "author": "Historical_Article_2",
      "body": "I've had a 27\" before I got the 24.5 360hz and it's the smoothest monitor I've ever had and the best size for my gameplay, \nI'm not a professional player but an avid fps player for many years and even though I do come top in most matches (especially since the new monitor) \nmy pc is ageing now over 3 years old and even though I'm happy somewhat playing with what I've got I thought It's time to upgrade a few stuff without going to big but would like more fps if I can and maybe a bit more competitive edge through faster hardware (less latency etc) plus I have some money spare and have the upgrade bug after 3+years\nThanks for replying and anymore info appreciated"
    }
  ],
  "88": [
    {
      "author": "geodek69",
      "body": "This is the kind of performance increase you can gain with an overvolt curve or overclock on this card (It's about a 6.5% increase in the graphics score below). I don't see how you can gain any performance benefit from an undervolt curve other than getting better temps and using less power (efficiency as you say). If that's what you're shooting for, then more power to you, or should I say less power to you...lol \ud83d\ude01\n\nhttps://preview.redd.it/2nog8h5zlsea1.jpeg?width=2880&format=pjpg&auto=webp&v=enabled&s=e31e34360a04a37c162a9b37d0255863e37f3ab5"
    },
    {
      "author": "Smerfcy",
      "body": "Just wanna add I ran another set without obs and the undervolt fps was ~90."
    },
    {
      "author": "Smerfcy",
      "body": "Average was 76 according to the summary metro gives you at the end."
    },
    {
      "author": "Smerfcy",
      "body": "Do you mind running these two benchmarks again with the demo part at the beginning?\n\nI\u2019m hitting 3ghz with no added power. But the demo section doesn\u2019t fully utilize the gpu all the time like GPU test 1 and 2 do."
    },
    {
      "author": "geodek69",
      "body": "After last night, I have about 2 dozen test saved in the 3D Mark server, most of which are public but I get what your saying with the under-volt curve and it works great with my old 3080 Ti and 3090 cards cause they put out sooo much heat that it would choke most setups and throttle limits would inhibit the performance of our systems. This isn't the case with the 4070 Ti except if maybe your in a tiny form factor even smaller then my R13. Also, our test will never be the same except if we had the same exact hardware. We'd only be able to compare percentages lost or gained.\n\n\"Undervolting generally only increases performance in two related circumstances, either the chip is power limited (usually only a thing in laptops/small integrated devices) or it's thermal limited.\" The 4070 Ti is low power, low temp and efficient right out of the box so you'll only be getting lower temps and using less power with a undervolt curve unless your in one of the conditions mentioned in the quotes.\n\nUpdate: I ran the TimeSpy again and got a 22823. I love this card!!!\n\n&#x200B;\n\nhttps://preview.redd.it/e2i3ssfb0wea1.png?width=3840&format=png&auto=webp&v=enabled&s=4b7bc206fc4d27958b56af8821b41267bf5d6392"
    },
    {
      "author": "Smerfcy",
      "body": "So, my 4070 ti\u2019s vbios has a hard limit of 1.1v so I can\u2019t increase power limit above 100%. But, I did play with the curve and got ~5.5% more performance with a very slight undervolt. It\u2019s only ~30 watts less than stock, but I\u2019m hitting a bit above 3ghz, instead of 2760. \n\nhttps://i.imgur.com/LXQUs9e.jpg"
    },
    {
      "author": "geodek69",
      "body": "Congrats, I'm playing Dead Space. It's amazing how good this game looks and feels in 4k at over 100 fps...lol"
    }
  ],
  "89": [
    {
      "author": "r1y4h",
      "body": "Did I lie? why are you triggered?  OP is asking for gpu models and obviously Gaming OC is the least premium. It\u2019s literally in the bottom 2 of Gigabye\u2019 offerings"
    },
    {
      "author": "roguehypocrites",
      "body": "Not triggered, was just curious if there was another reason why you suggested to not get it"
    },
    {
      "author": "r1y4h",
      "body": "You should probably have asked that earlier. 4070 ti Gaming OC fans at 1300rpm is loud at least in my experience with a mesh filled case. Usually premium cards are quieter at a similar fan speed. That\u2019s the reason to go with premium cards."
    },
    {
      "author": "roguehypocrites",
      "body": "I see. That makes sense! I guess I got lucky with mine because I can't really hear the fan much at 1300 rpm. It also on 0 rpm on idle, so no noise there."
    },
    {
      "author": "r1y4h",
      "body": "your case must have more sound proofing. I have asus prime ap201 case so it\u2019s easier to hear fan noise."
    },
    {
      "author": "roguehypocrites",
      "body": "Maybe but only for fan noise. I have the p500a BTW. I had a zotac aero extreme that had a very high-pitched motor sound. Still could hear that through the case no matter what. After returning and getting a gigabyte, I feel much happier."
    }
  ],
  "90": [
    {
      "author": "BlackDeath3",
      "body": "Wouldn't that imply that price/performance is equal with the 4080?\n\nBy what metric is the 4090 the \"only one worth buying\"?"
    },
    {
      "author": "Capt-Clueless",
      "body": ">Unless you have a Scrooge McDuck money pick and truly don't care about cost I would suggest going with the 4080 or 7900xtx\n\nIf you're buying a 4080 or 7900XTX, you can buy a 4090."
    },
    {
      "author": "Capt-Clueless",
      "body": ">Wouldn't that imply that price/performance is equal with the 4080?  \n>  \n>By what metric is the 4090 the \"only one worth buying\"?\n\nYou answered your own question.\n\nWhy buy 4080 when the 4090 is 33% faster and provides the same price/performance?"
    },
    {
      "author": "BlackDeath3",
      "body": "It also costs more, not just up-front, but also in terms of power required to drive it.\n\nIf it truly has the same power/performance as the 4080 then that's great, but why does that make it \"the only card worth buying\"?"
    },
    {
      "author": "Capt-Clueless",
      "body": ">It also costs more, not just up-front, but also in terms of power required to drive it.\n\nThe power difference is irrelevant. Show me the math if you believe otherwise.\n\n>If it truly has the same power/performance as the 4080 then that's great, but why does that make it \"the only card worth buying\"?\n\nBecause \"next gen\" typically brings a \\~33% performance improvement over current gen. Why buy a 4080 if you can have \"next gen\" performance compared to it NOW?"
    },
    {
      "author": "BlackDeath3",
      "body": ">The power difference is irrelevant. Show me the math if you believe otherwise.\n\nDon't really feel a need to prove anything to some random dude on Reddit. TDP difference is a consideration to make, and one I made.\n\n>Because \"next gen\" typically brings a \\~33% performance improvement over current gen. Why buy a 4080 if you can have \"next gen\" performance compared to it NOW?\n\nKind of an odd way to look at things, and I'm not even sure if that percentage difference actually holds true.\n\nDo you understand why the $400 difference between cards might matter to somebody more than the performance gain? Can you envision such a circumstance?"
    },
    {
      "author": "Capt-Clueless",
      "body": ">Do you understand why the $400 difference between cards might matter to somebody more than the performance gain? Can you envision such a circumstance? \n\nIf you can't really afford either card in the first place and are financially stretching yourself just to buy a 4080, sure."
    },
    {
      "author": "BlackDeath3",
      "body": "It's not necessarily about financially stretching yourself (though for some it probably is), it's about looking at the cost/benefit of what that extra $400 gets you, and deciding that it either is, or is not worth it. Unless you've got cash coming out your ears, there's always going to be something else that you can put that same money towards.\n\nIn my case, that extra $400 gets me a card that may well start causing me power draw issues on 850W (and maybe it doesn't, but if it does then I've got to manage that now), not to mention it isn't as readily in stock as the 4080 is. Is it going to be a bottleneck for my other hardware, even more than the 4080 may be? If I find that I don't really need the extra room offered by the 4090 right now, then why not just put that $400 into the RTX 6XXX fund? And so on. Lots of questions one can ask of themselves.\n\nNow, I'm not saying that my decision to buy a 4080 is any better a decision than buying a 4090, my point is that there are enough factors at play that it's quite reasonable for somebody to make a decision either way. There's no \"one card worth buying\" for all people - that's silly."
    }
  ],
  "91": [
    {
      "author": "CptTombstone",
      "body": "Maybe so, in terms of DLSS Frame Generation support. In games where it's supported, it's a game changer, easily doubles performance with minor drawbacks. \nAlso, the Lovelace architecture has 12 times the cache compared to Ampere, which means vastly superior performance in some games like Call of Duty Modern Warfare - the 4070 To is 20% faster than the 3090 there. \nAlso, the TSMC 4N node is 2-3 generations ahead of the Samsung 8nm node Ampere was made on, thus for an equivalent performance, you get much much lower power consumption. You can undervolt the 4070 Ti at 0.9V and it will just sip power well under 200W, while the 3090 is a 350W card at similar undervolt. Put a modest +500 overclock on the VRAM on the 4070Ti, and it will perform better than stock even with the undervolt. \nIt's really hard to recommend any 3000 series cards now, unless Lovelace cards are out of your budget completely."
    },
    {
      "author": "ItsMZB",
      "body": "Sounds good, Ultrawide isnt confirmed, but I do like the idea as currently on a standard curved centre then off left coding style screen"
    },
    {
      "author": "ItsMZB",
      "body": "Im on a medium spread of settings as even though its running low, if I increase the settings the FPS drops while the card doesnt work much more. \n\nI know EFT is heavily CPU dependent or would it be that my older card isn't as compatible?"
    },
    {
      "author": "ItsMZB",
      "body": "Well priced in relation to the market I must say then."
    }
  ],
  "92": [
    {
      "author": "Miloapes",
      "body": "3 seperate cables,"
    },
    {
      "author": "SummonMason",
      "body": "Thanks"
    },
    {
      "author": "SummonMason",
      "body": "Thank you. I was confused whether these cables themselves were deemed not good enough but it just seems like I can use them as long as a don't try to use the other connector on it as well.  three separate cables, one connector from each. as you said."
    },
    {
      "author": "SummonMason",
      "body": "Thanks!"
    }
  ],
  "93": [
    {
      "author": "notlikethis_wokege",
      "body": "people downvoting you because you got lucky. lol this sub sucks."
    },
    {
      "author": "cwwjr1681",
      "body": "I dont know. I didnt get too involved because I didnt care to be honest. Im a firm believe in don't look a gift horse in the mouth.\n\nSaid he needed a fast sale because I need to recoup the funds ASAP so he listed it at a price that I knew would sell instantly. He was willing to take a hit to make that happen\n\nHe bought the 4080  and fell behind on doing his rig but when he saw the 4090 become available he bought it right away and it was too late to return the 4080. Why he needed the cash back so fast I dont know. Not sure why he needed to recoup the funds so fast.  Didnt care. It gave me a good deal."
    },
    {
      "author": "cwwjr1681",
      "body": "Yep. Its not just here though its everywhere.  I won a drawing for a 65\" LG TV at work during a raffle giveaway.  The next day I told my coworker who sits across from me expecting him to say something like \"grats man!\". Nope he replied with *BLEEP* you man! and got all pissed off. People are just unhappy in general and pessimistic now days.\n\nYou say \"yay the sun is out\" someone wont hesitate to say but it looks like its clouding up quick"
    },
    {
      "author": "notlikethis_wokege",
      "body": "Yeah I've stopped telling people stuff like that, I just end up with backlash for no reason"
    },
    {
      "author": "cwwjr1681",
      "body": ">Yeah I've stopped telling people stuff like that, I just end up with backlash for no reason\n\nDownvotes inc for this comment lol. Yet you hate on something and you get a storm of upvotes.\n\nI swear its almost like people think \"Im miserable therefor you must be too!\""
    },
    {
      "author": "notlikethis_wokege",
      "body": "I think our thread is too long for people to care at this point lol"
    }
  ],
  "94": [
    {
      "author": "DeadPrexident",
      "body": "Link pls?"
    },
    {
      "author": "TheFather__",
      "body": "[DXB Gamers Best Price | Buy New Galax D26R BLACK GeForce RTX 4090 SG PCI-E 24GB GDDR6X 384BIT](https://dxbgamers.com/product/new-galax-d26r-black-geforce-rtx-4090-sg-pci-e-24gb-gddr6x-384bit/)\n\nHowever, dont order it online, go to the shop and negotiate the price, he will go down to 6700 AED or maybe below, my friend lives in Dubai and he got me the card for 6700 AED after negotiating the price. make sure to call them before going, as these aholes dont usually update their online inventory."
    },
    {
      "author": "DeadPrexident",
      "body": "Thanks a lot!"
    },
    {
      "author": "TheFather__",
      "body": "NP bro, anytime :)"
    }
  ],
  "95": [
    {
      "author": "captainmalexus",
      "body": "I'm just going to assume you're joking"
    },
    {
      "author": "Apprehensive-You-888",
      "body": "https://preview.redd.it/gzj7z8vgdjea1.jpeg?width=1080&format=pjpg&auto=webp&v=enabled&s=db5131253873aa0d2d54b97556ad56db21235d6b"
    },
    {
      "author": "captainmalexus",
      "body": "That's straight nonsense"
    },
    {
      "author": "Apprehensive-You-888",
      "body": "Keep telling yaself that one buddy you're amongst the ppl that believe it so they spend more money on monitors that claim to give this huge increase in display."
    },
    {
      "author": "captainmalexus",
      "body": "Actually I play on a 60hz TV.\n\nDoesn't change the fact that a human eye can easily discern a difference at much higher rates than 60hz.\n\nIf you can't tell, you either have vision impairment or a neurological issue."
    },
    {
      "author": "Apprehensive-You-888",
      "body": "Lol says the guy on reddit to the guy who went to school for the shit. But you believe what you want to each thier own."
    },
    {
      "author": "captainmalexus",
      "body": "No you didn't."
    },
    {
      "author": "Apprehensive-You-888",
      "body": "Lmfao sure didn't buddy"
    }
  ],
  "96": [
    {
      "author": "itboyband1433",
      "body": "Tried that. Lcd works. Fans are stuck in stobe. Card itself  top and side of the card no RGB. I even unplugged it for a few minutes see if it will reset... nothing."
    },
    {
      "author": "EmilMR",
      "body": "so you have fan lights on but rest are off?\n\nI was struggling to keep the fans off but rest on. What I did was set them to black color (0,0,0) otherwise there was no other way.\n\nSet everything to black, then select the zones you want to change and set their color. see if that helps."
    },
    {
      "author": "itboyband1433",
      "body": "All I can get to work at the moment."
    },
    {
      "author": "itboyband1433",
      "body": "Things have gone from bad to worse. Looks like i now have no RGB firmware. I have even tried flashing from the earliest firware to the latest same result. RGB fusion doesn't even show up as a tab in GCC anymore. Card still works great. no issues. Just zero RGB anywhere on the card. Ticket opened with gigabyte...but who knows...\n\nhttps://preview.redd.it/5maxddb10vea1.png?width=959&format=png&auto=webp&v=enabled&s=b1817b3b372d3ec1271f7f4739daa7334880d0e1"
    }
  ],
  "97": [
    {
      "author": "Fair-Particular-6388",
      "body": "Either or on the Monitor Setup not all 6 of course."
    },
    {
      "author": "skylord_luke",
      "body": "Bait question,way to brag"
    },
    {
      "author": "Fair-Particular-6388",
      "body": "i had a 10900k on a z490 msi with 32 gb Hyper 3600 mhz and I hit 65 fps max settings on forza 5 and 66 fps in borderlands 3 max settings on triple screen mode so basic 3k.\n\nI run the 4090 setup on these now and I hit 140 fps in BL3 at 3k and forza 5 I hit 165fps max settings. \n\nSo, I figured I would hit 70 fps in bl3 and 75 in forza 5 is that about, right? Now that I will be at 6k basically. \n\nI was kinda thinking about a 49\" curved but just do not think i would get the real estate I do with 3 32\"\n\nThank you for the reply."
    },
    {
      "author": "Fair-Particular-6388",
      "body": "I am 53 years old and a contractor it was not a matter of bragging.\n\nEvery time someone post's a question people ask what the specs of their system are.  So, I thought I would save the trouble of someone asking the question.  \n\nAs for the other specs I was trying to figure out if buying the 1440p monitors would ruin the fun for me or still give me great gameplay and performance.  Please do not take the time to post if you're not going to answer the question or give some good advice based on your own experience."
    }
  ],
  "98": [
    {
      "author": "heartbroken_nerd",
      "body": "Obviously not all of 4070 ti models are small but there are a few notable ones.\n\nYou can get some 4070 ti models that are 295-310mm length and 118-125mm width. For example:\n\nMSI 4070 Ti VENTUS 3X 12G\n\nInno3D GeForce RTX 4070 Ti X3\n\nZotac GeForce RTX 4070 Ti Gaming TRINITY\n\nZOTAC GAMING GeForce RTX 4070 Ti AMP AIRO\n\nTechpowerup has a list of AIB cards as per usual:\n\nhttps://www.techpowerup.com/gpu-specs/geforce-rtx-4070-ti.c3950\n\n>170mm CPU heatsink clearance\n\nadd 36mm or slightly less than that to the width of the card and compare against CPU clearance as a rule of thumb. It depends on how deeply seated the plug is on the card."
    },
    {
      "author": "resumestuff",
      "body": "Thanks.\n\nI'm actually more so looking at this from a case purchase perspective. Basically I'm trying to gauge what limitations there would be with respect to future GPUs given the case width. Most of the cases I'm currently interested in are more so around 165mm and lesser in CPU heatsink clearance. \n\nMy current case for example could in theory fit FE cards except the current 4080/4090 and 3090+ last gen, but is too short for triple fan AiB cards even as low as the 3060. Most cases now seem to be fine length wise, but width is another issue, and buying a new case that opens up the length just to be short width wise would be kind of self defeating."
    },
    {
      "author": "heartbroken_nerd",
      "body": "Anything that has 185mm+ CPU cooler height and like 380mm+ GPU length should be really future proof, as in even the biggest 4090s should fit for the most part."
    },
    {
      "author": "resumestuff",
      "body": "I'm trying to weigh the balance of factors because otherwise I want a smaller and also typically cheaper case."
    },
    {
      "author": "heartbroken_nerd",
      "body": "I don't know about smaller... but check out:\n\nNZXT H7 Flow\n\nPhanteks ECLIPSE G500A"
    }
  ],
  "99": [
    {
      "author": "ThisPlaceisHell",
      "body": "Can you post your own screenshots similar to mine? Make sure to leave the system completely idle with no major programs running in the background (Wallpaper Engine things like that, Afterburner/Steam are fine.) I'm really curious to see if just leaving your system idle with power saving plans active really sees these kinds of spikes on nvlddmkm.sys like you said."
    },
    {
      "author": "rdalcroft",
      "body": "Here are 3 seperate readings,  these are a few weeks old, as I have just installed 511.79 drivers which are good for dpc latency.\n\n528.02 driver\n\n[https://freeimage.host/i/HlpYc42](https://freeimage.host/i/HlpYc42)\n\n[https://freeimage.host/i/HlpY02S](https://freeimage.host/i/HlpY02S)\n\n[https://freeimage.host/i/HlpYaEl](https://freeimage.host/i/HlpYaEl)"
    },
    {
      "author": "ThisPlaceisHell",
      "body": "Wow that's really bizarre. What's funny is your highest ISR latency is significantly lower than mine, where I get dxgkrnl.sys spikes constantly.\n\nAnyways, curious. I see you have the Nvidia Inspector window open for the multi-monitor power saver. Do you use that feature/function? I remember trying it in the past on my 1080 Ti and I noticed that not only did it cause \"hiccups\" on my desktop but sometimes even artifacting. If you are indeed using this, that could be the cause of your high DPC spikes. This feature is a hack that really shouldn't be used. There's a reason graphics cards have to boost up with certain multimonitor configurations. Faster cards can handle higher configs without needing to boost, but it is what it is."
    },
    {
      "author": "rdalcroft",
      "body": "Hello, no, for those screen shots I was asked to use Nvidia inspector to force a P08 power state, for testing purposes, to see if it was the lower power states causing the high DPC latency.\n\nthe other screen shot is without inspector running.  I just grabbed all 3 screen shots.\n\nI only have one monitor."
    },
    {
      "author": "ThisPlaceisHell",
      "body": "Is your monitor 1440p 165hz or higher? Or possibly 4k 120hz or higher? What does your GPU idle at without any tweaks to power plans?"
    },
    {
      "author": "rdalcroft",
      "body": "1440p 165hz.  Idles at 210mhz"
    },
    {
      "author": "ThisPlaceisHell",
      "body": "Ok I see. Yeah this is a really weird and frustrating problem. It drives me nuts that I can't figure it out as I have my own DPC issues but not necessarily tied to Nvidia. No clue how to solve them unfortunately and it seems we're basically told \"too bad\" by the OEMs."
    },
    {
      "author": "rdalcroft",
      "body": "It\u2019s frustrating. \nUsual culprits. \nSound, video or network card.\n\nI blame Microsoft. They keep changing windows so much. That other companies struggle to keep up."
    },
    {
      "author": "ThisPlaceisHell",
      "body": "Here's what 1 minute of logging with everything running in high performance mode looks like: https://i.imgur.com/B4w5GPo.png\n\nIf I let the test run longer it'll eventually get hit with ntoskrnl.exe spiking highest measure interrupt to process and DPC latency to about 200 microseconds. But yeah this looks pretty much perfect otherwise to me.\n\nI did learn something interesting. I'm keeping my Valve Index unplugged 24/7 until I'm ready to game now. When the USB plug is connected, which drives the microphone and the dual front passthrough cameras, Wdf01000.sys sees a MASSIVE increase in ISR counts. The funky thing is, I'm not even using the headset/it's not activated. Makes me super sketched out, has me thinking the mic/cameras are always active. That shit is staying unplugged from here on out."
    },
    {
      "author": "rdalcroft",
      "body": "Nothing to complain about those results. I wish mine was like that. \n\nYeah I\u2019d keep the vive unplugged until you need to use it."
    },
    {
      "author": "ThisPlaceisHell",
      "body": "For sure man. And you've tried using high performance Windows power plan + Nvidia Prefer Max Performance and still see huge latency spikes? Because if so, man I just don't know what to say. Good luck is about it =/"
    },
    {
      "author": "rdalcroft",
      "body": "If I set both to highest yes it gets much better. \n\nBut that\u2019s the problem. I don\u2019t want to be in high power mode when just idling or watching streaming video. \n\nYes for audio work using my DAW. this is a must. \n\nI use process lasso this is good as it automatically switches windows power plans to highest when I run a game. \n\nBut usually normal is good for gaming for the GPU. I have never really needed to switch to max performance for gaming. No spikes when at max clocks."
    }
  ],
  "100": [
    {
      "author": "CatoMulligan",
      "body": "> The MSI Ventus (OC or not) is actually 3 slots.\n\nIt is, but the H1 V2 has room for a true 3 slot (60mm) card.  The problem is that where the mounting bracket screws in they have only provided room for a 2-slot card.  So a true 3 slot card will fit as long as it only has a 2 slot mounting plate/IO plate."
    },
    {
      "author": "dhskiskdferh",
      "body": "Ah, I see your problem now. I\u2019m not sure there are any 2-slot-bracket but only 3-slot 4080s/90s.\n\nHave you considered the 7900xtx? I bought one due to its lower profile and about equal performance to the 4080, but then switched to a 4080 as the 7900xtx has problems with VFIO-PCI for my windows gaming VM. Otherwise you could consider the liquid cooled, albeit at a cost premium"
    },
    {
      "author": "CatoMulligan",
      "body": "Yeah, I'd considered going back to AMD but I really like the boost from DLSS.  I know that AMD has their own flavor but I'm not sure how well supported it is out there, and I really want to give DLSS3 framegen a shot.  Realistically a 4070ti will probably be sufficient if I'm not enabling RT everywhere.  I've got a 3080 Strix OC right now that needs to go back into a different PC and I think that the only time I ever turned on RT was when Portal RTX came out."
    },
    {
      "author": "dhskiskdferh",
      "body": "Hah, I just did portal with rtx, this is my first nvidia card. I\u2019m a fan of the ray tracing, I\u2019m not as sold on DLSS though, mostly because of the limited number of games that support it. Up to you, but I\u2019d personally avoid the 4070TI, and since nothing else will fit I\u2019d probably go with a 7900xtx"
    }
  ],
  "101": [
    {
      "author": "nopointinlife1234",
      "body": "Just my two cents, but if he pays for it, why not? What else is money good for if not making you happy in whatever way you choose? Ferrari or shitbox."
    },
    {
      "author": "zodiacrelic44",
      "body": "I mean yeah, but there\u2019s no need to spend that much money to use something when you can spend 30-40% and still get the same joy out of it"
    },
    {
      "author": "nopointinlife1234",
      "body": "What happens when he wants to run at full power in the winter months? \n\nI think the solution to all this is just an air conditioner lol\n\nEDIT: OP should move to Alaska."
    },
    {
      "author": "zodiacrelic44",
      "body": ">I think the solution to all this is just an air conditioner lol\n\nThat\u2019s because that is the perfect solution. \n\nBut a 2080ti or a 3080 still put off plenty of heat. My 3060TI heats my room no problem in the winter, without making it unbearably hot in the summer."
    },
    {
      "author": "nopointinlife1234",
      "body": "I feel that. I'm worried what summer's going to look like with this 4090. On top of it, I recently put in a 420mm AIO that launches out heat. And I'm fairly certain I'm going to bake alive in the 100F+ California summers. My room faces the sun most days, so AC's only do so much even with blackout curtains."
    },
    {
      "author": "zodiacrelic44",
      "body": "Good luck with that man. You\u2019re gonna fry like an egg."
    }
  ],
  "102": [
    {
      "author": "Starbuckz42",
      "body": "There is no benefit to go beyond what the FE has to offer. \n\n> It doesn't have the best VRM.\n\nIt may not have the literal best VRM but it's more than enough, all components are high quality, the PCB is a piece of art. There is simply no point to go with anything else.\n\n> It doesn't have the best cooler.\n\nIt does, arguably. Its more than adequate, great temps, virtually inaudible. Again, it doesn't make sense to go with a 'better' solution.\n\n> It doesn't have the best BIOS options.\n\nCompletely irrelevant.\n\nIt's the only card that makes sense, everything else is just personal preference for their looks and maybe warranty."
    },
    {
      "author": "Capt-Clueless",
      "body": ">It does, arguably. Its more than adequate, great temps, virtually inaudible. Again, it doesn't make sense to go with a 'better' solution.\n\nExcept it objectively does not have the best cooler. All the AIB cards run cooler and quieter."
    },
    {
      "author": "Starbuckz42",
      "body": "Silent and cool enough is silent and cool enough.\n\nIf you want to spend hundreds of dollars more to get a card that won't do anything better in any practical sense, then go for it."
    },
    {
      "author": "Capt-Clueless",
      "body": "Silent? Maybe at idle...\n\nALL 4090s are loud. The FE is one of the louder ones at default. And at the same noise levels, it's the worst performing.\n\nhttps://www.techpowerup.com/review/gigabyte-geforce-rtx-4090-gaming-oc/38.html"
    },
    {
      "author": "Starbuckz42",
      "body": "You are the perfect gullible customer, numbers go brrr, must buy. Leaves the better options for the rest of us at least, not complaining."
    },
    {
      "author": "Capt-Clueless",
      "body": ">You are the perfect gullible customer, numbers go brrr, must buy. \n\nUm, English please?"
    }
  ],
  "103": [
    {
      "author": "IamQL",
      "body": "The cpu is ok, maybe later upgrade to 5600.. Which card do you have now, which exact model?"
    },
    {
      "author": "Ficklehickle",
      "body": "Don\u2019t know the model i\u2019m afraid and i\u2019m working currently so cannot check but according to one google search this new card along with 2600x should obly have 8% bottleneck. Down the line i should probably upgrade cpu but for now it should be fine"
    },
    {
      "author": "IamQL",
      "body": "I am asking about the card because of the dimensions.. You can also check your case on a website what is the biggest gpu which will fit inside.."
    },
    {
      "author": "Ficklehickle",
      "body": "Okay thanks, what is the website? I will check when i get home."
    },
    {
      "author": "IamQL",
      "body": "You would need to know the name of the case.."
    },
    {
      "author": "Ficklehickle",
      "body": "Doesn\u2019t it say on the case?"
    }
  ],
  "104": [
    {
      "author": "TomorrowWillBeDecent",
      "body": "I ordered the corsair 5000d"
    },
    {
      "author": "Ballfade",
      "body": "Sounds good"
    },
    {
      "author": "TomorrowWillBeDecent",
      "body": "Thanks bro that\u2019s the only thing that had me worried"
    },
    {
      "author": "Ballfade",
      "body": "No prob, you enjoy"
    }
  ],
  "105": [
    {
      "author": "CptTombstone",
      "body": "> I have never in my life used a G-Sync module display but I can guarantee you without a shadow of a doubt that there's no such thing happening with a G-Sync Compatible display. \n\nI have a G-sync module and a G-sync compatible display. With my G-sync compatible display (LG C1 OLED) engaging V-sync from the in-game menu locks framerate to 117 fps. On my G-sync module equipped display, it locks the framerate to 98 fps. (120 Hz and 100Hz native refresh rates respectively.) The same values are displayed with both software based framerate counters and the displays' VRR counters. If this is not the behavior you're seeing, this might be down to the displays than, and I might be wrong, I just assumed that Nvidia implemented those settings into the driver.    \n\n\n> What on earth? V-Sync On LITERALLY LOCKS YOUR FRAMERATE TO YOUR REFRESH RATE if unsupervised. So you're never in a situation where the framerate goes meaningfully above your native refresh rate if any V-Sync is used. \n\nIt's not as simple as that. With modern V-sync solutions, there are at least 3 buffers for frames, so that you can maintain an arbitrary fps value in the game (this is when G-sync is not enabled) A traditional double buffer V-sync solution would only allow to game to run at native and half native refresh rates, for example. So with three buffers, you can achieve an arbitrary fps in game and still keep the display from displaying incomplete frames. But there is a latency impact by doing so. Fast Sync keeps an unlimited number of buffers to minimize latency (basically, so that only the last complete frame gets displayed ). That keeps the latency impact at a minimum, but you are basically throwing away frames that do not line up with the display's polling interval. This means that when the display is scanning for a new frame, it is not guaranteed to get a frame that is the exact \"next\" frame rendered by the game, but if the framerate is high enough, it might be 3rd next frame, as an example. This would mean that the \"in-game time\" between the displayed frames on the monitor is not constant, thus animations are not 100% fluid. I believe that when forcing V-sync on with DLSS Frame Generation, the driver switches to a Fast-sync like method to minimize latency. [Digital Foundry's video](https://youtu.be/92ZqYaPXxas?t=432) also references this behavior with forced V-sync. And this leads to:  \n\n\n> How is your framerate above your refresh rate? I swear your settings must be wrong somewhere, there's no shot. V-Sync wouldn't let that happen even without Frame Generation involved, it'd be capped at 120hz (if that's your refresh rate). \n\nI wasn't exactly clear on what I meant. More like when the framerate *would* be above 120 when not capped with V-sync. This ties in with my above point, animations may not be 100% evenly paced.   \n\n\n> You're supposed to configure Frame Generation ever since Miles Morales Game Ready drivers mid-November update. In all DLSS3 games, NVCP V-Sync On is RECOMMENDED by Nvidia if you have G-Sync and want to use Frame Generation. \n\nAs far as I know, V-sync is not officially supported when Frame Generation is used. If it were supported, the games would not switch V-sync off when enabling Frame Generation. This is consistent with what's in Digital Foundry's video, where Nvidia commented on V-sync not being officially supported yet. Of course there is the option to force it. Can you link to an article/video that points to the contrary?\n\n&#x200B;\n\n> Nvidia Reflex will cap framerate for you at much lower input latency when it detects G-Sync+NVCP V-Sync On+Frame Generation On. \n\nIf you're comparing to V-sync off vs NVCP V-sync on, this is objectively untrue, you can monitor PC latency via the Nvidia driver, and see for yourself, you get higher latency with V-sync forced on. If you're comparing to Reflex off vs Reflex on, of course you get lower latency with reflex, but you cannot turn reflex off when Frame Generation is set to on, and in general, why would you turn it off. So I'm not exactly sure what you meant by this."
    },
    {
      "author": "heartbroken_nerd",
      "body": "> As far as I know, V-sync is not officially supported when Frame Generation is used. If it were supported, the games would not switch V-sync off when enabling Frame Generation.\n\nYou're not supposed to use the in-game V-Sync, once again - NVCP V-Sync is what you are supposed to use as per official Nvidia recommendation from the middle of November update.\n\n>This is consistent with what's in Digital Foundry's video, where Nvidia commented on V-sync not being officially supported yet. Can you link to an article/video that points to the contrary?\n\nYes, the 526.98 driver article on Nvidia.com - CTRL+F search \"vsync\".\n\nhttps://www.nvidia.com/en-us/geforce/news/geforce-rtx-4080-game-ready-driver/\n\n>If you're comparing to V-sync off vs NVCP V-sync on, this is objectively untrue, you can monitor PC latency via the Nvidia driver, and see for yourself, you get higher latency with V-sync forced on.\n\nYes, \"higher latency with V-Sync On\", but how much higher? Negligible compared to what it was before mid-November update.\n\nhttps://youtu.be/OHiNtg-5qnc?t=823"
    },
    {
      "author": "CptTombstone",
      "body": "Thanks for the article!   \n\n\n> DLSS 3 uses its NVIDIA Reflex technology to limit the output frame rate to slightly below the refresh rate of the G-SYNC monitor. \n\nThis is exactly what I've experienced it seems. \n\n&#x200B;\n\n> Yes, \"higher latency with V-Sync On\", but how much higher? Negligible compared to what it was before mid-November update. \n\nI concur, the additional latency is imperceivable for me, but I'm also not very latency sensitive. I only got the 4090 in December, so I haven't tried Frame Generation before that, so I don't have a reference point from before 526.98."
    },
    {
      "author": "heartbroken_nerd",
      "body": ">I only got the 4090 in December, so I haven't tried Frame Generation before that, so I don't have a reference point from before 526.98.\n\nFair enough!"
    }
  ],
  "106": [
    {
      "author": "sudo-rm-r",
      "body": "Yeah but I'm not buying a display to replace after 2 years. I wanna see a test with 5 years of use."
    },
    {
      "author": "Eorlas",
      "body": "\u20262 sentences up from what i quoted:\n\n\u201cUpdate 05/31/2019: The TVs have been running for over 9000 hours (around five years at 5 hours every day).\u201d\n\nyou people literally do not read"
    },
    {
      "author": "sudo-rm-r",
      "body": "Yes but playing movies not with a static pc interface."
    },
    {
      "author": "Eorlas",
      "body": "> Yes but playing movies not with a static pc interface. \n\nyes but no. not at all. playing movies isn't part of that test. at all. i understand you're hopeless, but in case anyone's reading this thread, misinformation isn't going to help anyone. \n\nthere were 6 different test scenarios on 6 different TVs. none of them were playing movies. at all. when i said \"you people literally do not read\", not only was that not a question, but clearly i was also spot on. \n\nwith a \"static pc interface\" - who, buying an lg oled for \\*gaming\\*, is hanging out on their desktop for 5+ hours \\*every day\\*? for 5 years? this is not happening. \n\nfifa, the worst offending game, had \\*some\\* burn in issues appearing at 90 weeks. straight. \n\ncall of duty, a more dynamic game, did not.\n\nthese things are spelled out in plain english for anyone to read in the link. testing methodology included, and logs of details taken along the way. \n\nburn-in for OLEDs, \\*especially\\* for newer tech displays is a grossly exaggerated issue. \n\nthis is a fact, not a discussion."
    }
  ],
  "107": [
    {
      "author": "xDoWnFaLL",
      "body": "Ahhh copy, no worries and all good. Always curious what people upgrade to/from G7, thanks for the info though, sick tempting refresh rate. Wish some quality 1440p ones would get to that range, insane!"
    },
    {
      "author": "MissSkyler",
      "body": "i\u2019m honestly waiting for that asus 1440p240hz oled so i can move back to 27\u201d and 1440p. i mainly play valorant so the 360hz gives a very nice non-input delay. i often miss out on some performance being at 1080p but G7 was my endgame until OLEDS become mainstream / mini-leds"
    },
    {
      "author": "xDoWnFaLL",
      "body": "Agreed, mainly play CSGO and 240hz feels so quality. Ultrawide 21:9 is cool when games support it but the performance hit is *rough* for some games coming from standard 1440p 16:9, speaking from 3080FE but def little bottleneck with CPU currently.\n\nEdit: Saw an OLED in store and became a believer. Stunning monitors and price tags to match."
    },
    {
      "author": "MissSkyler",
      "body": "that\u2019s what i\u2019m saying! i would\u2019ve gotten that alienware ultra wide oled but frankly not enough games support + it\u2019s only 175hz which i can feel when i\u2019m so used to the input delay or 360/240."
    }
  ],
  "108": [
    {
      "author": "little_jade_dragon",
      "body": "Corolla is baseline today."
    },
    {
      "author": "HolyAndOblivious",
      "body": "Car feature creep is real. I just want a LADA to run some errands and take my kids to kindergarten"
    },
    {
      "author": "little_jade_dragon",
      "body": "And get shish kebab'd when someone bumps into you."
    },
    {
      "author": "HolyAndOblivious",
      "body": "I was born and raised in the station wagon era. I dont care."
    }
  ],
  "109": [
    {
      "author": "boilermaker815",
      "body": "Agreed. Most people also forget that you get an additional 75w from the PCIE slot too... Plenty of power from two different 8 pin PCIE connections + 75w from the PCIe slot."
    },
    {
      "author": "Fragrant-Cut3597",
      "body": "Lol thanks"
    },
    {
      "author": "Fragrant-Cut3597",
      "body": "Hmm. Maybe upgrade to a PSU that has 3 separate cables?"
    },
    {
      "author": "Fragrant-Cut3597",
      "body": "So I would be safe to plug only 2 cables into the splitter and not 3?"
    },
    {
      "author": "Fragrant-Cut3597",
      "body": "https://preview.redd.it/bokegb2ptuda1.png?width=1284&format=png&auto=webp&v=enabled&s=bd6ced3b3fd56f84a351684211680d6e9276f6e9"
    },
    {
      "author": "Fragrant-Cut3597",
      "body": "https://preview.redd.it/r0r95smqtuda1.png?width=1284&format=png&auto=webp&v=enabled&s=da0ba38cfd4b8fa5137c38931ad8af1d0d6c2ad2\n\nThis is from my PSU manual. So that\u2019s called a daisy?"
    }
  ],
  "110": [
    {
      "author": "Riiyan",
      "body": "There is proof all over YouTube. Just go watch the compassion videos and all the various settings. The 15% is only for cyberpunk it varies with other games. Some games it\u2019s less some games it\u2019s more but one thing doesn\u2019t change, your going to use DLSS anyway for the taxing games and that is when the 4090 looks the worst compared to half the price and half the power draw."
    },
    {
      "author": "RemedyGhost",
      "body": "I just watched a cyberpunk benchmark, the 4090 was on average 60% faster than the 4070ti"
    },
    {
      "author": "RemedyGhost",
      "body": "Here is a screen shot from hardware unboxed results.. Where is the 15% ??\n\nhttps://preview.redd.it/7tgavgs13pda1.jpeg?width=1281&format=pjpg&auto=webp&v=enabled&s=e396c31788236bc66b04070edb145073d6cff3db"
    },
    {
      "author": "Riiyan",
      "body": "No DLSS No RTX. With RTX on the gap shrinks, if we are comparing cyberpunk. This is a direct screenshot in game. On this stretch of road they both dip and rise about 20fps.\n\nhttps://preview.redd.it/t2fm6ggb3pda1.png?width=1941&format=png&auto=webp&v=enabled&s=bff26e376aff060eab25004a9e569f5da40ee5fd"
    },
    {
      "author": "Riiyan",
      "body": "I don't know where you got that graph from but all those cards do much higher then that on High Quality no RTX 4k."
    },
    {
      "author": "RemedyGhost",
      "body": "This is not an average and not the same scene. you are really trying to cherry pick a frame were they are the closest. I also notice the 12fps .1% low on the 4070ti compared to the 67fps .1% lows on the 4090... That's a major issue for the 4070ti."
    },
    {
      "author": "RemedyGhost",
      "body": "This is from hardware unboxed. a very well known tech YouTube channel and not some random benchmarks from unknown sources which are known to be faked."
    },
    {
      "author": "Riiyan",
      "body": "It\u2019s the same stretch of road and I told you the average difference that\u2019s not cherry picking your trying to say it gets that much more performance and it doesn\u2019t when everything is cranked up."
    },
    {
      "author": "Riiyan",
      "body": "The 4070ti isn\u2019t a 4k card. You are missing the point. It\u2019s standing toe to toe at 800 dollars vs a 2k card on something it\u2019s not even designed to do."
    },
    {
      "author": "RemedyGhost",
      "body": "Except it's not standing toe to toe at all.."
    },
    {
      "author": "Riiyan",
      "body": "It totally is, you stuck on 1% or less of the time. With no DLSS at a screen resolution it\u2019s not designed to perform optimally on."
    },
    {
      "author": "RemedyGhost",
      "body": "Dude, just look up any benchmarks from trusted sources. The 4070ti is nowhere near the performance of a 4090. it's doesn't matter what upscaling is added or at any resolution."
    },
    {
      "author": "Riiyan",
      "body": "Use DLSS and it is for half the price.\n\nhttps://preview.redd.it/kr047n8r7pda1.png?width=2560&format=png&auto=webp&v=enabled&s=20dddf535ba5d4fed291769c4609f40e97241129"
    },
    {
      "author": "RemedyGhost",
      "body": "60% faster... imagine that."
    },
    {
      "author": "Riiyan",
      "body": "That is without using any of the cards AI features that\u2019s just tit for tat one is a 4k card one is a 1440."
    }
  ],
  "111": [
    {
      "author": "Taterbob75",
      "body": "Most games stay in the 60 - low 70 degree Fahrenheit range. Not a hot card."
    },
    {
      "author": "Randy313",
      "body": "What ate your GPU stock clocks then? 2805?"
    },
    {
      "author": "Taterbob75",
      "body": "2520 MHz core and 22.4 Gbps memory"
    },
    {
      "author": "Randy313",
      "body": "I was asking about the actual clocks, in games.\nYou can check them with msi afterburner."
    },
    {
      "author": "Taterbob75",
      "body": "Did a little overclocking and the max was up to 2985 MHz core and 24.5 Gbps memory, in game."
    },
    {
      "author": "Randy313",
      "body": "Well done, gg, enjoy your card"
    }
  ],
  "112": [
    {
      "author": "WretchedBinary",
      "body": "And stepping over motherboards stacked on the floor never helped me XD"
    },
    {
      "author": "michty_me",
      "body": "I've had similar with other 'projects' I had car clutches, Turbos and exhausts in my hallway. Certainly got strange looks when having people coming into the house."
    },
    {
      "author": "WretchedBinary",
      "body": "The next time you do get one of those weird looks, just act surprised and say \"what the heck is all this and who is responsible for putting this here?\""
    },
    {
      "author": "michty_me",
      "body": "I like it! Fortunately, I've managed to clean the hoarding of bits up to an extent, Well at least they are stored in a cupboard now."
    }
  ],
  "113": [
    {
      "author": "exclaimprofitable",
      "body": "Configure OBS to use NVEC instead of the cpu for encoding.\n\nI use Nvidia shadowplay all the time, and because it uses NVEC to encode, no fps hit.\n\nSo you don't need a new processor, you just need to setup OBS correctly.\n\nAnd if you for some reason can't set it up correctly, stream with nvidia experience/shadowplay."
    },
    {
      "author": "Barcacrew",
      "body": "Same thing after reinstalling system."
    },
    {
      "author": "Barcacrew",
      "body": "I have nvenc set up. But even if I'm not streaming/recording, I have 20 percent fps drops. For just turning on the obs with one scene and only screen capture"
    },
    {
      "author": "exclaimprofitable",
      "body": "Then you have something wrong in the setup.\n\nTry nvidia geforce experience, and see if that also drops the FPS by 20%, I would wager it doesnt."
    }
  ],
  "114": [
    {
      "author": "ankor77",
      "body": "Woa. Weird that posted 3 times. Its the sku for 4090 fe if u look on the best buy website"
    },
    {
      "author": "slincoln2k8",
      "body": "Yeah i tried the chat and no luck :("
    },
    {
      "author": "slincoln2k8",
      "body": "Amazing it\u2019s this difficult to give Nvidia $1,600"
    },
    {
      "author": "ankor77",
      "body": "exactly what I was thinking for months."
    }
  ],
  "115": [
    {
      "author": "Papa_Jay_",
      "body": "[The Peladn Gaming RTX 3060Ti on Newegg is only $389](https://www.newegg.com/p/1FT-00EY-00011?item=9SIBF3CJ558974&source=region&nm_mc=knc-googlemkp-pc&cm_mmc=knc-googlemkp-pc-_-pla-peladn+flagship+store-_-video+card+-+nvidia-_-9SIBF3CJ558974&utm_source=google&utm_medium=paid+shopping&utm_campaign=knc-googlemkp-pc-_-pla-peladn+flagship+store-_-video+card+-+nvidia-_-9SIBF3CJ558974&id0=Google&id1=19485915243&id2=145186510539&id3=&id4=&id5=pla-1854815389593&id6=&id7=9015546&id8=&id9=g&id10=c&id11=&id12=Cj0KCQiAt66eBhCnARIsAKf3ZNFEJsJOPoTZHu0MySsFGn-7pf8vPfc34t_mRFl3g7MyM1CAjQNzHpUaAh_aEALw_wcB&id13=&id14=Y&id15=&id16=643882174971&id17=&id18=&id19=&id20=&id21=pla&id22=604133240&id23=online&id24=9SIBF3CJ558974&id25=US&id26=1854815389593&id27=Y&id28=&id29=&id30=12250686344355610383&id31=en&id32=&id33=&id34=&gclid=Cj0KCQiAt66eBhCnARIsAKf3ZNFEJsJOPoTZHu0MySsFGn-7pf8vPfc34t_mRFl3g7MyM1CAjQNzHpUaAh_aEALw_wcB&gclsrc=aw.ds)"
    },
    {
      "author": "DragonCraft11",
      "body": "okay, thanks for the input"
    },
    {
      "author": "DragonCraft11",
      "body": "oh nvm wrong card"
    },
    {
      "author": "DragonCraft11",
      "body": "compared to the founder edition, which is better?"
    },
    {
      "author": "DragonCraft11",
      "body": ">yeah but is it better than the other one?"
    }
  ],
  "116": [
    {
      "author": "RiskEnvironmental568",
      "body": "Clean up my wiring?\n\nhttps://preview.redd.it/smor7i7mroda1.png?width=4032&format=png&auto=webp&v=enabled&s=657e23ff8e7b849835beadcbb3378c57b6fb2979"
    },
    {
      "author": "n19htmare",
      "body": "Almost all dedicated 12VHPWR cables sold by PSU manufacturers including Corsair use total of 12 wires. 6pos and 6neg. They run straight from PSU to each pin on the 12VHPWR connector.  Each PSU connector is 3+3 config out of the 4+4 available on the EPS12V connector.\n\nThey do however use 16ga wires instead of the 18ga that they use on included PCI-E cables (ones that come with PSU).  A 75C 16ga wire is typically rated to carry 18A. So each wire can carry 200+W, and there's 6 pair of them. PLENTY PLENTY of power can run though the wires.\n\nThe limitation resides in the rating of the 12VHPWR connector. Each pin in the 12VHPWR(Molex mini-fit jr) is rated 9A (108W per pin) x 6 that's 648W, thus a connector rating of 600W.\n\nRunning more than 6 pairs of 16GA wires (2 PSU connectors) is completely unnecessary and thus why all the 12VHPWR cables that PSU manufacturers sell are 12 wire 16ga w/ 2 PSU connectors. Easy, simple, tidy straight wires from point A to point B."
    },
    {
      "author": "n19htmare",
      "body": "I wasn\u2019t talking about the cable plugged into your card. I was referring to \u201cquite the long run\u201d and behind the scenes wiring. If you\u2019re ok with it then you\u2019re good to go."
    },
    {
      "author": "RiskEnvironmental568",
      "body": "Thanks for that explanation. I saw that the wires were 16ga and not 18ga, but didn't know how the PS connected to the card. You'd think the card vendor would save money on the connector by only having two connectors and not three. Though at the price the cards are commanding now, a few cents wouldn't matter."
    },
    {
      "author": "RiskEnvironmental568",
      "body": "I haven't decided yet if I'll keep the above routing or not. The CableMod above is the same length as the Corsair (I think), \\~64cm. I could just route it straight down and to the PS, but I think it looks cleaner like this. My concern, apparently not a problem, was of a voltage drop (and resulting power drop) due to the increase cable length's increased resistance."
    },
    {
      "author": "n19htmare",
      "body": "It does look good, just keep it as is. Wouldn't worry about V-drop that much, you've got plenty of connections and nothing should cause and high resistance leading to concerning Vdrops.\n\nYou can however check for yourself, for your peace of mind.\n\nYou can use software like GPU-Z or HWiNFO for example to monitor your V-drop if you'd rather see the numbers. You're looking for delta change more so than what the actual number is.\n\nFor example if your idle input power is 12.4V but under load drops to 11.6V, then that delta of .8 more concerning then say dropping from 12.0v to 11.6v etc. (it shouldn't drop to 1.6V, it's just an example)\n\nIdeally a good power supply/connections should be able to manage the voltage around 12.0-12.2V at idle and at load."
    },
    {
      "author": "RiskEnvironmental568",
      "body": "I checked with both GPU-Z and HWiNFO and the voltage at the connector only dropped from 12.4V to 12.3V when running Half-Life: Alyx, with the card pulling about 325W so it looks pretty good. Thanks!"
    }
  ],
  "117": [
    {
      "author": "NinjAsylum",
      "body": "Are those reverse flow fans on the bottom under the GPU? \n\nIf they are reverse flow, then thats a pretty nice build.  If they're not reverse flow then you're suffocating your GPU"
    },
    {
      "author": "Georgio281",
      "body": "Thanks. I\u2019ve always liked DeepCool\u2019s aesthetic and when I saw this case I finally decided to hop on board. It was a pleasure to build in, as well."
    },
    {
      "author": "Georgio281",
      "body": "They don\u2019t, unfortunately. I bought a three pack of Arctic P14 RGB fans separately. The case only came with one 120mm fan."
    },
    {
      "author": "Georgio281",
      "body": "They\u2019re flipped now"
    },
    {
      "author": "Georgio281",
      "body": "Good catch. I\u2019ve flipped the orientation since the pics were taken yesterday. Initially made the mistake because I was finishing the build at 2am and was a bit delirious."
    }
  ],
  "118": [
    {
      "author": "Timonster",
      "body": "Just tested, i got 800 on Fan1 and a little under 700 for Fan2.\nI still think 57 is pretty high, do they put other fans in the Aorus than in the gaming oc?\n\nSo, this might be a shot in the dark, but have you tried turning off your RGB lighting on the card, it\u2018s not much power, but i\u2018m pretty sure it influences your Fans on low rpm."
    },
    {
      "author": "Mhmd1993",
      "body": "Yes, the reason your fans don't spin for a prolonged time is just because the ventilation of your case is excellent, and this makes the gpu reach its \"target\" temp for a fan stop quicker. My case is the nzxt h7 flow. it has one intake fan, but that's not the problem. It's in the middle slot now and it's blowing air directly on the closed front of the card. I'll move it to the lower slot to make it blow fresh air for the GPU fans."
    },
    {
      "author": "Mhmd1993",
      "body": "One of the comments here showed that this issue can also be fixed by a custom curve in Gigabyte control center. I managed to get as low as 800 RPM on all fans, and they stop spinning when the hotspot is at 42 degrees, most importantly, no revving. I'll also try turning off the RGB as you said."
    },
    {
      "author": "Mhmd1993",
      "body": "Thank you so much! I know this is 2 days later but disabling fan RGB **completely** solves the issue and allows a lower stable RPM, around 700 now :)"
    },
    {
      "author": "Timonster",
      "body": "Good news!"
    },
    {
      "author": "Timonster",
      "body": "very nice! Glad i could help."
    }
  ],
  "119": [
    {
      "author": "ListenBeforeSpeaking",
      "body": "Does that PSU fan go directly into a glass panel?"
    },
    {
      "author": "Nestledrink",
      "body": "So far so good. The plan is to move the PSU a little inside which should provide more gap. Fortunately for the workload I do most, the fans don't spin!"
    },
    {
      "author": "Nestledrink",
      "body": "Yeah! maniac indeed. The GPU is well cooled though on the mesh side"
    },
    {
      "author": "Nestledrink",
      "body": "No. This is actually more difficult to build as the parts come flat packed like IKEA. One of the more challenging built I've done yet."
    }
  ],
  "120": [
    {
      "author": "Arkplayer22711",
      "body": "Can you tell me what resolution you play on?"
    },
    {
      "author": "Blacknight499",
      "body": "Ah ok and thanks again."
    },
    {
      "author": "Arkplayer22711",
      "body": "Depends on what you do, i got the RTX cause i do 3D Animation and gaming and AMD isn't the best in that category, so Nvidia was neccesary for me, for a pure gamer i do think that the AMD GPU has a much better value , for 40\u20ac more tho , the Nvidia tax is ok. I got my 3060 for 420\u20ac , i really wanted a 3 fan gpu so i got 3060 gaming oc by gigabyte. It was a great upgrade from my RX 580 , Gaming Performance doubled and Blender Performance many times improved."
    },
    {
      "author": "Blacknight499",
      "body": "1920x1080"
    },
    {
      "author": "Arkplayer22711",
      "body": "Ok, for your budget, i advise an RX 6600 non xt. It's a great 1080p card and comes close to a 3060 for under 300\u20ac/dollars. Ray Tracing won't be something you're gonna be using with this card since it doesn't have a good enough ray tracing performance that i would recommend turning it on. But rasterized performance comes really close to a 3060, the similarly priced 3050 has a lot less power than this card so i dont advise it. Get the 6600 and you're gonna be happy, but if you do want more you can try a 6600xt/6650xt these cards go over your budget tho. I hope this helps you."
    },
    {
      "author": "Blacknight499",
      "body": "Thank you this helps a lot."
    },
    {
      "author": "Arkplayer22711",
      "body": "No problem! Glad to help!"
    }
  ],
  "121": [
    {
      "author": "Solemnity_12",
      "body": "Specs:\n\n**CPU**: i5-13600K (slight undervolt + P cores at 5.2 GHz & E cores at 4 GHz)\n\n**GPU**: RTX 4080 FE (PL slider set to 110%)\n\n**RAM**: 32 GB Corsair Vengeance DDR5 5600MT/s\n\n**Motherboard**: MSI MEG Z690i Unify \n\n**AIO**: ID-Cooling FrostFlow 240mm + 2x Noctua NF-F12s\n\n**Storage**: Samsung 980 Pro 2TB + Crucial P1 1TB\n\n**PSU**: Corsair SF750. Using Corsair\u2019s 12VHPWR cable for the GPU\n\n\nAs for temps on the GPU I\u2019ve yet to hit 70C playing games like Spiderman, Forza Horizon 5, Guardians of the Galaxy or Horizon Zero Dawn maxed out on an LG C2. \n\nCPU is usually around 65-75C depending on what I\u2019m playing. \n\nComing from an EVGA 3080Ti FTW Ultra, man is the 4080 way more quiet and power efficient. This thing barely gets over 300 watts and stays way cooler too. I had to limit my games to 4K 90fps with the 3080ti because it would pretty much hit 80C or higher in every thing I played (Crash 4 and Redout 2 being the worst offenders for whatever reason). \n\nAll in all, I can\u2019t complain. The 4080 is an efficiency monster!"
    },
    {
      "author": "RemedyGhost",
      "body": "Nice build my dude! I also have a 4080 in a A4-H2O however I am using the MSI (which is exactly the GPU limit at 322mm)"
    },
    {
      "author": "Solemnity_12",
      "body": "Oh nice! I didn\u2019t know any of the other AIB cards would fit lol. How you liking your MSI card?"
    },
    {
      "author": "RemedyGhost",
      "body": "Works great, no issues but the FE looks really good in that case."
    },
    {
      "author": "RemedyGhost",
      "body": "&#x200B;\n\nhttps://preview.redd.it/5xtvqjdj4cda1.jpeg?width=756&format=pjpg&auto=webp&v=enabled&s=551f063be2049405d926903306e060d909d0fe54"
    },
    {
      "author": "Solemnity_12",
      "body": "Man that is right at the edge lol"
    }
  ],
  "122": [
    {
      "author": "AzHP",
      "body": "https://preview.redd.it/tj7ipu7rxaea1.jpeg?width=3000&format=pjpg&auto=webp&v=enabled&s=f15202b2f01b6919276897a7cd69a9c01254e8b0\n\nI don't have a mm ruler on hand but it looks like it's exactly 1 inch. Thats measured from the edge of the PCB which is flush with the 16 pin terminal."
    },
    {
      "author": "Ryoohki_360",
      "body": "Nice thanks, i'll do my build hard line this spring and i'm going for a Retro Wave look so.. ;)"
    },
    {
      "author": "AzHP",
      "body": "You need to shove a spudger under a little flap on the back to expose the rest of the screws. I watched the official Nvidia tear down video of the 4090fe on YouTube as a guide."
    },
    {
      "author": "AzHP",
      "body": "If this is your first build I highly recommend the primochill kit that includes tubing, fittings, a silicone insert, bend guides, a finishing drill bit, and a tube cutter at a discount. I don't remember the exact price but it was somewhere between $100 and $150. It was at least much cheaper than any other equivalent package at the other major brands."
    },
    {
      "author": "Ryoohki_360",
      "body": "Yeah I saw petg kits and no thanks either pmma or acrylic for me especially here in the summer I bust 40c water temp easily"
    },
    {
      "author": "AzHP",
      "body": "Gotcha, I'm able to keep my temps under 40C but I live in the SF bay area and my ambient is usually 18C or lower and i have 10 fans on my radiators lol. \n\nBTW PMMA and acrylic are the same fyi. I would like to do an acrylic build eventually but this was my first build and I wanted something a bit more forgiving. Worked out for me but I totally understand why others would want acrylic."
    },
    {
      "author": "Ryoohki_360",
      "body": "Nice yeah I'm in Montr\u00e9al here we have -20c in the winter and 40c in summer white a swing and petg but I see they sell the fitting by package so it's fine"
    }
  ],
  "123": [
    {
      "author": "stefanels",
      "body": "But why using it 100%... better get some fans in the case , and use the card at max 70% fanspeed"
    },
    {
      "author": "BeautifulSingle4303",
      "body": "No I'm not, I'm just wondering how long it would last because I mainly use 55%"
    },
    {
      "author": "stefanels",
      "body": "Than it's fine , i use it max 70% - [PIC](https://i.ibb.co/wC2BCFs/6hr-division-2.jpg)"
    },
    {
      "author": "BeautifulSingle4303",
      "body": "What applications do you have open for that to be 61C??"
    },
    {
      "author": "stefanels",
      "body": "I have a great airflow in my case (Phanteks P600S , 6x 140mm fans) and i use my card undervolted 24/7 for better temps and lower power draw...\n\nI didn't fully understand your question..."
    },
    {
      "author": "BeautifulSingle4303",
      "body": "Nevermind then, I'm not that advanced really in this stuff. I don't really know how good airflow looks like either. I got a prebuilt PC so I dont know the case."
    },
    {
      "author": "BeautifulSingle4303",
      "body": "Main reason I made this post"
    },
    {
      "author": "stefanels",
      "body": "If it's a prebuilt, not great airflow then"
    },
    {
      "author": "BeautifulSingle4303",
      "body": "I got it from CyberPowerPC"
    },
    {
      "author": "stefanels",
      "body": "[THIS](https://i.ibb.co/yQSQ9hN/rwrwrwrwfvdv.jpg) is a great airflow, with this fan setup you can get 10C lower temps on CPU / GPU"
    },
    {
      "author": "BeautifulSingle4303",
      "body": "Should I share a picture of my PC? So you can have a look??"
    },
    {
      "author": "stefanels",
      "body": "OK"
    },
    {
      "author": "BeautifulSingle4303",
      "body": "https://preview.redd.it/6xofigffgdda1.jpeg?width=3024&format=pjpg&auto=webp&v=enabled&s=1bafa3b964782318af583ef301e98721ce57703c"
    },
    {
      "author": "BeautifulSingle4303",
      "body": "https://preview.redd.it/8v61tefggdda1.jpeg?width=3024&format=pjpg&auto=webp&v=enabled&s=ba7b5016747fdd0eeab7269884ef0f84627d3432"
    },
    {
      "author": "BeautifulSingle4303",
      "body": "it is pretty dusty lol"
    },
    {
      "author": "stefanels",
      "body": "&#x200B;\n\nhttps://preview.redd.it/y8llb99bidda1.jpeg?width=2048&format=pjpg&auto=webp&v=enabled&s=e5e80e76ef1e2ed1dc555b0c4a2831c47d015bc2"
    },
    {
      "author": "stefanels",
      "body": "Not bad, front of the case is solid plastic or mesh?\n\n[THIS](https://i.ibb.co/qJt3XLT/IMG-20210928-203130.jpg) is my PC - like a while ago, now i have other fans (non RGB) other GPU , other mobo,  but it's mostly the same airflow wise ([other pic](https://i.ibb.co/NgRdMH6/IMG-20210929-221740.jpg))"
    },
    {
      "author": "BeautifulSingle4303",
      "body": "Maybe solid plastic and the back where the usb ports are is also solid plastic"
    },
    {
      "author": "BeautifulSingle4303",
      "body": "Oh and by the way on Garten Of BanBan my temperatures were rising to about 70C (maybe it was because Im in the main menu)"
    },
    {
      "author": "BeautifulSingle4303",
      "body": "Oh and plus how much fps + how much heat would my gpu run while playing Call Of Duty modern warfare 2?"
    }
  ],
  "124": [
    {
      "author": "Ok-Key-8109",
      "body": "4070ti was the choice. The 4060ti only has the performance of a 3070, you would've barely moved up in performance"
    },
    {
      "author": "AverageKaynMain",
      "body": "I tought anything more than 4060 ti wouldnt fit my case and my psu wouldnt be enough luckily zotac one were barely smaller than 32 cm and my 650 w asus psu seems like working well probably at might next upgrade i will have to switch psu and case but 4070 ti will be enough until 60 to me"
    },
    {
      "author": "Ok-Key-8109",
      "body": "I mean feel free to upgrade your psu, but I have a 750w (gold) 80+ from evga and I do fine with a 4090 and 5800x3d. I have my gpu power limited and undervolted, and while gaming the max it'll go is 350w, but most of the time it's pulling 200 or under. The cpu only pulls 60 to 70 max. When I put it in a stress test, my pc only pulls 500w, and with stock settings 600w. \nI'd say just undervolt if that interests you, runs cooler and less power usage."
    },
    {
      "author": "AverageKaynMain",
      "body": "I use bronze psu and honestly i dont care much about power usage since its enough for now i would rather to spend my money to extra fans or cpu cooler"
    },
    {
      "author": "Ok-Key-8109",
      "body": "yeah I would personally prioritize other things.   \nCpu Cooler > Extra Fans > PSU. If your psu is doing exactly what you want it to do, no point in upgrading unless it's some random brand unknown bomb or you just want to upgrade"
    },
    {
      "author": "AverageKaynMain",
      "body": "Its bronze(i guess its the bad one) but asus tuf model ive been playing ac valhalla for 6 hours and my pc didnt exploded or crashed yet i guess that means psu is fine"
    },
    {
      "author": "Ok-Key-8109",
      "body": "It's a B tier or a mid range, so still good imo. But you should be fine if you're playing without any crashes / pc shut offs! Enjoy the gpu then!"
    },
    {
      "author": "AverageKaynMain",
      "body": "Thanks"
    }
  ],
  "125": [
    {
      "author": "Rowley71",
      "body": "Like to see how it does against the real competitor. I.e. the new instinct chip"
    },
    {
      "author": "norcalnatv",
      "body": "[https://learn.microsoft.com/en-us/windows/arm/overview](https://learn.microsoft.com/en-us/windows/arm/overview)"
    },
    {
      "author": "norcalnatv",
      "body": "Pretty sure every benchmarking site in the universe is eager to be first out with that comparison."
    },
    {
      "author": "Rowley71",
      "body": "It will be very interesting that's for sure."
    }
  ],
  "126": [
    {
      "author": "piter_penn",
      "body": "It can record 8k 60fps using shadowplay. I also think that it is able to record your situation at 120 fps, the only thing I don't get is why you need that.\n\nOr you want to stream like:\n\n1. record lossless 5760x2160 \n2. stream 4k 60\n3. play at 5760x2160 \n\nYes?"
    },
    {
      "author": "OstrixTheOstrich",
      "body": "I don't really understand how you came to this conclusion. Firstly, I want to record at 120FPS because I like smoothness in my recordings.\n\nSecondly, only #2 there is right. Ability to record lossless is nice but low priority, and I won't use it often. I won't be playing anything at 5760x2160, I'll be playing at 3840x2160, with OBS capturing 5760x2160 so it can include other things without overlaying the game.\n\nSo, to reformat that:\n\n1: (Low priority) Record lossless 5760x2160 at 60-120FPS.\n\n2: Record high quality 5760x2160 with little or no loss in frames in game or recording, recording other things in addition to my game (my other displays). This recording is for personal use.\n\n3: Stream 4k60, streaming only the game, while still doing #2 at the same time."
    },
    {
      "author": "piter_penn",
      "body": "You are playing 3840x2160, and you can't record that and get 5760x2160.\n\nYou can play 4k at any fps and capture footage with 60, record it at 60, and stream it at 60.\n\nThis is already a hard task for your system and something beyond is highly unlikely without upgrading your system."
    },
    {
      "author": "OstrixTheOstrich",
      "body": "Then explain why I can record 3840x1080p on my RX 570 8GB while playing 1920x1080p. Instead of denying its abilities, how about you test it yourself if you haven't already, which I'm pretty sure you haven't since we've gotten to this point. I'm convinced by now you don't know what either of us are talking about.\n\nUnless you're ONLY talking about Nvidia Shadowplay, which I never mentioned at all and will likely not be using. I specifically mentioned OBS."
    },
    {
      "author": "piter_penn",
      "body": "3840x1080p is a double fullhd display, I will skip that part for why you need to record the second monitor at the same time while you're playing.\n\n5760x2160 is likely like 4k plus FHD with unused space, and recording all of that is way out of my assuming.\n\nPlaying/recording at 4k is another level of doing the same with FHD. Oh my lord."
    }
  ],
  "127": [
    {
      "author": "fenghuang1",
      "body": "Different countries have their own pricing plans.  \n\nAlso, Nvidia might not be servicing these \"some countries\" that cannot afford it.  \n\nCurrently, of the countries this plan is being offered to, $10/month is nothing."
    },
    {
      "author": "1F35C",
      "body": "You got me curious, so I took a look, and I was surprised by how large the service area actually was: [https://nvidia.custhelp.com/app/answers/detail/a\\_id/5023/\\~/what-are-the-supported-locations-for-geforce-now%3F](https://nvidia.custhelp.com/app/answers/detail/a_id/5023/~/what-are-the-supported-locations-for-geforce-now%3F)\n\nI took a look at Serbia pricing (just picked a random lower GDP country, could be an outlier), and while the pricing is different from NA pricing like you mentioned, they're lumped with the rest of the EU and have to pay 10 euros/mo, while their GDP per capita is just a bit over 9000USD/person. So there do seem to be some areas where this service is supported, but is actually quite expensive."
    },
    {
      "author": "fenghuang1",
      "body": "So what?  \nIs the 9000USD/person going to be able to afford a $300-1600 GPU in that case for gaming?  \n\nBear in mind Nvidia is also running a business. There are operating costs to pay and employees' salaries to upkeep the GFN service."
    },
    {
      "author": "1F35C",
      "body": "I'm not saying they should price the service at $5 like the OC, or Nvidia should become a charity. I used to deal with cloud services like AWS and Azure, so I get there are costs in running services like this.\n\nI'm only saying there are places where $10/mo can be significant and could be a dealbreaker in taking up the service or not, especially for a hobby. \ud83e\udd37"
    }
  ],
  "128": [
    {
      "author": "P2Wlover",
      "body": "I have the dual aio setup and probably the same case (define 7?) as well, unless yours is XL or some other case, 420 won\u2019t fit in on top(I ordered 360 but they sent me 420). I have my msi suprim rad on top exhausting the air out cause it\u2019s pretty hot and 360 place in front intake, one fan bottom intake and rear fan intake and one more fan on top next to rad exhaust. My 360 arrive on Sunday I will update the result. But not sure if my result would be useful or nah since you got one more GPU\u2026\ud83d\uddff\ud83d\uddff"
    },
    {
      "author": "Timmaigh",
      "body": "Thanks!\n\nThis was actually my initial idea, but then i have been warned by another owner of the Suprim Liquid that the air from GPUs AIO radiator is quite hot, so he advised to have it as exhaust. Basically, CPU is 170W and GPU 450+, so it makes more sense to use CPU one as intake i guess.\n\nThus i am considering option 3, CPU rad infront as intake and GPU one uptop as exhaust. But again, someone else advised me to have both rads as exhausts and all the fans as intakes... all in all, it seems there is really no consensus on the matter."
    },
    {
      "author": "Timmaigh",
      "body": "Yes, my case is 7XL, so 420 will indeed fit. Please do inform me about your results anyway. Thank you!"
    },
    {
      "author": "P2Wlover",
      "body": "I was hesitant to post this update because the temperature wasn\u2019t looking very decent but fuck it: Gpu aio on top exhausted (was trying to add another fan next to pump but no space for it, tube from cpu aio get in the way) 360 cpu aio front intake, 2 bottom fan intake, I changed the rear fan to be exhausted since the fans mounted in pump are in zero rpm most of the time so there won\u2019t be any exhaust fan available if the rear fan was intake. That\u2019s total 5 intake fans and 1 (+2 when gpu fans start spinning) exhaust fans. I also changed msi liquid 4090 to vertical mount since I had to bend the 12hvhpwr cables and tubes hard in order to close the side panel..on the first boot, temperature was fantastic, every components showed in Aida64 were below 30C, around 33C after 10 mins. I guess it was because literally all of them were cold af\u2026I moved the case onto my desktop, the temperature was a bit worse than on the ground,  cpu around 35-38, gpu 37, so did the benchmarks ( 60C cyberpunk, 64C furmark, 89 furmark cpu test, I did undervolte cpu a bit) I then moved it back to the ground\u2026I swear the temperature dropped 2C lol. I think I\u2019m fine with most of the temperature except GPU. I was expecting some 50ish temperature no matter what task you had..even though 60C wasn\u2019t bad either \ud83e\udd26\ud83c\udffe\ud83e\udd37\ud83c\udffe\u200d\u2642\ufe0f"
    },
    {
      "author": "P2Wlover",
      "body": "I saw your original post in watercooling so I\u2019m using the third option, it is definitely the best one though as someone suggested in your other post. The air coming from gpu pump was HOT but you can definitely touch it(well, I don\u2019t know if you could touch it when the gpu temperature goes up to 80+, don\u2019t think it will happen, if I turn my fans to 100%, the temperature stays in 52Cish) You do not want to mount it in front. I tested it using my hand to feel the temperature when my cpu was in 50+C, the air was warm, gets warmer as the temperature goes up but still colder than the liquid in the pump for sure plus the 2 bottom intake fans cool down the air as well. All for all, you should test different combinations for yourself since you got another air cooled 4090? Which would generate shit tons of hot air for sure."
    },
    {
      "author": "P2Wlover",
      "body": "Final tip: don\u2019t use the smart fan setting in bios, they sucks (I\u2019m using gigabyte motherboard) use a software called Fan control instead. It has literally every option you want to do with your fans."
    },
    {
      "author": "P2Wlover",
      "body": "Lmao another one\ud83e\udd23: as someone suggested in your other post, A and C are the same, IMO no they aren\u2019t. The one difference is that the ventilate hole on top of the case is much larger than in front panel. In the front panel, your pump is against a filter+a solid door cover where if you mount the pump on top, you literally expose the whole pump therefore more cooling performance. I think A and C will be similar if you open the front door cover."
    },
    {
      "author": "Timmaigh",
      "body": "Thank you, i truly appreciate your input.\n\nI have been thinking about it for past couple of days and slowly getting used to \"C\" idea. It was my least favourite, since i originally wanted to mount CPU rad up top - the reason being aesthetics. It would look better there covering entire top, than 240mm GPU rad will. The CPU rad will be now bit wasted upfront, i mean its RGB fans. I had opportunity to buy Meshify XL over Define, actually it was more readily available, as i needed to wait for Define couple of weeks, and yeah, in that one it would be no issue. But i just liked simple box with sharp edges design of Define more - its too bad Meshify isnt the exacly same frame as Define with just mesh instead of door infront.\n\nAnyway, temps and noise beat aesthetics.\n\nOne more thing, i have 2 more Arctic P12 PWM PST ARG fans - i think its the same model thats used on their Freezer AIOs (like the CPU one i have, except there are 14cm fans). Do you think i can replace the stock fans on GPU rad with them? How are the stock fans, arent they noisy? Or maybe i could just add them into push-pull config?"
    },
    {
      "author": "P2Wlover",
      "body": "I found it that any fan over 1200 rpm is kinda noisy. With that being said, if you wanna swap them for better performance? Get something else, like what am I going to swap with: T30 fans. If you want it for a better noise? The stock fans are good enough."
    },
    {
      "author": "P2Wlover",
      "body": "Lmao, reports back: 2 T30 spinning at 1500 rpm when furmarking, don\u2019t see any temperature difference, I\u2019m done with this shit.\n\nEdited: gaming wise, I see 2-3 degrees lower in cyberpunk 2077 \ud83e\udd37\ud83c\udffe\u200d\u2642\ufe0fnoice"
    },
    {
      "author": "Timmaigh",
      "body": "Wanted them mostly cause of the RGB, since the stock ones are not. But if they would be downgrade performance or noise ones, i would stick to stock ones.\n\nEDIT: How about push-pull, again? Would it be a problem to have 2 different fan brands? Arctic ones pushing on the bottom and stock Gales pulling on top..."
    }
  ],
  "129": [
    {
      "author": "filisterr",
      "body": "1 second has 1000ms, hence the milli in the name and unless you play at 625fps (25*25), which I strongly doubt in the worst case, it is 4-5 frames at 4K or maybe even less."
    },
    {
      "author": "Divinicus1st",
      "body": "What? You guys are terrible at math."
    },
    {
      "author": "filisterr",
      "body": "25*40 = 1000ms == 1s, he said 40ms = 25 less frames so 25 * 25 = 625fps. So you are the one terrible at math."
    },
    {
      "author": "Divinicus1st",
      "body": "The point is you can't calculate fps from latency, the premise is wrong. Math isn't just about additions and multiplications, you have to understand what you're doing, and you clearly don't."
    }
  ],
  "130": [
    {
      "author": "Apprehensive_Edge658",
      "body": " Oh thanks! I have a rmx850 ill have to see if that works on mine."
    },
    {
      "author": "Ponald-Dump",
      "body": "It will, I have the rm1000x"
    },
    {
      "author": "Apprehensive_Edge658",
      "body": "Nice. Also my gigabyte 4080 connector has 3 wires (for 3 PCEI cables to go into) but this 12vhpwr cable only has 2, will it still works?"
    },
    {
      "author": "Ponald-Dump",
      "body": "Yes"
    }
  ],
  "131": [
    {
      "author": "Ditz3n",
      "body": "\\+1! I'm unsure to wait as I'll build a new PC at the end of February, or wait for that new RTX 4090 TI"
    },
    {
      "author": "Kgury",
      "body": " Its going to be at least $2500 USD"
    },
    {
      "author": "Ditz3n",
      "body": "I've got my money saved up for that."
    },
    {
      "author": "Kgury",
      "body": "Dope. Im with you bro lol"
    }
  ],
  "132": [
    {
      "author": "root_b33r",
      "body": "Yes and yes, the next runner up is the 2060 and that's like 40 bucks cheaper for like 20% less performance, unless I misremembered the prices you posted"
    },
    {
      "author": "karias_r",
      "body": "It's a Corsair cx power supply and the 2060 12gb is 140dlls less under the 3060\n\n$360 vs $500 \n$400 is the 3060 8gb version"
    },
    {
      "author": "root_b33r",
      "body": "Oh yeah but you don't need the 12gb for 1080p, vram comes into play with high quality textures or advanced shading which are more for triple a's than anything else 8g should be plenty unless you're gunning to play Hogwarts legacy or Forespoken right away here"
    },
    {
      "author": "karias_r",
      "body": "Yeah i don't plan to play demanding games\nWow and a few shoters is the only thing a have been playing in the last years"
    },
    {
      "author": "root_b33r",
      "body": "Yeah I wouldn't worry about it then unless you plan to upgrade your monitor soon to soonish but even then, then you neeeed the 3060"
    },
    {
      "author": "karias_r",
      "body": "Thanks for the input honestly was tinking that the 3060 Will ve a bit overkill for My needs especiallly for the price"
    },
    {
      "author": "root_b33r",
      "body": "Like it's high but there's cheaper used in my area if that's something you're open to looking at (the used market) it might be a bit overkill I'm more basing my experience with the 1070 which I still run, if you can find a 1080ti or 1070 I feel like you'll also be happy with those but the modern equivalent is the 3060, 1070 isn't going to do 144 numbers where the 3060 should but hopefully this information has helped"
    }
  ],
  "133": [
    {
      "author": "DanGleebitz",
      "body": "Would rather buy new for the warranty- after watching my rx480 die, every used video card is a ticking time bomb in my mind"
    },
    {
      "author": "jonginator",
      "body": "Consider a used EVGA card because those warranties transfer.\n\nCheck out r/hardwareswap"
    },
    {
      "author": "DanGleebitz",
      "body": "They said they're backing out of the GPU business so long term I'm wary of how long they will honor warranties in the future"
    },
    {
      "author": "jonginator",
      "body": "No need to be wary about it. \n\nThey have said they\u2019ll be able to honor warranties."
    }
  ],
  "134": [
    {
      "author": "nmkd",
      "body": "No, you're talking about VP9."
    },
    {
      "author": "nitroplus570",
      "body": "I can't find it now but one reddit user posted a video of Crysis gameplay, he recorded a minute of gameplay lossless and then encoded it twice, once with NVENC AV1 and once with NVENC H.265 and posted links to both videos on mega, the consensus in the thread was that current NVENC AV1 was disappointing and couldn't match H.265, at best they were equal. If I find it i'll post the link. \n\n[There's also this comparison from the 4090 review on benchmark.pl](https://youtu.be/YXweGeExm5s?t=9) \n\nEposVox also has some videos comparing AV1 to H.264 and it wins but not by much, suggesting that H.265 would beat it if compared, but it wasn't compared because H.265 can't be used for streaming.\n\nAlso linus said [\"H.265 looked as good or maybe even better than AV1\"](https://www.youtube.com/watch?v=WVjtK71qqXU&t=477s) \n\nI think for local recording H.265 is still best  \n\nActually, since you have a 4090, if you have the time, could you test this by recording a short lossless gameplay video and then encoding it twice, once with AV1 and once with 265 and upload both of them?"
    },
    {
      "author": "nmkd",
      "body": "Can do.\n\nHow can I do lossless recording? I guess OBS can do lossless NVENC?\n\nEDIT: Figured it out"
    },
    {
      "author": "nitroplus570",
      "body": "Yeah OBS can do it, alternatively you could download one of the lossless sample videos from https://media.xiph.org/ but those are huge files (and they're not gameplay) so it could take a while depending on your internet speed"
    },
    {
      "author": "nmkd",
      "body": "I recorded a run of Superposition @ 1080p Extreme, limited to 60 FPS for clean frametimes.\n\nAll videos were encoded to 2500 kbps.\n\nx264 Medium (2-Pass): https://m1.afileditch.ch/vjpRdFsHEtHASqtMikoZ.mp4\n\nNVENC H264: https://m1.afileditch.ch/YOqspHWofhCvMSDWTWGT.mp4\n\nx265 Medium (2-Pass): https://m1.afileditch.ch/JiuCOqctaViMYXHnGMXR.mp4\n\nNVENC H265: https://m1.afileditch.ch/CCpWFnSbVjBQKRvaJWD.mp4\n\nSVT-AV1 Preset 7: https://m1.afileditch.ch/NkqmIzLpKApLSLSPcYfc.mp4\n\nNVENC AV1: https://m1.afileditch.ch/vjpRdFsHEtHASqtMikoZ.mp4"
    },
    {
      "author": "nitroplus570",
      "body": "Thanks. Subjectively, NVENC H265 looks better than NVENC AV1 (although H265 here has slightly higher bitrate than AV1) but it depends on the frame. I opened both videos on fullscreen and alt-tabbing between them on the same frame and sometimes AV1 looks better, sometimes 265 looks better, but 265 looking better more often. \n\nI wonder if AV1 would perform better at 2160p, maybe 265 is only better at 1080p"
    }
  ],
  "135": [
    {
      "author": "HelixViewer",
      "body": "Several years ago I was in charge of manufacturing an electronic product for integration in a larger product.  I got ahead of the schedule and put the extras in company stores so that they would be safe until needed.  One was pulled out that had been put into a storage machine.  It fell out of its packaging and got stuck in the gears of the machine.\n\nThe unit was placed back into its packaging for storage.  When it was pulled out for use the assemblers saw that it was destroyed.  I complained to management not because my product was destroyed but because some human saw that it was stuck in the gears, pulled it out and placed it back in the packaging when they knew it was damaged.  That was the problem.  The problem was not found until months later.  My complaint was that the product cost $10,000 but the delay could have cost millions.  It was only the fact that I was ahead that allowed me to build a replacement with no impact to later production.  The point is that a person who wishes to avoid blame or does not care can occur in any company.  Changing the company does not address the root cause of then kind of problem.\n\nConsider the number of media sources who have stated as fact that NVIDIA or AMD hates their customers or does not care about gamers in just the last 8 weeks.  I had a good experience with an MSI RMA process.  That does not mean it will always be good or that I will not have a good or bad experience with some other brand."
    },
    {
      "author": "gritz90",
      "body": "I have an MSI CPU AIO and it works very well. Haven't needed to RMA it yet as it's only a year old but hopefully it lasts"
    },
    {
      "author": "gritz90",
      "body": "I respect that. My issue with Asus was not only the fact that they sent me a damaged card but their response was through email was always 'If the GPU functions properly under normal use, it cannot be replaced with another RMA'. They acknowledged the damage from the pictures I sent but they continued to say, \"if it works, that's that\".\n\nI jumped back and forth between 5 reps, some said they will issue a replacement, followed by a \"we cannot replace it if it works\" from another rep. Finally I got on the phone with them 2 days ago and pushed for a replacement. It seems like they are pushing the Canada service centre to provide a suitable replacement. This has been a nightmare experience."
    },
    {
      "author": "HelixViewer",
      "body": "I agree, that is a painful experience.  It seems that they had a problem but did not own up to making it right.  Replacing a non-working product with damaged but working unit suggest the same poor decision making as happened in my case.  They knew it was bad and did not step up and make it right."
    }
  ],
  "136": [
    {
      "author": "NBAYoungBoyGOD",
      "body": "\ud83d\ude33 that\u2019s the first one damn. Timespy link \ud83e\udd7a\ud83d\udc49\ud83d\udc48"
    },
    {
      "author": "Any_Cook_2293",
      "body": "Were you asking for a timespy link?\n\nHere's one when I first received the card and put it through its paces (ECC is on) - [https://www.3dmark.com/spy/33906620](https://www.3dmark.com/spy/33906620)"
    },
    {
      "author": "NBAYoungBoyGOD",
      "body": "Have u got an oc one with the 600W?"
    },
    {
      "author": "Any_Cook_2293",
      "body": "I'm not certain what you mean. That result was with +33 on the power slider, max voltage slider, and overclocked."
    },
    {
      "author": "NBAYoungBoyGOD",
      "body": "it lost to my ventus limited at 450 with a +175 and +1200 closed case so I assumed it wasn\u2019t max oc mb g"
    }
  ],
  "137": [
    {
      "author": "SnooWalruses8636",
      "body": "If you're used to large OLED in living room, then S95B is probably a good choice right now. Or wait for the new G3 with micro lens."
    },
    {
      "author": "austinsguitar",
      "body": "well then i'm afraid your out of luck on the 4k monitor front unless you go samsung g8 or g9 with tons of local dimming zones, or other va panels. i love my 65\" oled, im typing to you now on it, but i cant do the display brightness issues of oled for general monitor use, just media pc use.\n\noled isnt ready for desktop use unless full field white brightness is at around 300 nits and we are at around 220\\~250 right now, i would know, im on the brightest oled tv on the planet, the s95b. 120hz vs 280 for me has been a huge blessing, even if oled is technically \"faster\". you really do feel the latency hit at 120hz.\n\nalso, I would not go asus. I recently got an asus monitor and had to return it. I like my displays to have good contrast, and allow me to increase or decrease screen sharpness, and not have hdcp issues with my 4090. idk what it is about asus but that was not the play for me."
    },
    {
      "author": "austinsguitar",
      "body": "i mean some people make purchasing decisions based on what they feel they need, instead of what they actually need. dont need that bmw, but might as well get it cuz i got the cash kinda thing. I cant see any pixels on my 2 27 inch 1440p 280hz displays paired to my 4090, but I guess if you wanna go to 32 inches and higher it makes since. \n\nHe literally asked reddit to help him decide his needs... I'm giving my two cents."
    },
    {
      "author": "austinsguitar",
      "body": "i disagree. I use the s95b as my media pc and ps5 display. it is good as a media pc display, but there are some serious problems with it (the build is terrible, there is a weird tint to the display, the software is trash,and brightness abl still does not hold up to low low end monitors even if it is the brightest oled, and pixel layout is bad) all these things you dont notice laying in bed."
    },
    {
      "author": "SnooWalruses8636",
      "body": "Until microLED is viable and affordable, choosing compromises is just the unfortunate downside of using single display for every uses. Multiple displays for multiple uses is probably the easiest (but not necessarily the cheapest) way to avoid regret.\n\nS95b is pretty good for media and gaming. Non art productivity monitor generally aren't that expensive relatively."
    }
  ],
  "138": [
    {
      "author": "casual_brackets",
      "body": "https://www.nvidia.com/en-us/geforce/news/reflex-latency-analyzer-360hz-g-sync-monitors/\n\nBudget option as ldat is pricey."
    },
    {
      "author": "ThePie69",
      "body": "Do you need a special monitor for this??"
    },
    {
      "author": "casual_brackets",
      "body": "Yea and a special mouse but it\u2019s still gonna be far cheaper and easier to acquire than an LDAT.\n\nThe site I linked has a list of supported monitors/mouses."
    },
    {
      "author": "ThePie69",
      "body": "Mmm. \n\nWhere did you see the price or purchase option for LDAT?"
    },
    {
      "author": "casual_brackets",
      "body": "There is no purchase option, if you can find one expect to pay 1.5-2 thousand USD for it."
    },
    {
      "author": "ThePie69",
      "body": "Do you happen to have a link to some past sale event / auction or anything like that?"
    },
    {
      "author": "casual_brackets",
      "body": "No offhand I don\u2019t. It\u2019s from memory nvidia sends these to reviewers and tech news outlets from what I can tell, I remember at some point in a video someone briefly mentioned how much this ldat module actually cost. It was 1500 or so I\u2019m assuming anybody selling one would know they\u2019ve got one of the only ones for sale and it\u2019d be marked up. \n\nPoint is, they aren\u2019t cheap at all. Probably why they don\u2019t bother selling them."
    },
    {
      "author": "casual_brackets",
      "body": "https://developer.nvidia.com/nvidia-latency-display-analysis-tool\n\nBecome a dev lol"
    },
    {
      "author": "ThePie69",
      "body": "Good number of Nvidia employees are unaware that this even exists after some recent discussions."
    }
  ],
  "139": [
    {
      "author": "nottoshabbie",
      "body": "Just my opinion, I know some people will disagree with me, and that's fine. If I'm spending $1200 plus on a graphics card, I want my monies worth in fps, the full potential performance of the card. Not being cpu bottlenecked, getting the raw performance I paid for at stock settings on my monitor. Thats just me though, I know people that just don't care about it and that's cool, but I want my monies worth. I guarantee you thou you won't get good performance with cyberpunk with that cpu thou, just saying."
    },
    {
      "author": "Zeratqc",
      "body": "Actually cyberpunk is the kind of game where the difference is the least... gpu will run at 99%... and not the cpu. \n\nA 13900k will give a bit more frame because of the speed to process a frame, but the 8700k isn't bottlenecking since it's not running at 100%. Without a fps counter you wouldn't notice the difference.\n\n1200$ 90% perf, 1500$ more for 100%. Do the math. \nWould you buy a 1200$ gpu for 90% the perf of a 2700$ gpu or the 2700$ one ?\n\nWith your mentality nothing short of a 13900k + 4090 is worth buying. I hope you have a qd oled monitor at least to match those since this is where my 1500$ went instead of upgrading cpu I upgraded monitor to the best monitor available on the market."
    },
    {
      "author": "nottoshabbie",
      "body": "First off, you put words in my mouth. Nowhere did I even suggest you got to have a 13900k to run a 4080. I will not discuss any further until you retract your statement and stop being aggressive, I'm not being aggressive."
    },
    {
      "author": "Zeratqc",
      "body": "Lol, there is no aggressivity. Stop thinking any disagreement is aggressivity...\nYou just said you need to leave 0 perf buying a gpu. You will leave perf unless you get a 13900k."
    },
    {
      "author": "nottoshabbie",
      "body": "That isn't what I meant at all. I guess I didn't explain myself good enough. There are bottlenecks in games with a 4080 and a inferior cpu. I've seen several benchmarks with exactly that. That is all."
    }
  ],
  "140": [
    {
      "author": "carrot_gg",
      "body": "Yes, connect your 8pin PCI cables to the adaptor. The 4070 ti uses 2 I believe."
    },
    {
      "author": "SshadowAngelL",
      "body": "But are those 8 pin PCI 6+2?"
    },
    {
      "author": "carrot_gg",
      "body": "6+2=8"
    },
    {
      "author": "SshadowAngelL",
      "body": "Alright thank you"
    },
    {
      "author": "carrot_gg",
      "body": "Make sure that the adaptor cable is fully inserted into the GPU. Connect the cable to the GPU before inserting it into the motherboard as it will make it easier to work with. You should hear a click when the cable is properly inserted."
    },
    {
      "author": "SshadowAngelL",
      "body": "Got it thank you very much !"
    }
  ],
  "141": [
    {
      "author": "Genius_51",
      "body": "I also have this issue once installing my new monitor (Gigabyte M32U) only way to get 4 monitors was to plug that in with HDMI and use the rest with displayport. Using displayport on the M32U limited me to 3 monitors with the fourth being recognized by the control panel but unable to be used without unchecking a previous one. Still searching for a fix."
    },
    {
      "author": "Yokerkey",
      "body": "Interesting, thx for your input\u2026 might need to contact asus support :/"
    },
    {
      "author": "Yokerkey",
      "body": "Thx for your answer on this old post! I will test this out in a few days\u2026 because the new monitor is the Gigabyte M34QW-EK (or whatever the name is)\u2026\nSo maybe it\u2019s actually a fault of Gigabyte monitors\u2026 that would be so infuriating\u2026 if that\u2019s the case I\u2019ll hit up Gigabyte support\n\nEdit: I\u2019ll try to update if I get any new infos"
    },
    {
      "author": "Genius_51",
      "body": "I\u2019m also contacting gigabyte support. Would be very frustrated if I couldn\u2019t use 4 monitors"
    },
    {
      "author": "Genius_51",
      "body": "I\u2019m also contacting gigabyte support. Would be very frustrated if I couldn\u2019t use 4 monitors"
    }
  ],
  "142": [
    {
      "author": "ja-ki",
      "body": "Nah I used the original FE 4090 Adapter, so there are 3 connections from my PSU to the GPU. I still have one to spare but I don't need 600W Power Target"
    },
    {
      "author": "adok971",
      "body": "Thank you for your feedback.   \nIt look like you have a different version of the RM750x.   \nMy version has only 2x8pin pci-express.\n\nThe other one is reserved for the CPU.\n\nhttps://preview.redd.it/deffo7taezca1.jpeg?width=1639&format=pjpg&auto=webp&v=enabled&s=38b2d1889ac2833ccab67d2334abd15ccf107bc0"
    },
    {
      "author": "ja-ki",
      "body": "yeah I have the 2018 version, which has 4 8 pin pcie connectors. Which one do you have?"
    },
    {
      "author": "adok971",
      "body": "I guess that I have an older version.\nI bought it in 2017 on Amazon. [amazon](https://amzn.eu/d/9xzRqUq)\nHere is the detailed reference : \nCorsair CP-9020092-EU RMX Serie RM750X ATX/EPS Voll Modular 80 PLUS Gold 750W Netzteil, EU"
    },
    {
      "author": "ja-ki",
      "body": "interesting, I have this one: https://geizhals.de/corsair-rmx-series-rm750x-2018-cp-9020179-eu-a1777727.html\n\nCheck out the pictures, you can see the 4 pcie 8 pin connectors."
    },
    {
      "author": "adok971",
      "body": "Newer version of the PSU I think.\nThank you for the confirmation \ud83d\udc4c"
    }
  ],
  "143": [
    {
      "author": "casual_brackets",
      "body": "Ok don\u2019t let it run that long like 5-10 minutes. The card will downclock and I do it will all my cards, but there\u2019s no real reason to let it torture the VRMS of the card unnecessarily. Just let it drain the power for a few minutes while present and watching thermals then kill it. \n\nMsi kombustor is the one you want. Then run gl-furmark donut at 4K fullscreen while OC\u2019d."
    },
    {
      "author": "MDXZFR",
      "body": "Yup, I'll try gl furmark. I have cyberpunk too if the results will be valid for u"
    },
    {
      "author": "casual_brackets",
      "body": "Oh yea I\u2019m just wanting to see how that cooler handles 500 + watts anything approaching that would be great"
    },
    {
      "author": "MDXZFR",
      "body": "We'll see"
    },
    {
      "author": "MDXZFR",
      "body": "https://preview.redd.it/jn6qmhm0lhaa1.png?width=1080&format=pjpg&auto=webp&v=enabled&s=5bcd67eb872267ffb7f87226863611fad8461ad0\n\nStock clock, GL donut 4K preset"
    },
    {
      "author": "MDXZFR",
      "body": "https://preview.redd.it/2rh31co8lhaa1.png?width=1080&format=pjpg&auto=webp&v=enabled&s=69b932350b5a5d538f97b7fcb32fb8e87867771b\n\nOc core +240, mem +1550, 510w 1.1v"
    },
    {
      "author": "MDXZFR",
      "body": "https://preview.redd.it/ylsrwuh9mhaa1.jpeg?width=4736&format=pjpg&auto=webp&v=enabled&s=cf6c92a556613684dcb06eeb4bfeccc94d3be77f"
    },
    {
      "author": "MDXZFR",
      "body": "https://preview.redd.it/oh2pilbhmhaa1.jpeg?width=4736&format=pjpg&auto=webp&v=enabled&s=1af8a5aaf68a314c6cbe1c1406fdb147b1f6978e"
    },
    {
      "author": "casual_brackets",
      "body": "Not bad at all for 510 w. Good cooler"
    }
  ],
  "144": [
    {
      "author": "Cheesyburps",
      "body": "It's funny because your mindset will completely change when you get your new monitor. You won't even want to use the other monitors often (I have moved my 1440p screen to a different pc cause it just feels horrible to use, regardless of the picture quality). Also you don't need FPS games to show the difference. Even Windows itself feels amazing at 120+hz.\n\nBtw you can use a 1070/1080 for 1440p144hz, assuming you turn just a few settings down (mainly AA), then it becomes a cpu issue if you don't have something like an i7 7700k or better."
    },
    {
      "author": "Ichigojam",
      "body": "Interesting. Do your 1440p screens happen to have g-sync / do you have any opinions on g-sync with a 1080 ti at 1440p - necessary or not?"
    },
    {
      "author": "Cheesyburps",
      "body": "Yeah StarCraft won't benefit much, but if you have any input lag at all, that will be eliminated. I'll test it myself later on, to see if it's better than my 60hz screen.\n\nI totally know how you feel, I would have argued those exact points less than a year ago. I am actually sacrificing both of those features for high refresh rates, and I can say that I personally find 144hz beats everything else. \n\nLike, my first good monitor, that I only recently retired, was an ASUS IPS 1440p 27\" monitor with insanely good colour accuracy. But none of that matters now because 144hz is the real game changer. It is honestly the best upgrade anyone can get (if you already have an ssd).\n\nI know I'm kind of preaching about high refresh rates here but I genuinely believe they are the way forward. Maybe pick up a cheap one for less than $200 to see if it's worth you personally upgrading to?"
    },
    {
      "author": "Cheesyburps",
      "body": ">It's 100ish vs 130ish on a subset of titles\n\n100 vs 130 is still massive, so that isn't a good argument. And it can be 140+ in all titles if you have an i7 8700k.\n\n>and no practical difference on many.\n\nWhat games don't benefit from it? I can only think of RTS games, that's about it. I have tried nearly every genre, and I see benefits in pretty much all of them.\n\n>Your criticism of me was on a different basis and I listed my potential source of bias upfront as opposed to omitting it.\n\nWat? I think you're looking too deep into this. I'm just telling you that 144hz is superior lol."
    },
    {
      "author": "Cheesyburps",
      "body": ">If you're a professional whose livelihood depends on high frame rates, go 1080p. If you're anyone else... And you know you are... 1440p or even 4k. You can always lower the resolution in game. Protip, 1080p perfectly maps onto 4k. \n\nThis threw me off and since there are no 144hz 4k monitors atm it seemed like you were telling him to go for a higher res at lower hz, which would be stupid.\n\n\nOlder games would benefit from pretty much zero input lag, so it may be worth running at as high as you can go. Not that you could hit 900 consistently with even the fastest cpu today, it'll be more like 400-600. \n\nFixed framerates won't benefit at all, unless say your ips 1440p has shitty response time then there could be a difference. But yeah if you only play games with 60fps locks then it's not worth it at all.\n\n>Pretty much the only games where frame rate differences matter (for a 1080 class card) are fairly new first person shooters and not much else.\n\nYou've never even used a 144hz monitor, have you?\nAnd remember that DSR is a thing so if you want use more horsepower, in older titles, then you can just crank up the res."
    },
    {
      "author": "Cheesyburps",
      "body": "The response time for those tvs are like 22ms at very best, either choosing between 1080p120hz and 4k60hz. But that response time is at least 21ms worse than your standard sub $200 144hz monitor. I was looking into them myself for a while, but as you say, we're not there yet.\n\nOh okay. I didn't know that, will have to look into the framerates you can achieve. But you are wrong there. Even games like rocket league still benefit from high framerates like 600fps. I have personally tried switching between different locked fps for a few hundred hours, and the difference between even 400 and 500 is insane when it comes to input lag. Just make sure you don't have ancient peripherals. And just for reference, my reaction times are abysmal. If you have good reactions it would be an even better experience. Also that translates to shooters even more, be it first or third person.\n\nI totally understand, I mean quite a few years back you needed sli to pull off what we would consider decent fps by today's standards. Doesn't change the fact you would have enjoyed 144+ if you were able to do it.\n\nOh you will 100%. My only issue with that is 1080p on a 1440p screen looks terrible, but 1440p on a 1080p screen looks good, albeit a little bit blurry, but that can be fixed. Now back to your original comment, 1080p fits into 4k quite nicely, it's just that high framerates aren't possible right now. That's why I personally went with 1080p144hz, so that I can always hit 144fps and so I can upscale in games that don't need as much power. I will just buy a 4k 165hz monitor when they come out, and retire this one to my second pc."
    },
    {
      "author": "Cheesyburps",
      "body": "Just watched that video again and the \"gaming tv\" LG Nano Cell has 37.494ms input lag. Waaaay to much for a main gaming display.\n\nNah, most of them are around 5ms or higher.\n\nOh for sure. You have mentioned StarCraft a lot so I'm guessing that's your main game, if so, yeah you don't need a 144hz monitor, but it would objectively be a better experience overall. If you want to test it just set your monitor to 24hz and play around in windows a little bit. If you find the difference to be noticeable, you will feel it when you upgrade to higher refresh rates."
    },
    {
      "author": "Cheesyburps",
      "body": "Fluidity trumps picture quality every time, and this is coming from somebody who exclusively played games at 4k+ since the launch of DSR. I mean tbh you don't really have an argument until you own a high refresh rate monitor. I own one, and a high res monitor, so I can argue from both perspectives. \n\nHonestly, just come back to me when you buy one, and we can talk about how wrong you were."
    },
    {
      "author": "Cheesyburps",
      "body": "I mean you joke, but a lot of people use 1024 x 768 in csgo to ensure the highest framerate possible. People seem to like high framerates way more than res, probably because they are better for everyone except you lol.\n\nThat is certainly true, but again you still can't fully comprehend my argument until you upgrade. Don't bother trying when you lack crucial information."
    }
  ]
}